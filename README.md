# JAX-NSL: Neural Scientific Learning with JAX

[![CI](https://github.com/SatvikPraveen/JAX-NSL/actions/workflows/ci.yml/badge.svg)](https://github.com/SatvikPraveen/JAX-NSL/actions/workflows/ci.yml)
[![Documentation Status](https://github.com/SatvikPraveen/JAX-NSL/actions/workflows/docs.yml/badge.svg)](https://github.com/SatvikPraveen/JAX-NSL/actions/workflows/docs.yml)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

A comprehensive implementation of neural network architectures and scientific computing techniques using JAX, focusing on scalability, numerical stability, and advanced research applications.

## üéØ Project Overview

JAX-NSL demonstrates the full spectrum of modern neural scientific computing, from foundational array operations to large-scale distributed training. The project emphasizes production-ready implementations with rigorous attention to numerical stability, memory efficiency, and computational performance.

### Core Philosophy

- **Pure JAX Implementation**: Leverages JAX's native capabilities without high-level abstractions
- **Scientific Rigor**: Emphasizes numerical stability and mathematical correctness
- **Scalability First**: Designed for single-device to multi-cluster deployment
- **Research-Grade**: Implements cutting-edge techniques and optimization strategies

## üèóÔ∏è Architecture

```
jax-nsl/
‚îú‚îÄ‚îÄ üìö src/                     # Core library implementation
‚îÇ   ‚îú‚îÄ‚îÄ üßÆ core/               # Fundamental operations and utilities
‚îÇ   ‚îú‚îÄ‚îÄ üîÑ autodiff/           # Automatic differentiation extensions
‚îÇ   ‚îú‚îÄ‚îÄ üìê linalg/             # Linear algebra and numerical methods
‚îÇ   ‚îú‚îÄ‚îÄ üß† models/             # Neural network architectures
‚îÇ   ‚îú‚îÄ‚îÄ üéØ training/           # Optimization and training utilities
‚îÇ   ‚îú‚îÄ‚îÄ ‚ö° transforms/          # JAX transformations and control flow
‚îÇ   ‚îú‚îÄ‚îÄ üåê parallel/           # Distributed computing primitives
‚îÇ   ‚îî‚îÄ‚îÄ üõ†Ô∏è utils/              # Benchmarking and tree utilities
‚îú‚îÄ‚îÄ üìñ notebooks/              # Educational and demonstration materials
‚îÇ   ‚îú‚îÄ‚îÄ 01_fundamentals/       # JAX basics and core concepts
‚îÇ   ‚îú‚îÄ‚îÄ 02_linear_algebra/     # Matrix operations and solvers
‚îÇ   ‚îú‚îÄ‚îÄ 03_neural_networks/    # Network architectures from scratch
‚îÇ   ‚îú‚îÄ‚îÄ 04_training_optimization/ # Training loops and optimizers
‚îÇ   ‚îú‚îÄ‚îÄ 05_parallelism/        # Multi-device and distributed computing
‚îÇ   ‚îú‚îÄ‚îÄ 06_special_topics/     # Advanced research techniques
‚îÇ   ‚îî‚îÄ‚îÄ capstone_projects/     # Complex implementations
‚îú‚îÄ‚îÄ üß™ tests/                  # Comprehensive test suite
‚îú‚îÄ‚îÄ üìä data/                   # Synthetic data generation
‚îú‚îÄ‚îÄ üìë docs/                   # Documentation and guides
‚îî‚îÄ‚îÄ üê≥ docker/                 # Containerization setup
```

## ‚ú® Key Features

### üî¨ Scientific Computing

- **Numerical Stability**: Implements numerically stable algorithms for production use
- **Custom Derivatives**: Advanced VJP/JVP implementations for complex operations
- **Physics-Informed Networks**: Differential equation solvers with neural networks
- **Probabilistic Computing**: Bayesian methods and stochastic optimization

### ‚ö° Performance Optimization

- **JIT Compilation**: Optimized compilation strategies for maximum performance
- **Memory Efficiency**: Gradient checkpointing and mixed-precision training
- **Vectorization**: Efficient batching and SIMD utilization
- **Profiling Tools**: Built-in performance analysis and debugging utilities

### üåê Distributed Computing

- **Multi-Device Training**: Seamless scaling across GPUs and TPUs
- **Model Parallelism**: Sharding strategies for large-scale models
- **Data Parallelism**: Efficient batch distribution and gradient synchronization
- **Collective Operations**: Advanced communication patterns for distributed training

### üß† Neural Architectures

- **Transformers**: Attention mechanisms with linear scaling optimizations
- **Convolutional Networks**: Efficient convolution implementations
- **Recurrent Models**: Modern RNN variants and sequence modeling
- **Graph Networks**: Message passing and attention-based graph models

## üìö Learning Path

### Foundation Level

1. **JAX Fundamentals** - Array operations, PRNG systems, functional programming
2. **Automatic Differentiation** - Forward and reverse-mode AD, custom gradients
3. **Linear Algebra** - Matrix decompositions, iterative solvers, numerical methods

### Intermediate Level

4. **Neural Networks** - MLPs, CNNs, attention mechanisms from first principles
5. **Training Systems** - Optimizers, loss functions, training loop patterns
6. **Numerical Stability** - Precision handling, overflow prevention, robust algorithms

### Advanced Level

7. **Parallel Computing** - Multi-device coordination, sharding strategies
8. **Research Techniques** - Advanced optimizations, memory management, debugging
9. **Specialized Applications** - Physics-informed networks, probabilistic methods

### Capstone Projects

- **Physics-Informed Neural Networks**: Solving PDEs with deep learning
- **Large-Scale Training**: Distributed training of transformer models

## üöÄ Quick Start

### Prerequisites

```bash
# Minimum requirements
Python 3.8+
JAX >= 0.4.0
NumPy >= 1.21.0
```

### Installation

#### Standard Installation

```bash
git clone https://github.com/SatvikPraveen/JAX-NSL.git
cd JAX-NSL
pip install -r requirements.txt
pip install -e .
```

#### GPU Support

```bash
# For CUDA 11.x
pip install "jax[cuda11_pip]" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html

# For CUDA 12.x
pip install "jax[cuda12_pip]" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
```

#### Docker Environment

```bash
docker-compose -f docker/docker-compose.yml up --build
# Access Jupyter at http://localhost:8888
```

### Verification

```bash
# Run test suite
pytest tests/ -v

# Verify JAX installation
python -c "import jax; print(f'JAX version: {jax.__version__}'); print(f'Devices: {jax.devices()}')"

# Generate synthetic data
python data/synthetic/generate_data.py
```

## üìñ Usage Examples

### Basic Neural Network

```python
from src.models.mlp import MLP
from src.training.optimizers import create_adam_optimizer
from src.core.arrays import init_glorot_normal
import jax.numpy as jnp
import jax

# Initialize model
key = jax.random.PRNGKey(42)
model = MLP([784, 256, 128, 10])
params = model.init(key)

# Setup training
optimizer = create_adam_optimizer(learning_rate=1e-3)
opt_state = optimizer.init(params)

# Training step
def train_step(params, opt_state, batch):
    loss, grads = jax.value_and_grad(model.loss)(params, batch)
    updates, opt_state = optimizer.update(grads, opt_state)
    params = optax.apply_updates(params, updates)
    return params, opt_state, loss
```

### Distributed Training

```python
from src.parallel.pjit_utils import create_mesh, shard_params
from src.models.transformer import Transformer
from jax.experimental import pjit

# Setup device mesh
mesh = create_mesh(devices=jax.devices(), mesh_shape=(4, 2))

# Shard model parameters
with mesh:
    sharded_params = shard_params(params, partition_spec)

    # Distributed forward pass
    @pjit.pjit(in_axis_resources=(...), out_axis_resources=(...))
    def distributed_forward(params, inputs):
        return model.forward(params, inputs)
```

### Physics-Informed Networks

```python
from src.models.pinn import PINN
from src.training.losses import pde_loss

# Define PDE: ‚àÇu/‚àÇt = ‚àÇ¬≤u/‚àÇx¬≤
def heat_equation_residual(params, x, t):
    u = pinn.forward(params, x, t)
    u_t = jax.grad(lambda t: pinn.forward(params, x, t))(t)
    u_xx = jax.grad(jax.grad(lambda x: pinn.forward(params, x, t)))(x)
    return u_t - u_xx

# Training with physics constraints
pinn = PINN(layers=[2, 50, 50, 1])
loss = pde_loss(heat_equation_residual, boundary_conditions, initial_conditions)
```

## üß™ Testing

The project includes comprehensive testing across all modules:

```bash
# Run all tests
pytest tests/

# Test specific modules
pytest tests/test_autodiff.py -v
pytest tests/test_parallel.py -v
pytest tests/test_numerics.py -v

# Run with coverage
pytest --cov=src tests/

# Performance benchmarks
python -m pytest tests/ -k "benchmark" --benchmark-only
```

## üìä Benchmarks

Performance characteristics on various hardware configurations:

### Single Device (V100)

- **MLP Forward Pass**: ~2.3ms (batch_size=1024, hidden=[512, 256, 128])
- **Transformer Layer**: ~5.1ms (seq_len=512, embed_dim=512, 8 heads)
- **Convolution**: ~1.8ms (224x224x3 ‚Üí 224x224x64, 3x3 kernel)

### Multi-Device (8x V100)

- **Data Parallel Training**: 7.2x speedup (transformer, batch_size=512)
- **Model Parallel Training**: 5.8x speedup (large transformer, 1B parameters)
- **Pipeline Parallel**: 6.4x speedup (deep networks, 24+ layers)

## üìà Project Statistics

- **21 Jupyter Notebooks**: Comprehensive educational content
- **50+ Core Modules**: Production-ready implementations
- **150+ Unit Tests**: Rigorous testing coverage
- **10+ Advanced Techniques**: Research-grade optimizations
- **Multi-Platform Support**: CPU, GPU, TPU compatibility

## üõ†Ô∏è Development

### Code Style

```bash
# Format code
black src/ tests/
isort src/ tests/

# Type checking
mypy src/

# Linting
flake8 src/ tests/
```

### Contributing Guidelines

1. Fork the repository and create a feature branch
2. Implement changes with comprehensive tests
3. Ensure all existing tests pass
4. Add documentation for new features
5. Submit a pull request with clear description

### Development Setup

```bash
# Development dependencies
pip install -r requirements-dev.txt

# Pre-commit hooks
pre-commit install

# Build documentation locally
cd docs/ && make html
```

## üìã Requirements

### Core Dependencies

```
jax >= 0.4.0
jaxlib >= 0.4.0
numpy >= 1.21.0
scipy >= 1.7.0
optax >= 0.1.4
```

### Optional Dependencies

```
matplotlib >= 3.5.0      # Visualization
jupyter >= 1.0.0         # Notebooks
pytest >= 6.0.0          # Testing
black >= 22.0.0          # Code formatting
mypy >= 0.991            # Type checking
```

### System Requirements

- **Memory**: 8GB+ RAM (16GB+ recommended for large models)
- **Storage**: 2GB+ free space
- **GPU**: Optional but recommended (CUDA 11.0+)
- **OS**: Linux, macOS, Windows (WSL2)

## üåü Advanced Features

### Custom Operators

- **Fused Operations**: Memory-efficient compound operations
- **Custom Kernels**: Low-level GPU kernel implementations
- **Sparse Operations**: Efficient sparse matrix computations

### Memory Management

- **Gradient Checkpointing**: Trade computation for memory
- **Mixed Precision**: FP16/BF16 training support
- **Memory Profiling**: Built-in memory usage analysis

### Optimization Techniques

- **Learning Rate Scheduling**: Adaptive and cyclic schedules
- **Gradient Accumulation**: Simulate large batch training
- **Quantization**: Model compression techniques

## üîó Related Projects

- [JAX](https://github.com/google/jax) - The underlying framework
- [Flax](https://github.com/google/flax) - Neural network library for JAX
- [Optax](https://github.com/deepmind/optax) - Gradient processing and optimization
- [Haiku](https://github.com/deepmind/dm-haiku) - Neural network library

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- **JAX Team** for the exceptional framework and documentation
- **Scientific Computing Community** for algorithmic innovations
- **Open Source Contributors** who make projects like this possible

## üìû Contact & Support

- **GitHub Issues**: [Report bugs or request features](https://github.com/SatvikPraveen/JAX-NSL/issues)
- **GitHub Discussions**: [Community discussion and questions](https://github.com/SatvikPraveen/JAX-NSL/discussions)
- **Documentation**: [Comprehensive guides and API reference](https://satvikpraveen.github.io/JAX-NSL/)

---

**JAX-NSL** represents a comprehensive exploration of neural scientific computing, demonstrating the power and flexibility of JAX for both educational purposes and production deployments.
