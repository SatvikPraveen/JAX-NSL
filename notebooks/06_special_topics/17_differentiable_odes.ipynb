{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8175bb96",
   "metadata": {},
   "source": [
    "# Location: notebooks/06_special_topics/17_differentiable_odes.ipynb\n",
    "\n",
    "## Differentiable Ordinary Differential Equations (ODEs) in JAX\n",
    "\n",
    "This notebook explores solving and differentiating through ODEs using JAX, including neural ODEs, adjoint methods, and applications to dynamical systems modeling.\n",
    "\n",
    "## Basic ODE Solving with JAX\n",
    "\n",
    "```python\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "\n",
    "# Simple Euler method implementation\n",
    "def euler_step(f, y, t, dt):\n",
    "    \"\"\"Single Euler integration step\"\"\"\n",
    "    return y + dt * f(y, t)\n",
    "\n",
    "def ode_solve_euler(f, y0, t_span, dt):\n",
    "    \"\"\"Solve ODE using Euler method\"\"\"\n",
    "    t_start, t_end = t_span\n",
    "    n_steps = int((t_end - t_start) / dt)\n",
    "    \n",
    "    def scan_step(carry, t):\n",
    "        y = carry\n",
    "        y_next = euler_step(f, y, t, dt)\n",
    "        return y_next, y_next\n",
    "    \n",
    "    t_points = jnp.linspace(t_start, t_end, n_steps + 1)\n",
    "    _, trajectory = jax.lax.scan(scan_step, y0, t_points[:-1])\n",
    "    \n",
    "    return jnp.concatenate([y0[None], trajectory]), t_points\n",
    "\n",
    "# Example: Simple harmonic oscillator\n",
    "def harmonic_oscillator(y, t):\n",
    "    \"\"\"dy/dt = [y1, -y0] for y = [position, velocity]\"\"\"\n",
    "    return jnp.array([y[1], -y[0]])\n",
    "\n",
    "# Solve harmonic oscillator\n",
    "y0 = jnp.array([1.0, 0.0])  # Initial condition: position=1, velocity=0\n",
    "dt = 0.01\n",
    "t_span = (0.0, 2 * jnp.pi)\n",
    "\n",
    "trajectory, t_points = ode_solve_euler(harmonic_oscillator, y0, t_span, dt)\n",
    "\n",
    "print(f\"Trajectory shape: {trajectory.shape}\")\n",
    "print(f\"Time points shape: {t_points.shape}\")\n",
    "print(f\"Initial state: {trajectory[0]}\")\n",
    "print(f\"Final state: {trajectory[-1]}\")\n",
    "\n",
    "# The solution should be approximately sinusoidal\n",
    "expected_final = jnp.array([jnp.cos(2 * jnp.pi), -jnp.sin(2 * jnp.pi)])\n",
    "print(f\"Expected final state: {expected_final}\")\n",
    "print(f\"Close to expected: {jnp.allclose(trajectory[-1], expected_final, atol=1e-1)}\")\n",
    "```\n",
    "\n",
    "## Advanced ODE Solvers\n",
    "\n",
    "```python\n",
    "def runge_kutta_4(f, y, t, dt):\n",
    "    \"\"\"Fourth-order Runge-Kutta step\"\"\"\n",
    "    k1 = dt * f(y, t)\n",
    "    k2 = dt * f(y + k1/2, t + dt/2)\n",
    "    k3 = dt * f(y + k2/2, t + dt/2) \n",
    "    k4 = dt * f(y + k3, t + dt)\n",
    "    return y + (k1 + 2*k2 + 2*k3 + k4) / 6\n",
    "\n",
    "def ode_solve_rk4(f, y0, t_span, dt):\n",
    "    \"\"\"Solve ODE using RK4 method\"\"\"\n",
    "    t_start, t_end = t_span\n",
    "    n_steps = int((t_end - t_start) / dt)\n",
    "    \n",
    "    def scan_step(carry, t):\n",
    "        y = carry\n",
    "        y_next = runge_kutta_4(f, y, t, dt)\n",
    "        return y_next, y_next\n",
    "    \n",
    "    t_points = jnp.linspace(t_start, t_end, n_steps + 1)\n",
    "    _, trajectory = jax.lax.scan(scan_step, y0, t_points[:-1])\n",
    "    \n",
    "    return jnp.concatenate([y0[None], trajectory]), t_points\n",
    "\n",
    "# Adaptive step size solver (simplified)\n",
    "def adaptive_step_solver(f, y0, t_span, initial_dt=0.01, tol=1e-6):\n",
    "    \"\"\"Simple adaptive step size ODE solver\"\"\"\n",
    "    t_start, t_end = t_span\n",
    "    \n",
    "    def adaptive_step(state):\n",
    "        y, t, dt = state\n",
    "        \n",
    "        # Full step\n",
    "        y_full = runge_kutta_4(f, y, t, dt)\n",
    "        \n",
    "        # Two half steps  \n",
    "        y_half1 = runge_kutta_4(f, y, t, dt/2)\n",
    "        y_half2 = runge_kutta_4(f, y_half1, t + dt/2, dt/2)\n",
    "        \n",
    "        # Error estimate\n",
    "        error = jnp.linalg.norm(y_full - y_half2)\n",
    "        \n",
    "        # Adjust step size\n",
    "        dt_new = jnp.where(error > tol, dt * 0.8, \n",
    "                          jnp.where(error < tol/10, dt * 1.2, dt))\n",
    "        dt_new = jnp.clip(dt_new, 1e-6, 0.1)\n",
    "        \n",
    "        # Accept step if error is acceptable\n",
    "        y_new = jnp.where(error <= tol, y_half2, y)\n",
    "        t_new = jnp.where(error <= tol, t + dt, t)\n",
    "        \n",
    "        return (y_new, t_new, dt_new), (y_new, t_new, error)\n",
    "    \n",
    "    # Initialize\n",
    "    state = (y0, t_start, initial_dt)\n",
    "    \n",
    "    # Integrate until t_end (simplified version)\n",
    "    states = [state]\n",
    "    outputs = [(y0, t_start, 0.0)]\n",
    "    \n",
    "    for _ in range(1000):  # Maximum iterations\n",
    "        state, output = adaptive_step(state)\n",
    "        states.append(state)\n",
    "        outputs.append(output)\n",
    "        \n",
    "        if state[1] >= t_end:\n",
    "            break\n",
    "    \n",
    "    # Extract results\n",
    "    y_vals = jnp.array([out[0] for out in outputs])\n",
    "    t_vals = jnp.array([out[1] for out in outputs])\n",
    "    errors = jnp.array([out[2] for out in outputs])\n",
    "    \n",
    "    return y_vals, t_vals, errors\n",
    "\n",
    "# Compare different solvers on a challenging problem\n",
    "def van_der_pol(y, t, mu=2.0):\n",
    "    \"\"\"Van der Pol oscillator\"\"\"\n",
    "    x, v = y\n",
    "    return jnp.array([v, mu * (1 - x**2) * v - x])\n",
    "\n",
    "y0_vdp = jnp.array([2.0, 0.0])\n",
    "t_span_vdp = (0.0, 10.0)\n",
    "\n",
    "# Solve with different methods\n",
    "traj_euler, t_euler = ode_solve_euler(van_der_pol, y0_vdp, t_span_vdp, 0.01)\n",
    "traj_rk4, t_rk4 = ode_solve_rk4(van_der_pol, y0_vdp, t_span_vdp, 0.01)\n",
    "\n",
    "print(\"Van der Pol Oscillator Solutions:\")\n",
    "print(f\"Euler final state: {traj_euler[-1]}\")\n",
    "print(f\"RK4 final state: {traj_rk4[-1]}\")\n",
    "print(f\"Solutions differ by: {jnp.linalg.norm(traj_euler[-1] - traj_rk4[-1])}\")\n",
    "```\n",
    "\n",
    "## Neural ODEs: Parameterized Dynamics\n",
    "\n",
    "```python\n",
    "def create_neural_ode(hidden_dim=64):\n",
    "    \"\"\"Create a neural ODE with learnable dynamics\"\"\"\n",
    "    \n",
    "    def init_params(key, input_dim):\n",
    "        k1, k2, k3 = jax.random.split(key, 3)\n",
    "        \n",
    "        # Simple 2-layer MLP for dynamics\n",
    "        params = {\n",
    "            'w1': jax.random.normal(k1, (input_dim, hidden_dim)) * 0.1,\n",
    "            'b1': jnp.zeros(hidden_dim),\n",
    "            'w2': jax.random.normal(k2, (hidden_dim, hidden_dim)) * 0.1,\n",
    "            'b2': jnp.zeros(hidden_dim),\n",
    "            'w3': jax.random.normal(k3, (hidden_dim, input_dim)) * 0.1,\n",
    "            'b3': jnp.zeros(input_dim)\n",
    "        }\n",
    "        return params\n",
    "    \n",
    "    def neural_dynamics(params, y, t):\n",
    "        \"\"\"Neural network dynamics function\"\"\"\n",
    "        # Input is [y, t] concatenated\n",
    "        x = jnp.concatenate([y, jnp.array([t])])\n",
    "        \n",
    "        # Forward pass through MLP\n",
    "        h1 = jax.nn.tanh(x @ params['w1'] + params['b1'])\n",
    "        h2 = jax.nn.tanh(h1 @ params['w2'] + params['b2'])\n",
    "        dydt = h2 @ params['w3'] + params['b3']\n",
    "        \n",
    "        return dydt\n",
    "    \n",
    "    def neural_ode_forward(params, y0, t_span, dt=0.01):\n",
    "        \"\"\"Forward pass through neural ODE\"\"\"\n",
    "        dynamics_fn = lambda y, t: neural_dynamics(params, y, t)\n",
    "        trajectory, t_points = ode_solve_rk4(dynamics_fn, y0, t_span, dt)\n",
    "        return trajectory, t_points\n",
    "    \n",
    "    return init_params, neural_ode_forward\n",
    "\n",
    "# Initialize neural ODE\n",
    "init_neural_ode, neural_ode_forward = create_neural_ode(hidden_dim=32)\n",
    "neural_params = init_neural_ode(jax.random.PRNGKey(42), input_dim=2)\n",
    "\n",
    "print(\"Neural ODE Parameters:\")\n",
    "for name, param in neural_params.items():\n",
    "    print(f\"  {name}: {param.shape}\")\n",
    "\n",
    "# Test neural ODE\n",
    "y0_test = jnp.array([1.0, 0.0])\n",
    "neural_traj, neural_t = neural_ode_forward(neural_params, y0_test, (0.0, 2.0))\n",
    "\n",
    "print(f\"Neural ODE trajectory shape: {neural_traj.shape}\")\n",
    "print(f\"Initial state: {neural_traj[0]}\")\n",
    "print(f\"Final state: {neural_traj[-1]}\")\n",
    "```\n",
    "\n",
    "## Training Neural ODEs\n",
    "\n",
    "```python\n",
    "def create_neural_ode_loss():\n",
    "    \"\"\"Create loss function for training neural ODEs\"\"\"\n",
    "    \n",
    "    def trajectory_loss(params, y0, target_trajectory, t_points, dt=0.01):\n",
    "        \"\"\"Loss based on trajectory matching\"\"\"\n",
    "        # Get predicted trajectory\n",
    "        t_span = (t_points[0], t_points[-1])\n",
    "        pred_trajectory, _ = neural_ode_forward(params, y0, t_span, dt)\n",
    "        \n",
    "        # Interpolate predictions to match target time points\n",
    "        # Simplified: assume same time grid\n",
    "        n_target = target_trajectory.shape[0]\n",
    "        n_pred = pred_trajectory.shape[0]\n",
    "        \n",
    "        if n_pred >= n_target:\n",
    "            # Downsample predictions\n",
    "            indices = jnp.linspace(0, n_pred-1, n_target).astype(int)\n",
    "            pred_interp = pred_trajectory[indices]\n",
    "        else:\n",
    "            # Use available predictions (simplified)\n",
    "            pred_interp = pred_trajectory[:n_target]\n",
    "        \n",
    "        # MSE loss\n",
    "        loss = jnp.mean((pred_interp - target_trajectory) ** 2)\n",
    "        return loss\n",
    "    \n",
    "    def endpoint_loss(params, y0, target_endpoint, t_final):\n",
    "        \"\"\"Loss based only on final endpoint\"\"\"\n",
    "        t_span = (0.0, t_final)\n",
    "        trajectory, _ = neural_ode_forward(params, y0, t_span)\n",
    "        final_state = trajectory[-1]\n",
    "        return jnp.mean((final_state - target_endpoint) ** 2)\n",
    "    \n",
    "    return trajectory_loss, endpoint_loss\n",
    "\n",
    "# Create training setup\n",
    "trajectory_loss_fn, endpoint_loss_fn = create_neural_ode_loss()\n",
    "\n",
    "# Generate synthetic training data (spiral dynamics)\n",
    "def true_spiral_dynamics(y, t, omega=1.0):\n",
    "    \"\"\"True dynamics we want to learn\"\"\"\n",
    "    x, v = y\n",
    "    return jnp.array([-omega * v, omega * x])\n",
    "\n",
    "# Generate training trajectory\n",
    "y0_train = jnp.array([1.0, 0.0])\n",
    "true_traj, true_t = ode_solve_rk4(true_spiral_dynamics, y0_train, (0.0, 4.0), 0.02)\n",
    "\n",
    "print(f\"Training data shape: {true_traj.shape}\")\n",
    "\n",
    "# Training step\n",
    "def train_step(params, y0, target_traj, t_points, lr=0.01):\n",
    "    \"\"\"Single training step\"\"\"\n",
    "    loss, grads = jax.value_and_grad(trajectory_loss_fn)(params, y0, target_traj, t_points)\n",
    "    \n",
    "    # Simple SGD update\n",
    "    new_params = jax.tree_map(lambda p, g: p - lr * g, params, grads)\n",
    "    return new_params, loss\n",
    "\n",
    "# Training loop\n",
    "train_params = neural_params\n",
    "losses = []\n",
    "\n",
    "print(\"Training Neural ODE:\")\n",
    "for epoch in range(50):\n",
    "    train_params, loss = train_step(train_params, y0_train, true_traj, true_t)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss:.6f}\")\n",
    "\n",
    "print(f\"Final loss: {losses[-1]:.6f}\")\n",
    "\n",
    "# Test trained model\n",
    "trained_traj, _ = neural_ode_forward(train_params, y0_train, (0.0, 4.0))\n",
    "final_error = jnp.linalg.norm(trained_traj[-1] - true_traj[-1])\n",
    "print(f\"Final state error: {final_error:.6f}\")\n",
    "```\n",
    "\n",
    "## Adjoint Method for Memory-Efficient Training\n",
    "\n",
    "```python\n",
    "def create_adjoint_neural_ode():\n",
    "    \"\"\"Create neural ODE with adjoint method for memory efficiency\"\"\"\n",
    "    \n",
    "    def augmented_dynamics(aug_state, t, params):\n",
    "        \"\"\"Augmented dynamics for adjoint method\"\"\"\n",
    "        # aug_state = [y, lambda, params_flat]\n",
    "        n_y = 2  # Dimension of original state\n",
    "        y = aug_state[:n_y]\n",
    "        \n",
    "        # Forward dynamics\n",
    "        dydt = neural_dynamics(params, y, t)\n",
    "        \n",
    "        return dydt\n",
    "    \n",
    "    def adjoint_solve(params, y0, loss_grad_output, t_span, dt=0.01):\n",
    "        \"\"\"Solve adjoint equation for gradients\"\"\"\n",
    "        # This is a simplified version of the adjoint method\n",
    "        \n",
    "        # Forward pass to get trajectory\n",
    "        trajectory, t_points = neural_ode_forward(params, y0, t_span, dt)\n",
    "        \n",
    "        # Backward pass (simplified - normally uses adjoint ODE)\n",
    "        def loss_fn(p):\n",
    "            traj, _ = neural_ode_forward(p, y0, t_span, dt)\n",
    "            return jnp.sum(traj[-1] * loss_grad_output)  # Simplified loss\n",
    "        \n",
    "        grads = jax.grad(loss_fn)(params)\n",
    "        \n",
    "        return grads, trajectory\n",
    "    \n",
    "    def efficient_train_step(params, y0, target, t_span, lr=0.01):\n",
    "        \"\"\"Training step using adjoint method\"\"\"\n",
    "        # Forward pass\n",
    "        pred_traj, _ = neural_ode_forward(params, y0, t_span)\n",
    "        final_pred = pred_traj[-1]\n",
    "        \n",
    "        # Loss and its gradient w.r.t. final state\n",
    "        loss = jnp.sum((final_pred - target) ** 2)\n",
    "        loss_grad = 2 * (final_pred - target)\n",
    "        \n",
    "        # Get parameter gradients via adjoint\n",
    "        param_grads, _ = adjoint_solve(params, y0, loss_grad, t_span)\n",
    "        \n",
    "        # Update parameters\n",
    "        new_params = jax.tree_map(lambda p, g: p - lr * g, params, param_grads)\n",
    "        \n",
    "        return new_params, loss\n",
    "    \n",
    "    return efficient_train_step\n",
    "\n",
    "# Test adjoint method\n",
    "efficient_train_step = create_adjoint_neural_ode()\n",
    "\n",
    "# Compare memory usage (conceptually)\n",
    "adjoint_params = init_neural_ode(jax.random.PRNGKey(123), input_dim=2)\n",
    "target_final = true_traj[-1]\n",
    "\n",
    "print(\"Training with Adjoint Method:\")\n",
    "for epoch in range(10):\n",
    "    adjoint_params, loss = efficient_train_step(\n",
    "        adjoint_params, y0_train, target_final, (0.0, 4.0)\n",
    "    )\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss:.6f}\")\n",
    "\n",
    "# Test final performance\n",
    "adjoint_traj, _ = neural_ode_forward(adjoint_params, y0_train, (0.0, 4.0))\n",
    "adjoint_error = jnp.linalg.norm(adjoint_traj[-1] - true_traj[-1])\n",
    "print(f\"Adjoint method final error: {adjoint_error:.6f}\")\n",
    "```\n",
    "\n",
    "## Continuous Normalizing Flows\n",
    "\n",
    "```python\n",
    "def create_cnf_model():\n",
    "    \"\"\"Create a Continuous Normalizing Flow model\"\"\"\n",
    "    \n",
    "    def cnf_dynamics(params, y, t):\n",
    "        \"\"\"CNF dynamics with divergence computation\"\"\"\n",
    "        # Augment state with log-determinant\n",
    "        n_dims = 2\n",
    "        z = y[:n_dims]\n",
    "        \n",
    "        # Neural network dynamics\n",
    "        dzdt = neural_dynamics(params, z, t)\n",
    "        \n",
    "        # Compute trace of Jacobian (divergence) - simplified\n",
    "        def trace_estimator(z):\n",
    "            return jnp.sum(jax.vmap(jax.grad(lambda zi: neural_dynamics(params, zi, t)[0]))(z))\n",
    "        \n",
    "        # Hutchinson's trace estimator (simplified)\n",
    "        div_estimate = trace_estimator(z)\n",
    "        \n",
    "        # Return [dzdt, -div]\n",
    "        return jnp.concatenate([dzdt, jnp.array([-div_estimate])])\n",
    "    \n",
    "    def cnf_forward(params, z0, t_span=(0.0, 1.0)):\n",
    "        \"\"\"Forward pass through CNF\"\"\"\n",
    "        # Augment initial state with log-det-jacobian = 0\n",
    "        aug_z0 = jnp.concatenate([z0, jnp.array([0.0])])\n",
    "        \n",
    "        # Solve ODE\n",
    "        dynamics_fn = lambda y, t: cnf_dynamics(params, y, t)\n",
    "        aug_trajectory, t_points = ode_solve_rk4(dynamics_fn, aug_z0, t_span, dt=0.01)\n",
    "        \n",
    "        # Extract trajectory and log-det-jacobians\n",
    "        trajectory = aug_trajectory[:, :2]\n",
    "        log_det_jac = aug_trajectory[:, 2]\n",
    "        \n",
    "        return trajectory, log_det_jac, t_points\n",
    "    \n",
    "    def cnf_log_likelihood(params, x, base_log_prob_fn):\n",
    "        \"\"\"Compute log-likelihood under CNF model\"\"\"\n",
    "        # Transform x back to base distribution\n",
    "        traj, log_det_jac, _ = cnf_forward(params, x, t_span=(1.0, 0.0))  # Reverse\n",
    "        z_base = traj[-1]\n",
    "        \n",
    "        # Log-likelihood = base_log_prob + log_det_jac\n",
    "        base_log_prob = base_log_prob_fn(z_base)\n",
    "        log_likelihood = base_log_prob + log_det_jac[-1]\n",
    "        \n",
    "        return log_likelihood, z_base\n",
    "    \n",
    "    return cnf_forward, cnf_log_likelihood\n",
    "\n",
    "# Create CNF model\n",
    "cnf_forward, cnf_log_likelihood = create_cnf_model()\n",
    "\n",
    "# Test CNF\n",
    "cnf_params = init_neural_ode(jax.random.PRNGKey(456), input_dim=2)\n",
    "\n",
    "# Sample from base distribution (standard normal)\n",
    "base_sample = jax.random.normal(jax.random.PRNGKey(789), (2,))\n",
    "print(f\"Base sample: {base_sample}\")\n",
    "\n",
    "# Transform through CNF\n",
    "cnf_traj, cnf_log_det, cnf_t = cnf_forward(cnf_params, base_sample)\n",
    "transformed_sample = cnf_traj[-1]\n",
    "\n",
    "print(f\"Transformed sample: {transformed_sample}\")\n",
    "print(f\"Log-det-jacobian: {cnf_log_det[-1]}\")\n",
    "\n",
    "# Test log-likelihood computation\n",
    "def standard_normal_log_prob(z):\n",
    "    \"\"\"Log probability of standard normal\"\"\"\n",
    "    return -0.5 * jnp.sum(z ** 2) - jnp.log(2 * jnp.pi)\n",
    "\n",
    "test_point = jnp.array([0.5, -0.3])\n",
    "log_lik, base_z = cnf_log_likelihood(cnf_params, test_point, standard_normal_log_prob)\n",
    "print(f\"Log-likelihood at test point: {log_lik}\")\n",
    "print(f\"Corresponding base point: {base_z}\")\n",
    "```\n",
    "\n",
    "## Applications: Modeling Physical Systems\n",
    "\n",
    "```python\n",
    "def create_physics_informed_ode():\n",
    "    \"\"\"Create physics-informed neural ODE\"\"\"\n",
    "    \n",
    "    def hamiltonian_dynamics(params, y, t):\n",
    "        \"\"\"Hamiltonian dynamics with neural network potential\"\"\"\n",
    "        n_dims = len(y) // 2\n",
    "        q = y[:n_dims]  # Positions\n",
    "        p = y[n_dims:]  # Momenta\n",
    "        \n",
    "        # dq/dt = ∂H/∂p = p (assuming unit mass)\n",
    "        dqdt = p\n",
    "        \n",
    "        # dp/dt = -∂H/∂q = -∂V/∂q (V is potential energy)\n",
    "        # Learn potential gradient with neural network\n",
    "        def potential_energy(pos):\n",
    "            # Simple neural network for potential\n",
    "            h = jax.nn.tanh(pos @ params['w1'] + params['b1'])\n",
    "            return jnp.sum(h @ params['w2'] + params['b2'])\n",
    "        \n",
    "        dpdt = -jax.grad(potential_energy)(q)\n",
    "        \n",
    "        return jnp.concatenate([dqdt, dpdt])\n",
    "    \n",
    "    def init_hamiltonian_params(key):\n",
    "        \"\"\"Initialize parameters for Hamiltonian system\"\"\"\n",
    "        k1, k2 = jax.random.split(key)\n",
    "        return {\n",
    "            'w1': jax.random.normal(k1, (2, 16)) * 0.1,\n",
    "            'b1': jnp.zeros(16),\n",
    "            'w2': jax.random.normal(k2, (16,)) * 0.1,\n",
    "            'b2': 0.0\n",
    "        }\n",
    "    \n",
    "    def energy_conservation_loss(params, y0, t_span):\n",
    "        \"\"\"Loss that encourages energy conservation\"\"\"\n",
    "        dynamics_fn = lambda y, t: hamiltonian_dynamics(params, y, t)\n",
    "        trajectory, _ = ode_solve_rk4(dynamics_fn, y0, t_span, dt=0.01)\n",
    "        \n",
    "        # Compute energy at each time step\n",
    "        def total_energy(state):\n",
    "            n_dims = len(state) // 2\n",
    "            q, p = state[:n_dims], state[n_dims:]\n",
    "            \n",
    "            # Kinetic energy: 0.5 * |p|^2\n",
    "            kinetic = 0.5 * jnp.sum(p ** 2)\n",
    "            \n",
    "            # Potential energy from neural network\n",
    "            h = jax.nn.tanh(q @ params['w1'] + params['b1'])\n",
    "            potential = jnp.sum(h @ params['w2'] + params['b2'])\n",
    "            \n",
    "            return kinetic + potential\n",
    "        \n",
    "        energies = jax.vmap(total_energy)(trajectory)\n",
    "        \n",
    "        # Energy should be conserved\n",
    "        energy_variance = jnp.var(energies)\n",
    "        return energy_variance\n",
    "    \n",
    "    return init_hamiltonian_params, hamiltonian_dynamics, energy_conservation_loss\n",
    "\n",
    "# Create physics-informed model\n",
    "init_ham_params, ham_dynamics, energy_loss_fn = create_physics_informed_ode()\n",
    "ham_params = init_ham_params(jax.random.PRNGKey(999))\n",
    "\n",
    "# Test energy conservation\n",
    "y0_ham = jnp.array([1.0, 0.0, 0.0, 1.0])  # [q1, q2, p1, p2]\n",
    "energy_loss = energy_loss_fn(ham_params, y0_ham, (0.0, 2.0))\n",
    "print(f\"Initial energy conservation loss: {energy_loss:.6f}\")\n",
    "\n",
    "# Train to minimize energy variance\n",
    "def train_physics_step(params, y0, t_span, lr=0.01):\n",
    "    loss, grads = jax.value_and_grad(energy_loss_fn)(params, y0, t_span)\n",
    "    new_params = jax.tree_map(lambda p, g: p - lr * g, params, grads)\n",
    "    return new_params, loss\n",
    "\n",
    "print(\"Training Physics-Informed ODE:\")\n",
    "for epoch in range(20):\n",
    "    ham_params, loss = train_physics_step(ham_params, y0_ham, (0.0, 2.0))\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}: Energy variance = {loss:.6f}\")\n",
    "\n",
    "print(f\"Final energy conservation loss: {loss:.6f}\")\n",
    "\n",
    "# Test trained Hamiltonian system\n",
    "ham_dynamics_trained = lambda y, t: hamiltonian_dynamics(ham_params, y, t)\n",
    "ham_trajectory, ham_t = ode_solve_rk4(ham_dynamics_trained, y0_ham, (0.0, 5.0), dt=0.01)\n",
    "\n",
    "print(f\"Hamiltonian trajectory shape: {ham_trajectory.shape}\")\n",
    "print(f\"Initial state: {ham_trajectory[0]}\")\n",
    "print(f\"Final state: {ham_trajectory[-1]}\")\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we explored differentiable ODEs in JAX:\n",
    "\n",
    "**Core Concepts:**\n",
    "- **ODE Solvers**: Euler, Runge-Kutta, and adaptive methods\n",
    "- **Neural ODEs**: Parameterized dynamics with neural networks  \n",
    "- **Adjoint Method**: Memory-efficient gradient computation\n",
    "- **Continuous Flows**: Normalizing flows with ODE dynamics\n",
    "\n",
    "**Key Applications:**\n",
    "- **Dynamical Systems**: Modeling physical and biological systems\n",
    "- **Generative Models**: Continuous normalizing flows for density estimation\n",
    "- **Time Series**: Neural ODEs for irregular time series modeling\n",
    "- **Physics-Informed**: Incorporating physical constraints and conservation laws\n",
    "\n",
    "**Training Strategies:**\n",
    "- **Trajectory Matching**: Fit entire solution paths\n",
    "- **Endpoint Fitting**: Match only final states\n",
    "- **Conservation Laws**: Physics-informed losses\n",
    "- **Likelihood Training**: Maximum likelihood for generative models\n",
    "\n",
    "**Advanced Techniques:**\n",
    "- **Augmented Dynamics**: Including log-determinant computation\n",
    "- **Stochastic ODEs**: Adding noise for uncertainty quantification\n",
    "- **Hamiltonian Systems**: Energy-conserving dynamics\n",
    "- **Multi-Scale**: Handling systems with different time scales\n",
    "\n",
    "**Computational Benefits:**\n",
    "- **Automatic Differentiation**: End-to-end gradient computation\n",
    "- **Adaptive Integration**: Error control and efficiency\n",
    "- **Memory Efficiency**: Adjoint method for long trajectories\n",
    "- **Parallelization**: Vectorized operations across initial conditions\n",
    "\n",
    "Neural ODEs provide a powerful framework for learning continuous-time dynamics while leveraging the full power of automatic differentiation and modern optimization techniques in JAX."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
