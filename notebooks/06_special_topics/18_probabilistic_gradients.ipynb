{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3667400b",
   "metadata": {},
   "source": [
    "# Location: notebooks/06_special_topics/18_probabilistic_gradients.ipynb\n",
    "\n",
    "## Probabilistic Programming and Stochastic Gradients in JAX\n",
    "\n",
    "This notebook explores probabilistic programming concepts in JAX, including variational inference, stochastic gradients, and gradient estimation techniques for discrete and continuous random variables.\n",
    "\n",
    "## Basic Probability Distributions and Sampling\n",
    "\n",
    "```python\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, vmap, grad\n",
    "from jax.scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "\n",
    "# Basic sampling and probability computations\n",
    "def demonstrate_basic_distributions():\n",
    "    \"\"\"Demonstrate basic probability distributions in JAX\"\"\"\n",
    "    key = random.PRNGKey(42)\n",
    "    \n",
    "    # Continuous distributions\n",
    "    keys = random.split(key, 5)\n",
    "    \n",
    "    # Normal distribution\n",
    "    normal_samples = random.normal(keys[0], (1000,))\n",
    "    normal_logprob = stats.norm.logpdf(normal_samples, 0.0, 1.0)\n",
    "    \n",
    "    # Beta distribution  \n",
    "    beta_samples = random.beta(keys[1], 2.0, 5.0, (1000,))\n",
    "    beta_logprob = stats.beta.logpdf(beta_samples, 2.0, 5.0)\n",
    "    \n",
    "    # Gamma distribution\n",
    "    gamma_samples = random.gamma(keys[2], 2.0, (1000,))\n",
    "    gamma_logprob = stats.gamma.logpdf(gamma_samples, 2.0)\n",
    "    \n",
    "    # Discrete distributions\n",
    "    categorical_samples = random.categorical(keys[3], jnp.array([0.1, 0.3, 0.4, 0.2]), (1000,))\n",
    "    \n",
    "    # Binomial (using Bernoulli approximation)\n",
    "    bernoulli_samples = random.bernoulli(keys[4], 0.3, (1000,))\n",
    "    \n",
    "    return {\n",
    "        'normal': (normal_samples, normal_logprob),\n",
    "        'beta': (beta_samples, beta_logprob), \n",
    "        'gamma': (gamma_samples, gamma_logprob),\n",
    "        'categorical': categorical_samples,\n",
    "        'bernoulli': bernoulli_samples\n",
    "    }\n",
    "\n",
    "# Test basic distributions\n",
    "distributions = demonstrate_basic_distributions()\n",
    "\n",
    "print(\"Distribution Statistics:\")\n",
    "for name, data in distributions.items():\n",
    "    if isinstance(data, tuple):\n",
    "        samples, logprobs = data\n",
    "        print(f\"{name:12}: mean={jnp.mean(samples):.3f}, std={jnp.std(samples):.3f}, \"\n",
    "              f\"logprob_mean={jnp.mean(logprobs):.3f}\")\n",
    "    else:\n",
    "        samples = data\n",
    "        print(f\"{name:12}: mean={jnp.mean(samples):.3f}, unique_vals={len(jnp.unique(samples))}\")\n",
    "```\n",
    "\n",
    "## Stochastic Computation Graphs\n",
    "\n",
    "```python\n",
    "def create_stochastic_graph():\n",
    "    \"\"\"Create a stochastic computation graph\"\"\"\n",
    "    \n",
    "    def stochastic_forward(key, x, noise_scale=0.1):\n",
    "        \"\"\"Forward pass with stochastic operations\"\"\"\n",
    "        k1, k2, k3 = random.split(key, 3)\n",
    "        \n",
    "        # Add Gaussian noise to input\n",
    "        noisy_x = x + noise_scale * random.normal(k1, x.shape)\n",
    "        \n",
    "        # Stochastic linear transformation\n",
    "        w_noise = random.normal(k2, (x.shape[-1], 64)) * 0.1\n",
    "        w_deterministic = jnp.ones((x.shape[-1], 64)) * 0.5\n",
    "        w = w_deterministic + w_noise\n",
    "        \n",
    "        h = jnp.tanh(noisy_x @ w)\n",
    "        \n",
    "        # Dropout-like stochastic masking\n",
    "        mask = random.bernoulli(k3, 0.8, h.shape)\n",
    "        h_masked = h * mask / 0.8  # Scale to maintain expectation\n",
    "        \n",
    "        # Final linear layer (deterministic)\n",
    "        output_w = jnp.ones((64, 10)) * 0.1\n",
    "        output = h_masked @ output_w\n",
    "        \n",
    "        return output, {\n",
    "            'noisy_input': noisy_x,\n",
    "            'weights': w,\n",
    "            'hidden': h,\n",
    "            'mask': mask,\n",
    "            'masked_hidden': h_masked\n",
    "        }\n",
    "    \n",
    "    def expected_output(x, n_samples=100):\n",
    "        \"\"\"Estimate expected output via Monte Carlo\"\"\"\n",
    "        keys = random.split(random.PRNGKey(0), n_samples)\n",
    "        \n",
    "        def single_forward(key):\n",
    "            output, _ = stochastic_forward(key, x)\n",
    "            return output\n",
    "        \n",
    "        outputs = vmap(single_forward)(keys)\n",
    "        return jnp.mean(outputs, axis=0), jnp.std(outputs, axis=0)\n",
    "    \n",
    "    return stochastic_forward, expected_output\n",
    "\n",
    "# Test stochastic computation graph\n",
    "stoch_forward, expected_forward = create_stochastic_graph()\n",
    "\n",
    "# Test input\n",
    "test_x = jnp.array([[1.0, 2.0, 3.0]])\n",
    "\n",
    "# Single stochastic forward pass\n",
    "single_output, intermediates = stoch_forward(random.PRNGKey(123), test_x)\n",
    "print(f\"Single forward output shape: {single_output.shape}\")\n",
    "print(f\"Output values: {single_output[0][:5]}\")  # First 5 values\n",
    "\n",
    "# Expected output via Monte Carlo\n",
    "expected_out, std_out = expected_forward(test_x)\n",
    "print(f\"Expected output: {expected_out[0][:5]}\")\n",
    "print(f\"Output std: {std_out[0][:5]}\")\n",
    "\n",
    "# Compare deterministic vs stochastic\n",
    "print(f\"Coefficient of variation: {jnp.mean(std_out / jnp.abs(expected_out)):.3f}\")\n",
    "```\n",
    "\n",
    "## Variational Inference with Reparameterization\n",
    "\n",
    "```python\n",
    "def create_variational_autoencoder():\n",
    "    \"\"\"Create a simple VAE for demonstrating reparameterization trick\"\"\"\n",
    "    \n",
    "    def encoder(params, x):\n",
    "        \"\"\"Encoder network: x -> (mu, log_sigma)\"\"\"\n",
    "        h1 = jax.nn.relu(x @ params['enc_w1'] + params['enc_b1'])\n",
    "        h2 = jax.nn.relu(h1 @ params['enc_w2'] + params['enc_b2'])\n",
    "        \n",
    "        mu = h2 @ params['enc_mu_w'] + params['enc_mu_b']\n",
    "        log_sigma = h2 @ params['enc_sigma_w'] + params['enc_sigma_b']\n",
    "        \n",
    "        return mu, log_sigma\n",
    "    \n",
    "    def decoder(params, z):\n",
    "        \"\"\"Decoder network: z -> x_recon\"\"\"\n",
    "        h1 = jax.nn.relu(z @ params['dec_w1'] + params['dec_b1'])\n",
    "        h2 = jax.nn.relu(h1 @ params['dec_w2'] + params['dec_b2'])\n",
    "        x_recon = jax.nn.sigmoid(h2 @ params['dec_w3'] + params['dec_b3'])\n",
    "        \n",
    "        return x_recon\n",
    "    \n",
    "    def reparameterize(key, mu, log_sigma):\n",
    "        \"\"\"Reparameterization trick: z = mu + sigma * epsilon\"\"\"\n",
    "        epsilon = random.normal(key, mu.shape)\n",
    "        sigma = jnp.exp(log_sigma)\n",
    "        z = mu + sigma * epsilon\n",
    "        return z\n",
    "    \n",
    "    def vae_loss(params, key, x):\n",
    "        \"\"\"VAE loss with KL divergence and reconstruction loss\"\"\"\n",
    "        # Encode\n",
    "        mu, log_sigma = encoder(params, x)\n",
    "        \n",
    "        # Reparameterize\n",
    "        z = reparameterize(key, mu, log_sigma)\n",
    "        \n",
    "        # Decode\n",
    "        x_recon = decoder(params, z)\n",
    "        \n",
    "        # Reconstruction loss (binary cross-entropy)\n",
    "        recon_loss = -jnp.sum(x * jnp.log(x_recon + 1e-8) + \n",
    "                             (1 - x) * jnp.log(1 - x_recon + 1e-8))\n",
    "        \n",
    "        # KL divergence loss\n",
    "        kl_loss = 0.5 * jnp.sum(jnp.exp(2 * log_sigma) + mu**2 - 1 - 2 * log_sigma)\n",
    "        \n",
    "        total_loss = recon_loss + kl_loss\n",
    "        \n",
    "        return total_loss, {\n",
    "            'recon_loss': recon_loss,\n",
    "            'kl_loss': kl_loss,\n",
    "            'mu': mu,\n",
    "            'log_sigma': log_sigma,\n",
    "            'z': z,\n",
    "            'x_recon': x_recon\n",
    "        }\n",
    "    \n",
    "    def init_vae_params(key, input_dim=784, latent_dim=20, hidden_dim=400):\n",
    "        \"\"\"Initialize VAE parameters\"\"\"\n",
    "        keys = random.split(key, 10)\n",
    "        \n",
    "        params = {\n",
    "            # Encoder\n",
    "            'enc_w1': random.normal(keys[0], (input_dim, hidden_dim)) * 0.01,\n",
    "            'enc_b1': jnp.zeros(hidden_dim),\n",
    "            'enc_w2': random.normal(keys[1], (hidden_dim, hidden_dim)) * 0.01,\n",
    "            'enc_b2': jnp.zeros(hidden_dim),\n",
    "            'enc_mu_w': random.normal(keys[2], (hidden_dim, latent_dim)) * 0.01,\n",
    "            'enc_mu_b': jnp.zeros(latent_dim),\n",
    "            'enc_sigma_w': random.normal(keys[3], (hidden_dim, latent_dim)) * 0.01,\n",
    "            'enc_sigma_b': jnp.zeros(latent_dim),\n",
    "            \n",
    "            # Decoder\n",
    "            'dec_w1': random.normal(keys[4], (latent_dim, hidden_dim)) * 0.01,\n",
    "            'dec_b1': jnp.zeros(hidden_dim),\n",
    "            'dec_w2': random.normal(keys[5], (hidden_dim, hidden_dim)) * 0.01,\n",
    "            'dec_b2': jnp.zeros(hidden_dim),\n",
    "            'dec_w3': random.normal(keys[6], (hidden_dim, input_dim)) * 0.01,\n",
    "            'dec_b3': jnp.zeros(input_dim)\n",
    "        }\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    return init_vae_params, vae_loss, encoder, decoder, reparameterize\n",
    "\n",
    "# Create VAE\n",
    "init_vae, vae_loss_fn, encode_fn, decode_fn, reparam_fn = create_variational_autoencoder()\n",
    "\n",
    "# Initialize parameters\n",
    "vae_params = init_vae(random.PRNGKey(456), input_dim=28, latent_dim=8, hidden_dim=64)\n",
    "\n",
    "print(\"VAE Parameters:\")\n",
    "for name, param in vae_params.items():\n",
    "    print(f\"  {name}: {param.shape}\")\n",
    "\n",
    "# Test VAE forward pass\n",
    "test_input = random.uniform(random.PRNGKey(789), (28,))  # Mock 28D input\n",
    "loss, info = vae_loss_fn(vae_params, random.PRNGKey(101), test_input)\n",
    "\n",
    "print(f\"\\nVAE Forward Pass:\")\n",
    "print(f\"Total loss: {loss:.4f}\")\n",
    "print(f\"Reconstruction loss: {info['recon_loss']:.4f}\")\n",
    "print(f\"KL divergence: {info['kl_loss']:.4f}\")\n",
    "print(f\"Latent mean: {info['mu'][:4]}\")  # First 4 dimensions\n",
    "print(f\"Latent log_sigma: {info['log_sigma'][:4]}\")\n",
    "```\n",
    "\n",
    "## Gradient Estimation for Discrete Variables\n",
    "\n",
    "```python\n",
    "def demonstrate_gradient_estimation():\n",
    "    \"\"\"Demonstrate gradient estimation techniques for discrete variables\"\"\"\n",
    "    \n",
    "    # REINFORCE (likelihood ratio) estimator\n",
    "    def reinforce_estimator(params, key, n_samples=1000):\n",
    "        \"\"\"REINFORCE gradient estimator\"\"\"\n",
    "        \n",
    "        def sample_and_loss(key):\n",
    "            # Sample discrete action from categorical distribution\n",
    "            logits = params['logits']\n",
    "            probs = jax.nn.softmax(logits)\n",
    "            action = random.categorical(key, logits)\n",
    "            \n",
    "            # Reward function (to maximize)\n",
    "            reward = jnp.where(action == 2, 1.0, -0.1)  # Prefer action 2\n",
    "            \n",
    "            # REINFORCE gradient: grad_log_prob * reward\n",
    "            log_prob = jax.nn.log_softmax(logits)[action]\n",
    "            \n",
    "            return -reward, log_prob, reward  # Negative for minimization\n",
    "        \n",
    "        keys = random.split(key, n_samples)\n",
    "        losses, log_probs, rewards = vmap(sample_and_loss)(keys)\n",
    "        \n",
    "        # REINFORCE estimator\n",
    "        def loss_fn(params):\n",
    "            logits = params['logits']\n",
    "            log_probs_param = jax.nn.log_softmax(logits)\n",
    "            \n",
    "            # Use pre-sampled actions and rewards\n",
    "            return jnp.mean(-rewards * log_probs_param[actions])\n",
    "        \n",
    "        # This is a simplified version - normally we'd use the actual sampled actions\n",
    "        mean_loss = jnp.mean(losses)\n",
    "        grad_estimate = jnp.mean(vmap(lambda lp, r: -r * grad(lambda x: x)(lp))(log_probs, rewards))\n",
    "        \n",
    "        return mean_loss, grad_estimate, jnp.mean(rewards)\n",
    "    \n",
    "    # Gumbel-Softmax (reparameterization for discrete)\n",
    "    def gumbel_softmax_estimator(params, key, temperature=1.0):\n",
    "        \"\"\"Gumbel-Softmax reparameterization trick\"\"\"\n",
    "        \n",
    "        def gumbel_softmax_sample(logits, key, temperature):\n",
    "            # Sample Gumbel noise\n",
    "            gumbel_noise = -jnp.log(-jnp.log(random.uniform(key, logits.shape) + 1e-8) + 1e-8)\n",
    "            \n",
    "            # Gumbel-Softmax\n",
    "            y = jax.nn.softmax((logits + gumbel_noise) / temperature)\n",
    "            return y\n",
    "        \n",
    "        def differentiable_loss(params, key):\n",
    "            logits = params['logits']\n",
    "            \n",
    "            # Sample using Gumbel-Softmax\n",
    "            soft_sample = gumbel_softmax_sample(logits, key, temperature)\n",
    "            \n",
    "            # Differentiable reward (prefer index 2)\n",
    "            target = jnp.array([0.0, 0.0, 1.0, 0.0])  # One-hot for action 2\n",
    "            reward = jnp.dot(soft_sample, target) - 0.1 * jnp.dot(soft_sample, 1 - target)\n",
    "            \n",
    "            return -reward  # Negative for minimization\n",
    "        \n",
    "        loss = differentiable_loss(params, key)\n",
    "        grad_fn = grad(differentiable_loss)\n",
    "        gradient = grad_fn(params, key)\n",
    "        \n",
    "        return loss, gradient\n",
    "    \n",
    "    # Control variate for REINFORCE\n",
    "    def reinforce_with_baseline(params, key, n_samples=1000):\n",
    "        \"\"\"REINFORCE with baseline to reduce variance\"\"\"\n",
    "        \n",
    "        def sample_and_evaluate(key):\n",
    "            logits = params['logits']\n",
    "            action = random.categorical(key, logits)\n",
    "            \n",
    "            # Reward and baseline\n",
    "            reward = jnp.where(action == 2, 1.0, -0.1)\n",
    "            baseline = params['baseline']  # Learned baseline\n",
    "            \n",
    "            # Advantage\n",
    "            advantage = reward - baseline\n",
    "            \n",
    "            log_prob = jax.nn.log_softmax(logits)[action]\n",
    "            \n",
    "            return reward, advantage, log_prob, action\n",
    "        \n",
    "        keys = random.split(key, n_samples)\n",
    "        rewards, advantages, log_probs, actions = vmap(sample_and_evaluate)(keys)\n",
    "        \n",
    "        # Policy gradient with baseline\n",
    "        policy_grad = jnp.mean(advantages * log_probs)\n",
    "        \n",
    "        # Baseline update (MSE with rewards)\n",
    "        baseline_loss = jnp.mean((rewards - params['baseline']) ** 2)\n",
    "        \n",
    "        return policy_grad, baseline_loss, jnp.mean(rewards)\n",
    "    \n",
    "    return reinforce_estimator, gumbel_softmax_estimator, reinforce_with_baseline\n",
    "\n",
    "# Test gradient estimation techniques\n",
    "reinforce_est, gumbel_est, reinforce_baseline = demonstrate_gradient_estimation()\n",
    "\n",
    "# Initialize parameters for discrete optimization\n",
    "discrete_params = {\n",
    "    'logits': jnp.array([0.1, 0.2, -0.3, 0.0]),  # 4 possible actions\n",
    "    'baseline': 0.0\n",
    "}\n",
    "\n",
    "print(\"Gradient Estimation for Discrete Variables:\")\n",
    "\n",
    "# Test REINFORCE\n",
    "key = random.PRNGKey(112)\n",
    "reinforce_loss, reinforce_grad, avg_reward = reinforce_est(discrete_params, key)\n",
    "print(f\"REINFORCE - Loss: {reinforce_loss:.4f}, Avg Reward: {avg_reward:.4f}\")\n",
    "\n",
    "# Test Gumbel-Softmax  \n",
    "gumbel_loss, gumbel_grad = gumbel_est(discrete_params, key)\n",
    "print(f\"Gumbel-Softmax - Loss: {gumbel_loss:.4f}\")\n",
    "print(f\"Gumbel gradient: {gumbel_grad['logits']}\")\n",
    "\n",
    "# Test REINFORCE with baseline\n",
    "policy_grad, baseline_loss, baseline_reward = reinforce_baseline(discrete_params, key)\n",
    "print(f\"REINFORCE + Baseline - Policy grad: {policy_grad:.4f}, \"\n",
    "      f\"Baseline loss: {baseline_loss:.4f}, Reward: {baseline_reward:.4f}\")\n",
    "```\n",
    "\n",
    "## Stochastic Optimization and Natural Gradients\n",
    "\n",
    "```python\n",
    "def create_natural_gradient_optimizer():\n",
    "    \"\"\"Create natural gradient optimizer for probabilistic models\"\"\"\n",
    "    \n",
    "    def fisher_information_matrix(params, data_batch):\n",
    "        \"\"\"Approximate Fisher Information Matrix\"\"\"\n",
    "        \n",
    "        def single_data_score(params, x):\n",
    "            # Log-likelihood for single data point\n",
    "            mu = params['mu']\n",
    "            log_sigma = params['log_sigma']\n",
    "            sigma = jnp.exp(log_sigma)\n",
    "            \n",
    "            log_lik = stats.norm.logpdf(x, mu, sigma)\n",
    "            return jnp.sum(log_lik)\n",
    "        \n",
    "        # Score function (gradient of log-likelihood)\n",
    "        score_fn = grad(single_data_score)\n",
    "        \n",
    "        # Compute scores for batch\n",
    "        scores = vmap(score_fn, in_axes=(None, 0))(params, data_batch)\n",
    "        \n",
    "        # Fisher Information = E[score * score^T]\n",
    "        # Approximate with sample covariance of scores\n",
    "        def vectorize_pytree(tree):\n",
    "            leaves, treedef = jax.tree_flatten(tree)\n",
    "            return jnp.concatenate([leaf.flatten() for leaf in leaves])\n",
    "        \n",
    "        score_vectors = vmap(vectorize_pytree)(scores)\n",
    "        fisher_matrix = jnp.cov(score_vectors.T)\n",
    "        \n",
    "        return fisher_matrix, score_vectors\n",
    "    \n",
    "    def natural_gradient_step(params, data_batch, lr=0.01, damping=1e-4):\n",
    "        \"\"\"Natural gradient update step\"\"\"\n",
    "        \n",
    "        # Standard gradient\n",
    "        def neg_log_likelihood(params):\n",
    "            mu = params['mu'] \n",
    "            log_sigma = params['log_sigma']\n",
    "            sigma = jnp.exp(log_sigma)\n",
    "            \n",
    "            log_liks = stats.norm.logpdf(data_batch, mu, sigma)\n",
    "            return -jnp.mean(jnp.sum(log_liks, axis=1))  # Negative log-likelihood\n",
    "        \n",
    "        std_grad = grad(neg_log_likelihood)(params)\n",
    "        \n",
    "        # Fisher information matrix\n",
    "        fisher_matrix, scores = fisher_information_matrix(params, data_batch)\n",
    "        \n",
    "        # Vectorize standard gradient\n",
    "        std_grad_vec, treedef = jax.tree_flatten(std_grad)\n",
    "        std_grad_flat = jnp.concatenate([g.flatten() for g in std_grad_vec])\n",
    "        \n",
    "        # Natural gradient = F^(-1) * standard_gradient\n",
    "        damped_fisher = fisher_matrix + damping * jnp.eye(fisher_matrix.shape[0])\n",
    "        nat_grad_flat = jnp.linalg.solve(damped_fisher, std_grad_flat)\n",
    "        \n",
    "        # Reconstruct gradient tree structure\n",
    "        shapes = [g.shape for g in std_grad_vec]\n",
    "        sizes = [g.size for g in std_grad_vec]\n",
    "        \n",
    "        start_idx = 0\n",
    "        nat_grad_leaves = []\n",
    "        for size, shape in zip(sizes, shapes):\n",
    "            nat_grad_leaves.append(nat_grad_flat[start_idx:start_idx+size].reshape(shape))\n",
    "            start_idx += size\n",
    "        \n",
    "        nat_grad = jax.tree_unflatten(treedef, nat_grad_leaves)\n",
    "        \n",
    "        # Update parameters\n",
    "        new_params = jax.tree_map(lambda p, g: p - lr * g, params, nat_grad)\n",
    "        \n",
    "        return new_params, std_grad, nat_grad\n",
    "    \n",
    "    return natural_gradient_step\n",
    "\n",
    "# Test natural gradients\n",
    "natural_grad_step = create_natural_gradient_optimizer()\n",
    "\n",
    "# Generate synthetic data from known distribution\n",
    "true_mu, true_sigma = 2.0, 1.5\n",
    "key = random.PRNGKey(333)\n",
    "data = random.normal(key, (100,)) * true_sigma + true_mu\n",
    "data = data[:, None]  # Make 2D for batch processing\n",
    "\n",
    "# Initialize parameters\n",
    "prob_params = {\n",
    "    'mu': jnp.array([0.0]),\n",
    "    'log_sigma': jnp.array([0.0])  # log(1.0) = 0.0\n",
    "}\n",
    "\n",
    "print(\"Natural Gradient Optimization:\")\n",
    "print(f\"True parameters: mu={true_mu}, sigma={true_sigma}\")\n",
    "print(f\"Initial parameters: mu={prob_params['mu'][0]:.3f}, sigma={jnp.exp(prob_params['log_sigma'][0]):.3f}\")\n",
    "\n",
    "# Optimization loop\n",
    "for step in range(10):\n",
    "    prob_params, std_g, nat_g = natural_grad_step(prob_params, data, lr=0.1)\n",
    "    \n",
    "    current_mu = prob_params['mu'][0]\n",
    "    current_sigma = jnp.exp(prob_params['log_sigma'][0])\n",
    "    \n",
    "    if step % 2 == 0:\n",
    "        print(f\"Step {step}: mu={current_mu:.3f}, sigma={current_sigma:.3f}\")\n",
    "\n",
    "print(f\"Final parameters: mu={current_mu:.3f}, sigma={current_sigma:.3f}\")\n",
    "print(f\"Parameter errors: mu_err={abs(current_mu - true_mu):.3f}, \"\n",
    "      f\"sigma_err={abs(current_sigma - true_sigma):.3f}\")\n",
    "```\n",
    "\n",
    "## Bayesian Neural Networks\n",
    "\n",
    "```python\n",
    "def create_bayesian_neural_network():\n",
    "    \"\"\"Create Bayesian Neural Network with variational inference\"\"\"\n",
    "    \n",
    "    def init_bnn_params(key, layer_sizes):\n",
    "        \"\"\"Initialize BNN parameters with variational distributions\"\"\"\n",
    "        keys = random.split(key, len(layer_sizes) - 1)\n",
    "        \n",
    "        params = []\n",
    "        for i, (in_size, out_size) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "            layer_key = keys[i]\n",
    "            w_key, b_key = random.split(layer_key)\n",
    "            \n",
    "            # Weight parameters: mean and log_std\n",
    "            w_mu = random.normal(w_key, (in_size, out_size)) * 0.1\n",
    "            w_log_std = jnp.full((in_size, out_size), -2.0)  # Small initial std\n",
    "            \n",
    "            # Bias parameters\n",
    "            b_mu = jnp.zeros(out_size)\n",
    "            b_log_std = jnp.full(out_size, -2.0)\n",
    "            \n",
    "            params.append({\n",
    "                'w_mu': w_mu, 'w_log_std': w_log_std,\n",
    "                'b_mu': b_mu, 'b_log_std': b_log_std\n",
    "            })\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def sample_bnn_weights(params, key):\n",
    "        \"\"\"Sample concrete weights from variational distributions\"\"\"\n",
    "        keys = random.split(key, len(params))\n",
    "        \n",
    "        sampled_params = []\n",
    "        for layer_params, layer_key in zip(params, keys):\n",
    "            w_key, b_key = random.split(layer_key)\n",
    "            \n",
    "            # Sample weights\n",
    "            w_eps = random.normal(w_key, layer_params['w_mu'].shape)\n",
    "            w = layer_params['w_mu'] + jnp.exp(layer_params['w_log_std']) * w_eps\n",
    "            \n",
    "            # Sample biases\n",
    "            b_eps = random.normal(b_key, layer_params['b_mu'].shape)\n",
    "            b = layer_params['b_mu'] + jnp.exp(layer_params['b_log_std']) * b_eps\n",
    "            \n",
    "            sampled_params.append({'w': w, 'b': b})\n",
    "        \n",
    "        return sampled_params\n",
    "    \n",
    "    def bnn_forward(sampled_params, x):\n",
    "        \"\"\"Forward pass through BNN with sampled weights\"\"\"\n",
    "        h = x\n",
    "        for i, layer in enumerate(sampled_params[:-1]):\n",
    "            h = jax.nn.relu(h @ layer['w'] + layer['b'])\n",
    "        \n",
    "        # Final layer (no activation)\n",
    "        output = h @ sampled_params[-1]['w'] + sampled_params[-1]['b']\n",
    "        return output\n",
    "    \n",
    "    def bnn_loss(params, key, x_batch, y_batch, n_samples=5):\n",
    "        \"\"\"BNN loss with KL divergence and likelihood terms\"\"\"\n",
    "        \n",
    "        # KL divergence between q(w) and prior p(w) = N(0, 1)\n",
    "        kl_loss = 0.0\n",
    "        for layer in params:\n",
    "            # KL for weights\n",
    "            w_mu, w_log_std = layer['w_mu'], layer['w_log_std']\n",
    "            w_var = jnp.exp(2 * w_log_std)\n",
    "            w_kl = 0.5 * jnp.sum(w_mu**2 + w_var - 1 - 2*w_log_std)\n",
    "            \n",
    "            # KL for biases  \n",
    "            b_mu, b_log_std = layer['b_mu'], layer['b_log_std']\n",
    "            b_var = jnp.exp(2 * b_log_std)\n",
    "            b_kl = 0.5 * jnp.sum(b_mu**2 + b_var - 1 - 2*b_log_std)\n",
    "            \n",
    "            kl_loss += w_kl + b_kl\n",
    "        \n",
    "        # Likelihood term via Monte Carlo sampling\n",
    "        keys = random.split(key, n_samples)\n",
    "        \n",
    "        def single_sample_loss(key):\n",
    "            sampled_weights = sample_bnn_weights(params, key)\n",
    "            pred = bnn_forward(sampled_weights, x_batch)\n",
    "            return jnp.mean((pred - y_batch) ** 2)\n",
    "        \n",
    "        likelihood_losses = vmap(single_sample_loss)(keys)\n",
    "        avg_likelihood_loss = jnp.mean(likelihood_losses)\n",
    "        \n",
    "        # Total ELBO loss\n",
    "        total_loss = avg_likelihood_loss + kl_loss / x_batch.shape[0]\n",
    "        \n",
    "        return total_loss, {\n",
    "            'likelihood_loss': avg_likelihood_loss,\n",
    "            'kl_loss': kl_loss,\n",
    "            'per_sample_losses': likelihood_losses\n",
    "        }\n",
    "    \n",
    "    def bnn_predict(params, key, x, n_samples=50):\n",
    "        \"\"\"BNN prediction with uncertainty quantification\"\"\"\n",
    "        keys = random.split(key, n_samples)\n",
    "        \n",
    "        def single_prediction(key):\n",
    "            sampled_weights = sample_bnn_weights(params, key)\n",
    "            return bnn_forward(sampled_weights, x)\n",
    "        \n",
    "        predictions = vmap(single_prediction)(keys)\n",
    "        \n",
    "        mean_pred = jnp.mean(predictions, axis=0)\n",
    "        std_pred = jnp.std(predictions, axis=0)\n",
    "        \n",
    "        return mean_pred, std_pred, predictions\n",
    "    \n",
    "    return init_bnn_params, bnn_loss, bnn_predict\n",
    "\n",
    "# Create and test Bayesian Neural Network\n",
    "init_bnn, bnn_loss_fn, bnn_predict_fn = create_bayesian_neural_network()\n",
    "\n",
    "# Initialize BNN\n",
    "layer_sizes = [1, 32, 32, 1]\n",
    "bnn_params = init_bnn(random.PRNGKey(444), layer_sizes)\n",
    "\n",
    "print(\"Bayesian Neural Network:\")\n",
    "print(f\"Layer sizes: {layer_sizes}\")\n",
    "print(f\"Number of variational parameters per layer:\")\n",
    "for i, layer in enumerate(bnn_params):\n",
    "    n_params = sum(param.size for param in layer.values())\n",
    "    print(f\"  Layer {i}: {n_params} parameters\")\n",
    "\n",
    "# Generate synthetic regression data\n",
    "key = random.PRNGKey(555)\n",
    "x_train = random.uniform(key, (50, 1)) * 4 - 2  # [-2, 2]\n",
    "y_train = x_train**2 + 0.1 * random.normal(random.split(key)[0], x_train.shape)\n",
    "\n",
    "# Test BNN forward pass\n",
    "loss, info = bnn_loss_fn(bnn_params, random.PRNGKey(666), x_train, y_train)\n",
    "print(f\"\\nBNN Loss Components:\")\n",
    "print(f\"Total loss: {loss:.4f}\")\n",
    "print(f\"Likelihood loss: {info['likelihood_loss']:.4f}\")\n",
    "print(f\"KL loss: {info['kl_loss']:.4f}\")\n",
    "\n",
    "# Test BNN prediction\n",
    "x_test = jnp.array([[-1.5], [0.0], [1.5]])\n",
    "mean_pred, std_pred, all_preds = bnn_predict_fn(bnn_params, random.PRNGKey(777), x_test)\n",
    "\n",
    "print(f\"\\nBNN Predictions (with uncertainty):\")\n",
    "for i, (x_val, mean_val, std_val) in enumerate(zip(x_test.flatten(), mean_pred.flatten(), std_pred.flatten())):\n",
    "    true_val = x_val**2\n",
    "    print(f\"x={x_val:4.1f}: pred={mean_val:6.3f}±{std_val:5.3f}, true={true_val:6.3f}\")\n",
    "\n",
    "# Training step for BNN\n",
    "def bnn_train_step(params, key, x_batch, y_batch, lr=0.01):\n",
    "    \"\"\"Single training step for BNN\"\"\"\n",
    "    loss, grads = jax.value_and_grad(lambda p: bnn_loss_fn(p, key, x_batch, y_batch)[0])(params)\n",
    "    new_params = jax.tree_map(lambda p, g: p - lr * g, params, grads)\n",
    "    return new_params, loss\n",
    "\n",
    "# Quick training demo\n",
    "print(\"\\nBNN Training:\")\n",
    "for epoch in range(5):\n",
    "    key = random.PRNGKey(epoch + 1000)\n",
    "    bnn_params, loss = bnn_train_step(bnn_params, key, x_train, y_train, lr=0.1)\n",
    "    print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we explored probabilistic programming and stochastic gradients in JAX:\n",
    "\n",
    "**Core Concepts:**\n",
    "- **Probability Distributions**: Sampling and density evaluation\n",
    "- **Stochastic Computation**: Graphs with randomness and noise\n",
    "- **Reparameterization Trick**: Making sampling differentiable  \n",
    "- **Gradient Estimation**: REINFORCE, Gumbel-Softmax, control variates\n",
    "\n",
    "**Variational Methods:**\n",
    "- **Variational Autoencoders**: Latent variable models with reparameterization\n",
    "- **Variational Inference**: Approximate posterior inference\n",
    "- **Natural Gradients**: Using Fisher information for efficient optimization\n",
    "- **Bayesian Neural Networks**: Weight uncertainty quantification\n",
    "\n",
    "**Gradient Estimation Techniques:**\n",
    "- **REINFORCE**: Likelihood ratio estimator for discrete variables\n",
    "- **Control Variates**: Variance reduction with baselines\n",
    "- **Gumbel-Softmax**: Continuous relaxation of discrete distributions\n",
    "- **Pathwise Derivatives**: Direct differentiation through continuous paths\n",
    "\n",
    "**Applications:**\n",
    "- **Generative Modeling**: VAEs and normalizing flows  \n",
    "- **Reinforcement Learning**: Policy gradient methods\n",
    "- **Uncertainty Quantification**: Bayesian neural networks\n",
    "- **Stochastic Optimization**: Natural gradient methods\n",
    "\n",
    "**Key Benefits:**\n",
    "- **End-to-End Differentiability**: Through stochastic computation graphs\n",
    "- **Variance Control**: Multiple techniques for reducing gradient variance  \n",
    "- **Uncertainty Quantification**: Principled handling of model uncertainty\n",
    "- **Flexible Modeling**: Support for complex probabilistic models\n",
    "\n",
    "Probabilistic programming in JAX enables sophisticated Bayesian modeling and inference while maintaining the efficiency and flexibility of automatic differentiation and just-in-time compilation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
