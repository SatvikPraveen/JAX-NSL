{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebf47140",
   "metadata": {},
   "source": [
    "# Location: notebooks/06_special_topics/19_research_tricks.ipynb\n",
    "\n",
    "## Advanced Research Tricks and Techniques in JAX\n",
    "\n",
    "This notebook covers advanced techniques and research tricks commonly used in machine learning research, including gradient manipulation, memory optimization, numerical tricks, and debugging strategies.\n",
    "\n",
    "## Advanced Gradient Manipulation\n",
    "\n",
    "```python\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, vmap, jit, random\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "# Gradient clipping and modification\n",
    "def create_gradient_manipulation_tools():\n",
    "    \"\"\"Create tools for advanced gradient manipulation\"\"\"\n",
    "    \n",
    "    def clip_gradients(grads, max_norm=1.0):\n",
    "        \"\"\"Clip gradients by global norm\"\"\"\n",
    "        # Compute global gradient norm\n",
    "        global_norm = jnp.sqrt(sum(jnp.sum(g**2) for g in jax.tree_leaves(grads)))\n",
    "        \n",
    "        # Clip if necessary\n",
    "        clip_factor = jnp.minimum(1.0, max_norm / (global_norm + 1e-8))\n",
    "        clipped_grads = jax.tree_map(lambda g: g * clip_factor, grads)\n",
    "        \n",
    "        return clipped_grads, global_norm\n",
    "    \n",
    "    def gradient_centralization(grads):\n",
    "        \"\"\"Apply gradient centralization to weight gradients\"\"\"\n",
    "        def centralize_grad(grad):\n",
    "            if len(grad.shape) >= 2:  # Weight matrix\n",
    "                # Center gradients\n",
    "                centered = grad - jnp.mean(grad, axis=tuple(range(1, len(grad.shape))), keepdims=True)\n",
    "                return centered\n",
    "            else:  # Bias vector or other\n",
    "                return grad\n",
    "        \n",
    "        return jax.tree_map(centralize_grad, grads)\n",
    "    \n",
    "    def gradient_noise_injection(grads, key, noise_scale=1e-3):\n",
    "        \"\"\"Add noise to gradients for improved generalization\"\"\"\n",
    "        def add_noise_to_grad(grad, key):\n",
    "            noise = random.normal(key, grad.shape) * noise_scale\n",
    "            return grad + noise\n",
    "        \n",
    "        keys = jax.tree_map(lambda g: random.split(key, g.size)[0], grads)\n",
    "        noisy_grads = jax.tree_map(add_noise_to_grad, grads, keys)\n",
    "        \n",
    "        return noisy_grads\n",
    "    \n",
    "    def gradient_standardization(grads, momentum=0.9, eps=1e-8):\n",
    "        \"\"\"Standardize gradients (similar to batch norm for gradients)\"\"\"\n",
    "        # This would typically maintain running statistics\n",
    "        # Simplified version for demonstration\n",
    "        \n",
    "        def standardize_grad(grad):\n",
    "            mean = jnp.mean(grad)\n",
    "            var = jnp.var(grad)\n",
    "            return (grad - mean) / jnp.sqrt(var + eps)\n",
    "        \n",
    "        return jax.tree_map(standardize_grad, grads)\n",
    "    \n",
    "    return clip_gradients, gradient_centralization, gradient_noise_injection, gradient_standardization\n",
    "\n",
    "# Test gradient manipulation\n",
    "clip_grads, center_grads, noise_grads, std_grads = create_gradient_manipulation_tools()\n",
    "\n",
    "# Create test gradients\n",
    "test_grads = {\n",
    "    'w1': jnp.array([[1.5, -2.0, 3.0], [0.5, -1.0, 2.5]]),\n",
    "    'b1': jnp.array([0.1, -0.3, 0.8]),\n",
    "    'w2': jnp.array([[10.0, -15.0], [5.0, -8.0], [12.0, -20.0]])\n",
    "}\n",
    "\n",
    "print(\"Original gradients:\")\n",
    "for name, grad in test_grads.items():\n",
    "    print(f\"  {name}: norm={jnp.linalg.norm(grad):.3f}\")\n",
    "\n",
    "# Test gradient clipping\n",
    "clipped_grads, global_norm = clip_grads(test_grads, max_norm=5.0)\n",
    "print(f\"\\nGradient clipping (max_norm=5.0):\")\n",
    "print(f\"  Global norm before: {global_norm:.3f}\")\n",
    "clipped_norm = jnp.sqrt(sum(jnp.sum(g**2) for g in jax.tree_leaves(clipped_grads)))\n",
    "print(f\"  Global norm after: {clipped_norm:.3f}\")\n",
    "\n",
    "# Test gradient centralization\n",
    "centered_grads = center_grads(test_grads)\n",
    "print(f\"\\nGradient centralization:\")\n",
    "for name, (orig, cent) in zip(test_grads.keys(), zip(test_grads.values(), centered_grads.values())):\n",
    "    if len(orig.shape) >= 2:\n",
    "        print(f\"  {name}: mean before={jnp.mean(orig):.3f}, after={jnp.mean(cent):.6f}\")\n",
    "\n",
    "# Test gradient noise injection\n",
    "noisy_grads = noise_grads(test_grads, random.PRNGKey(42), noise_scale=0.1)\n",
    "print(f\"\\nGradient noise injection:\")\n",
    "for name, (orig, noisy) in zip(test_grads.keys(), zip(test_grads.values(), noisy_grads.values())):\n",
    "    diff = jnp.linalg.norm(noisy - orig)\n",
    "    print(f\"  {name}: noise magnitude={diff:.3f}\")\n",
    "```\n",
    "\n",
    "## Memory Optimization Techniques\n",
    "\n",
    "```python\n",
    "def create_memory_optimization_tools():\n",
    "    \"\"\"Create tools for memory optimization in large models\"\"\"\n",
    "    \n",
    "    def gradient_checkpointing(fn, *args):\n",
    "        \"\"\"Manual gradient checkpointing implementation\"\"\"\n",
    "        # Store only forward pass inputs, recompute on backward pass\n",
    "        @jax.custom_vjp\n",
    "        def checkpointed_fn(*args):\n",
    "            return fn(*args)\n",
    "        \n",
    "        def checkpointed_fwd(*args):\n",
    "            # Forward pass - only store inputs\n",
    "            output = fn(*args)\n",
    "            return output, args  # Store args as residuals\n",
    "        \n",
    "        def checkpointed_bwd(residuals, output_grad):\n",
    "            args = residuals\n",
    "            # Recompute forward pass to get intermediates for backward\n",
    "            def inner_fn(*args):\n",
    "                return fn(*args)\n",
    "            _, vjp_fn = jax.vjp(inner_fn, *args)\n",
    "            return vjp_fn(output_grad)\n",
    "        \n",
    "        checkpointed_fn.defvjp(checkpointed_fwd, checkpointed_bwd)\n",
    "        return checkpointed_fn\n",
    "    \n",
    "    def activation_checkpointing_transformer_layer(params, x, layer_fn):\n",
    "        \"\"\"Checkpoint transformer layer activations\"\"\"\n",
    "        \n",
    "        @gradient_checkpointing\n",
    "        def checkpointed_layer(params, x):\n",
    "            return layer_fn(params, x)\n",
    "        \n",
    "        return checkpointed_layer(params, x)\n",
    "    \n",
    "    def mixed_precision_wrapper(fn):\n",
    "        \"\"\"Wrap function for mixed precision computation\"\"\"\n",
    "        def mixed_precision_fn(*args):\n",
    "            # Convert inputs to float16 for forward pass\n",
    "            fp16_args = jax.tree_map(\n",
    "                lambda x: x.astype(jnp.float16) if x.dtype == jnp.float32 else x,\n",
    "                args\n",
    "            )\n",
    "            \n",
    "            # Forward pass in fp16\n",
    "            output = fn(*fp16_args)\n",
    "            \n",
    "            # Convert output back to fp32 for stability\n",
    "            return jax.tree_map(\n",
    "                lambda x: x.astype(jnp.float32) if x.dtype == jnp.float16 else x,\n",
    "                output\n",
    "            )\n",
    "        \n",
    "        return mixed_precision_fn\n",
    "    \n",
    "    def reversible_layer(f, g):\n",
    "        \"\"\"Implement reversible/invertible layer for memory efficiency\"\"\"\n",
    "        def reversible_forward(x1, x2):\n",
    "            # RevNet forward: y1 = x1 + f(x2), y2 = x2 + g(y1)\n",
    "            y1 = x1 + f(x2)\n",
    "            y2 = x2 + g(y1)\n",
    "            return y1, y2\n",
    "        \n",
    "        def reversible_backward(y1, y2):\n",
    "            # Reverse computation: x2 = y2 - g(y1), x1 = y1 - f(x2)\n",
    "            x2 = y2 - g(y1)\n",
    "            x1 = y1 - f(x2)\n",
    "            return x1, x2\n",
    "        \n",
    "        return reversible_forward, reversible_backward\n",
    "    \n",
    "    return gradient_checkpointing, activation_checkpointing_transformer_layer, mixed_precision_wrapper, reversible_layer\n",
    "\n",
    "# Test memory optimization tools\n",
    "(grad_checkpoint, checkpoint_transformer, \n",
    " mixed_precision, reversible) = create_memory_optimization_tools()\n",
    "\n",
    "# Example: Memory-efficient large matrix multiplication\n",
    "def large_matmul(A, B):\n",
    "    \"\"\"Large matrix multiplication with potential memory issues\"\"\"\n",
    "    intermediate = jnp.tanh(A @ B)  # Large intermediate\n",
    "    return jnp.sum(intermediate ** 2)\n",
    "\n",
    "# Create large test matrices\n",
    "key = random.PRNGKey(123)\n",
    "A = random.normal(key, (1000, 800))\n",
    "B = random.normal(random.split(key)[0], (800, 600))\n",
    "\n",
    "# Compare memory usage (conceptually)\n",
    "regular_fn = large_matmul\n",
    "checkpointed_fn = grad_checkpoint(large_matmul)\n",
    "\n",
    "print(\"Memory Optimization Example:\")\n",
    "\n",
    "# Regular computation\n",
    "result1 = regular_fn(A, B)\n",
    "grad_fn1 = jax.grad(regular_fn)\n",
    "grads1 = grad_fn1(A, B)\n",
    "\n",
    "print(f\"Regular computation result: {result1:.6f}\")\n",
    "print(f\"Regular gradient norm: {jnp.linalg.norm(grads1):.6f}\")\n",
    "\n",
    "# Checkpointed computation  \n",
    "result2 = checkpointed_fn(A, B)\n",
    "grad_fn2 = jax.grad(checkpointed_fn)\n",
    "grads2 = grad_fn2(A, B)\n",
    "\n",
    "print(f\"Checkpointed result: {result2:.6f}\")\n",
    "print(f\"Checkpointed gradient norm: {jnp.linalg.norm(grads2):.6f}\")\n",
    "print(f\"Results match: {jnp.allclose(result1, result2)}\")\n",
    "print(f\"Gradients match: {jnp.allclose(grads1, grads2)}\")\n",
    "\n",
    "# Test mixed precision\n",
    "mp_fn = mixed_precision_wrapper(large_matmul)\n",
    "result3 = mp_fn(A, B)\n",
    "print(f\"Mixed precision result: {result3:.6f}\")\n",
    "```\n",
    "\n",
    "## Numerical Stability Tricks\n",
    "\n",
    "```python\n",
    "def create_numerical_stability_tools():\n",
    "    \"\"\"Create tools for numerical stability\"\"\"\n",
    "    \n",
    "    def stable_softmax(logits, axis=-1):\n",
    "        \"\"\"Numerically stable softmax implementation\"\"\"\n",
    "        # Subtract max for stability\n",
    "        shifted_logits = logits - jnp.max(logits, axis=axis, keepdims=True)\n",
    "        exp_shifted = jnp.exp(shifted_logits)\n",
    "        return exp_shifted / jnp.sum(exp_shifted, axis=axis, keepdims=True)\n",
    "    \n",
    "    def log_sum_exp(logits, axis=-1):\n",
    "        \"\"\"Stable log-sum-exp computation\"\"\"\n",
    "        max_logits = jnp.max(logits, axis=axis, keepdims=True)\n",
    "        return max_logits + jnp.log(jnp.sum(jnp.exp(logits - max_logits), axis=axis, keepdims=True))\n",
    "    \n",
    "    def stable_log_softmax(logits, axis=-1):\n",
    "        \"\"\"Stable log-softmax using log-sum-exp\"\"\"\n",
    "        return logits - log_sum_exp(logits, axis=axis)\n",
    "    \n",
    "    def gelu_stable(x):\n",
    "        \"\"\"Numerically stable GELU activation\"\"\"\n",
    "        # GELU(x) = 0.5 * x * (1 + tanh(sqrt(2/π) * (x + 0.044715 * x^3)))\n",
    "        # Use more stable implementation\n",
    "        return 0.5 * x * (1.0 + jax.nn.tanh(jnp.sqrt(2.0 / jnp.pi) * (x + 0.044715 * x**3)))\n",
    "    \n",
    "    def swish_stable(x, beta=1.0):\n",
    "        \"\"\"Stable Swish activation\"\"\"\n",
    "        # Swish(x) = x * sigmoid(β*x)\n",
    "        # Use stable sigmoid implementation\n",
    "        return x * jax.nn.sigmoid(beta * x)\n",
    "    \n",
    "    def layer_norm_stable(x, gamma, beta, eps=1e-6):\n",
    "        \"\"\"Numerically stable layer normalization\"\"\"\n",
    "        mean = jnp.mean(x, axis=-1, keepdims=True)\n",
    "        var = jnp.var(x, axis=-1, keepdims=True)\n",
    "        \n",
    "        # Stable normalization\n",
    "        normalized = (x - mean) / jnp.sqrt(var + eps)\n",
    "        return gamma * normalized + beta\n",
    "    \n",
    "    def attention_weights_stable(query, key, scale=None):\n",
    "        \"\"\"Stable attention weight computation\"\"\"\n",
    "        if scale is None:\n",
    "            scale = 1.0 / jnp.sqrt(query.shape[-1])\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = jnp.einsum('...qd,...kd->...qk', query, key) * scale\n",
    "        \n",
    "        # Stable softmax over key dimension\n",
    "        weights = stable_softmax(scores, axis=-1)\n",
    "        \n",
    "        return weights, scores\n",
    "    \n",
    "    return (stable_softmax, log_sum_exp, stable_log_softmax, \n",
    "            gelu_stable, swish_stable, layer_norm_stable, attention_weights_stable)\n",
    "\n",
    "# Test numerical stability tools\n",
    "(stable_softmax, log_sum_exp, stable_log_softmax,\n",
    " gelu_stable, swish_stable, layer_norm_stable, attention_stable) = create_numerical_stability_tools()\n",
    "\n",
    "# Test with extreme values\n",
    "extreme_logits = jnp.array([100.0, 101.0, 99.0, 102.0])\n",
    "print(\"Numerical Stability Tests:\")\n",
    "\n",
    "# Compare regular vs stable softmax\n",
    "regular_softmax = lambda x: jnp.exp(x) / jnp.sum(jnp.exp(x))\n",
    "try:\n",
    "    regular_result = regular_softmax(extreme_logits)\n",
    "    print(f\"Regular softmax: {regular_result}\")\n",
    "except:\n",
    "    print(\"Regular softmax: OVERFLOW ERROR\")\n",
    "\n",
    "stable_result = stable_softmax(extreme_logits)\n",
    "print(f\"Stable softmax: {stable_result}\")\n",
    "print(f\"Stable softmax sums to: {jnp.sum(stable_result)}\")\n",
    "\n",
    "# Test log-sum-exp\n",
    "lse_result = log_sum_exp(extreme_logits)\n",
    "print(f\"Log-sum-exp: {lse_result}\")\n",
    "\n",
    "# Test stable activations with extreme inputs\n",
    "extreme_inputs = jnp.array([-50.0, -10.0, 0.0, 10.0, 50.0])\n",
    "\n",
    "gelu_results = gelu_stable(extreme_inputs)\n",
    "swish_results = swish_stable(extreme_inputs)\n",
    "\n",
    "print(f\"\\nActivation functions on extreme inputs:\")\n",
    "print(f\"GELU: {gelu_results}\")\n",
    "print(f\"Swish: {swish_results}\")\n",
    "\n",
    "# Test stable layer norm\n",
    "layer_input = jnp.array([[1e6, 1e6 + 1, 1e6 + 2], [1e-6, 2e-6, 3e-6]])\n",
    "gamma = jnp.ones(3)\n",
    "beta = jnp.zeros(3)\n",
    "\n",
    "ln_result = layer_norm_stable(layer_input, gamma, beta)\n",
    "print(f\"\\nLayer norm on extreme scale inputs:\")\n",
    "print(f\"Input scale: {jnp.std(layer_input, axis=-1)}\")\n",
    "print(f\"Output scale: {jnp.std(ln_result, axis=-1)}\")\n",
    "print(f\"Output mean: {jnp.mean(ln_result, axis=-1)}\")\n",
    "```\n",
    "\n",
    "## Advanced Debugging and Profiling\n",
    "\n",
    "```python\n",
    "def create_debugging_tools():\n",
    "    \"\"\"Create advanced debugging tools for JAX\"\"\"\n",
    "    \n",
    "    def debug_callback(name, x, print_shape=True, print_stats=True):\n",
    "        \"\"\"Debug callback that prints tensor information\"\"\"\n",
    "        def _debug_callback(x):\n",
    "            print(f\"\\n=== DEBUG: {name} ===\")\n",
    "            if print_shape:\n",
    "                print(f\"Shape: {x.shape}\")\n",
    "                print(f\"Dtype: {x.dtype}\")\n",
    "            if print_stats:\n",
    "                print(f\"Min: {jnp.min(x):.6f}\")\n",
    "                print(f\"Max: {jnp.max(x):.6f}\")\n",
    "                print(f\"Mean: {jnp.mean(x):.6f}\")\n",
    "                print(f\"Std: {jnp.std(x):.6f}\")\n",
    "                print(f\"Has NaN: {jnp.any(jnp.isnan(x))}\")\n",
    "                print(f\"Has Inf: {jnp.any(jnp.isinf(x))}\")\n",
    "            return x\n",
    "        \n",
    "        return jax.debug.callback(_debug_callback, x)\n",
    "    \n",
    "    def gradient_debug_wrapper(fn):\n",
    "        \"\"\"Wrapper that debugs gradients\"\"\"\n",
    "        def debug_grad_fn(*args, **kwargs):\n",
    "            # Forward pass\n",
    "            result = fn(*args, **kwargs)\n",
    "            \n",
    "            # Gradient computation with debugging\n",
    "            def loss_fn(*args):\n",
    "                out = fn(*args, **kwargs)\n",
    "                return jnp.sum(out) if hasattr(out, 'shape') else out\n",
    "            \n",
    "            grads = jax.grad(loss_fn)(*args)\n",
    "            \n",
    "            # Debug gradient information\n",
    "            print(f\"\\n=== GRADIENT DEBUG ===\")\n",
    "            flat_grads = jax.tree_leaves(grads)\n",
    "            grad_norms = [jnp.linalg.norm(g) for g in flat_grads]\n",
    "            \n",
    "            print(f\"Number of gradient tensors: {len(flat_grads)}\")\n",
    "            print(f\"Gradient norms: {grad_norms}\")\n",
    "            print(f\"Total gradient norm: {jnp.sqrt(sum(norm**2 for norm in grad_norms))}\")\n",
    "            \n",
    "            # Check for problematic gradients\n",
    "            for i, (g, norm) in enumerate(zip(flat_grads, grad_norms)):\n",
    "                if jnp.any(jnp.isnan(g)):\n",
    "                    print(f\"  WARNING: Gradient {i} contains NaN!\")\n",
    "                if jnp.any(jnp.isinf(g)):\n",
    "                    print(f\"  WARNING: Gradient {i} contains Inf!\")\n",
    "                if norm > 100:\n",
    "                    print(f\"  WARNING: Large gradient norm in tensor {i}: {norm}\")\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        return debug_grad_fn\n",
    "    \n",
    "    def profile_function(fn, *args, n_runs=10, warmup=3):\n",
    "        \"\"\"Profile function execution time\"\"\"\n",
    "        # Warmup runs\n",
    "        for _ in range(warmup):\n",
    "            _ = fn(*args)\n",
    "            \n",
    "        # Timing runs\n",
    "        times = []\n",
    "        for _ in range(n_runs):\n",
    "            start_time = time.time()\n",
    "            result = fn(*args)\n",
    "            if hasattr(result, 'block_until_ready'):\n",
    "                result.block_until_ready()\n",
    "            end_time = time.time()\n",
    "            times.append(end_time - start_time)\n",
    "        \n",
    "        times = jnp.array(times)\n",
    "        return {\n",
    "            'mean_time': jnp.mean(times),\n",
    "            'std_time': jnp.std(times),\n",
    "            'min_time': jnp.min(times),\n",
    "            'max_time': jnp.max(times),\n",
    "            'all_times': times\n",
    "        }\n",
    "    \n",
    "    def memory_profiler():\n",
    "        \"\"\"Simple memory usage profiler\"\"\"\n",
    "        # This is a placeholder - actual memory profiling requires external tools\n",
    "        import gc\n",
    "        import psutil\n",
    "        import os\n",
    "        \n",
    "        process = psutil.Process(os.getpid())\n",
    "        memory_info = process.memory_info()\n",
    "        \n",
    "        return {\n",
    "            'rss_mb': memory_info.rss / 1024 / 1024,  # Resident set size\n",
    "            'vms_mb': memory_info.vms / 1024 / 1024,  # Virtual memory size\n",
    "        }\n",
    "    \n",
    "    def nan_detector(fn):\n",
    "        \"\"\"Wrapper that detects NaN/Inf in computation\"\"\"\n",
    "        def nan_detect_fn(*args, **kwargs):\n",
    "            result = fn(*args, **kwargs)\n",
    "            \n",
    "            def check_tensor(x, name=\"tensor\"):\n",
    "                if hasattr(x, 'shape'):\n",
    "                    if jnp.any(jnp.isnan(x)):\n",
    "                        raise ValueError(f\"NaN detected in {name}\")\n",
    "                    if jnp.any(jnp.isinf(x)):\n",
    "                        raise ValueError(f\"Inf detected in {name}\")\n",
    "                return x\n",
    "            \n",
    "            # Check all outputs\n",
    "            jax.tree_map(check_tensor, result)\n",
    "            return result\n",
    "        \n",
    "        return nan_detect_fn\n",
    "    \n",
    "    return debug_callback, gradient_debug_wrapper, profile_function, memory_profiler, nan_detector\n",
    "\n",
    "# Test debugging tools\n",
    "debug_cb, grad_debug, profile_fn, mem_profile, nan_detect = create_debugging_tools()\n",
    "\n",
    "# Example function to debug\n",
    "def test_function(x, w):\n",
    "    \"\"\"Test function for debugging\"\"\"\n",
    "    # Add debug point\n",
    "    x = debug_cb(\"input\", x)\n",
    "    \n",
    "    # Some computation\n",
    "    h1 = jax.nn.relu(x @ w)\n",
    "    h1 = debug_cb(\"after_relu\", h1)\n",
    "    \n",
    "    # Potential numerical instability\n",
    "    h2 = jnp.exp(h1) / (jnp.sum(jnp.exp(h1)) + 1e-8)\n",
    "    h2 = debug_cb(\"after_softmax\", h2)\n",
    "    \n",
    "    return jnp.sum(h2)\n",
    "\n",
    "# Test debugging\n",
    "print(\"=== DEBUGGING DEMO ===\")\n",
    "test_x = random.normal(random.PRNGKey(789), (32, 64))\n",
    "test_w = random.normal(random.PRNGKey(790), (64, 128))\n",
    "\n",
    "# Use debug wrapper\n",
    "debug_fn = grad_debug(test_function)\n",
    "result = debug_fn(test_x, test_w)\n",
    "\n",
    "# Profile the function\n",
    "print(\"\\n=== PROFILING DEMO ===\")\n",
    "profile_results = profile_fn(test_function, test_x, test_w, n_runs=5)\n",
    "print(f\"Mean execution time: {profile_results['mean_time']:.4f} ± {profile_results['std_time']:.4f} seconds\")\n",
    "print(f\"Min/Max time: {profile_results['min_time']:.4f}/{profile_results['max_time']:.4f} seconds\")\n",
    "\n",
    "# Memory profiling\n",
    "mem_before = mem_profile()\n",
    "large_array = jnp.zeros((10000, 10000))  # Allocate large array\n",
    "mem_after = mem_profile()\n",
    "\n",
    "print(f\"\\nMemory usage:\")\n",
    "print(f\"Before: {mem_before['rss_mb']:.1f} MB\")\n",
    "print(f\"After: {mem_after['rss_mb']:.1f} MB\")\n",
    "print(f\"Difference: {mem_after['rss_mb'] - mem_before['rss_mb']:.1f} MB\")\n",
    "\n",
    "# Test NaN detector\n",
    "safe_fn = nan_detect(test_function)\n",
    "result_safe = safe_fn(test_x, test_w)\n",
    "print(f\"\\nNaN detection passed: {result_safe:.6f}\")\n",
    "```\n",
    "\n",
    "## Research-Specific Optimization Tricks\n",
    "\n",
    "```python\n",
    "def create_research_optimization_tricks():\n",
    "    \"\"\"Advanced optimization tricks for research\"\"\"\n",
    "    \n",
    "    def cosine_annealing_schedule(step, total_steps, lr_max, lr_min=0.0):\n",
    "        \"\"\"Cosine annealing learning rate schedule\"\"\"\n",
    "        cosine_decay = 0.5 * (1 + jnp.cos(jnp.pi * step / total_steps))\n",
    "        return lr_min + (lr_max - lr_min) * cosine_decay\n",
    "    \n",
    "    def warmup_cosine_schedule(step, warmup_steps, total_steps, lr_max, lr_min=0.0):\n",
    "        \"\"\"Warmup followed by cosine annealing\"\"\"\n",
    "        def warmup_phase():\n",
    "            return lr_max * step / warmup_steps\n",
    "        \n",
    "        def cosine_phase():\n",
    "            cosine_steps = total_steps - warmup_steps\n",
    "            cosine_step = step - warmup_steps\n",
    "            cosine_decay = 0.5 * (1 + jnp.cos(jnp.pi * cosine_step / cosine_steps))\n",
    "            return lr_min + (lr_max - lr_min) * cosine_decay\n",
    "        \n",
    "        return jnp.where(step < warmup_steps, warmup_phase(), cosine_phase())\n",
    "    \n",
    "    def lookahead_optimizer(base_optimizer_state, params, grads, k=5, alpha=0.5):\n",
    "        \"\"\"Lookahead optimizer wrapper\"\"\"\n",
    "        # This is a simplified version - full implementation would track slow weights\n",
    "        \n",
    "        # Base optimizer update\n",
    "        new_base_state = base_optimizer_state  # Placeholder\n",
    "        fast_weights = jax.tree_map(lambda p, g: p - 0.01 * g, params, grads)  # Simplified SGD\n",
    "        \n",
    "        # Lookahead update (every k steps)\n",
    "        step = base_optimizer_state.get('step', 0) + 1\n",
    "        \n",
    "        def lookahead_update():\n",
    "            # Interpolate between fast and slow weights\n",
    "            slow_weights = base_optimizer_state.get('slow_weights', params)\n",
    "            new_slow_weights = jax.tree_map(\n",
    "                lambda slow, fast: slow + alpha * (fast - slow),\n",
    "                slow_weights, fast_weights\n",
    "            )\n",
    "            return new_slow_weights, {'slow_weights': new_slow_weights, 'step': step}\n",
    "        \n",
    "        def regular_update():\n",
    "            return fast_weights, {'slow_weights': base_optimizer_state.get('slow_weights', params), 'step': step}\n",
    "        \n",
    "        return jnp.where(step % k == 0, lookahead_update(), regular_update())\n",
    "    \n",
    "    def spectral_normalization(w, u=None, power_iterations=1):\n",
    "        \"\"\"Spectral normalization for weight matrices\"\"\"\n",
    "        if u is None:\n",
    "            u = random.normal(random.PRNGKey(42), (w.shape[0],))\n",
    "            u = u / jnp.linalg.norm(u)\n",
    "        \n",
    "        # Power iteration to find top singular value\n",
    "        for _ in range(power_iterations):\n",
    "            v = w.T @ u\n",
    "            v = v / jnp.linalg.norm(v)\n",
    "            u = w @ v\n",
    "            u = u / jnp.linalg.norm(u)\n",
    "        \n",
    "        # Compute spectral norm\n",
    "        sigma = jnp.dot(u, w @ v)\n",
    "        \n",
    "        # Normalize weights\n",
    "        w_sn = w / sigma\n",
    "        \n",
    "        return w_sn, u, sigma\n",
    "    \n",
    "    def orthogonal_regularization(params, reg_strength=1e-4):\n",
    "        \"\"\"Orthogonal regularization for weight matrices\"\"\"\n",
    "        reg_loss = 0.0\n",
    "        \n",
    "        for param in jax.tree_leaves(params):\n",
    "            if len(param.shape) == 2 and min(param.shape) > 1:  # Matrix\n",
    "                # Compute W^T W - I\n",
    "                wtw = param.T @ param\n",
    "                identity = jnp.eye(wtw.shape[0])\n",
    "                ortho_loss = jnp.sum((wtw - identity) ** 2)\n",
    "                reg_loss += ortho_loss\n",
    "        \n",
    "        return reg_strength * reg_loss\n",
    "    \n",
    "    def feature_matching_loss(real_features, fake_features):\n",
    "        \"\"\"Feature matching loss for GANs\"\"\"\n",
    "        return jnp.mean((jnp.mean(real_features, axis=0) - jnp.mean(fake_features, axis=0)) ** 2)\n",
    "    \n",
    "    def progressive_training_schedule(step, stages, stage_lengths):\n",
    "        \"\"\"Progressive training with different stages\"\"\"\n",
    "        cumulative_steps = jnp.cumsum(jnp.array(stage_lengths))\n",
    "        current_stage = jnp.sum(step >= cumulative_steps)\n",
    "        current_stage = jnp.clip(current_stage, 0, len(stages) - 1)\n",
    "        \n",
    "        return stages[current_stage]\n",
    "    \n",
    "    return (cosine_annealing_schedule, warmup_cosine_schedule, lookahead_optimizer,\n",
    "            spectral_normalization, orthogonal_regularization, feature_matching_loss,\n",
    "            progressive_training_schedule)\n",
    "\n",
    "# Test research optimization tricks\n",
    "(cosine_schedule, warmup_cosine, lookahead_opt, spectral_norm,\n",
    " ortho_reg, feature_match, progressive_schedule) = create_research_optimization_tricks()\n",
    "\n",
    "print(\"=== RESEARCH OPTIMIZATION TRICKS ===\")\n",
    "\n",
    "# Test learning rate schedules\n",
    "steps = jnp.arange(0, 1000, 10)\n",
    "cosine_lrs = [cosine_schedule(s, 1000, 0.1, 0.001) for s in steps]\n",
    "warmup_lrs = [warmup_cosine(s, 100, 1000, 0.1, 0.001) for s in steps]\n",
    "\n",
    "print(\"Learning rate schedules:\")\n",
    "print(f\"Cosine at steps [0, 250, 500, 750, 1000]: {[cosine_schedule(s, 1000, 0.1) for s in [0, 250, 500, 750, 1000]]}\")\n",
    "print(f\"Warmup+cosine at same steps: {[warmup_cosine(s, 100, 1000, 0.1) for s in [0, 250, 500, 750, 1000]]}\")\n",
    "\n",
    "# Test spectral normalization\n",
    "test_weight = random.normal(random.PRNGKey(999), (64, 32))\n",
    "w_sn, u_vec, sigma = spectral_norm(test_weight)\n",
    "\n",
    "print(f\"\\nSpectral normalization:\")\n",
    "print(f\"Original spectral norm: {sigma:.4f}\")\n",
    "print(f\"Normalized weight spectral norm: {jnp.linalg.norm(w_sn, ord=2):.4f}\")\n",
    "\n",
    "# Test orthogonal regularization\n",
    "test_params = {\n",
    "    'w1': random.normal(random.PRNGKey(1001), (50, 50)),\n",
    "    'w2': random.normal(random.PRNGKey(1002), (30, 40)),\n",
    "    'b1': jnp.zeros(50)\n",
    "}\n",
    "\n",
    "ortho_loss = ortho_reg(test_params)\n",
    "print(f\"Orthogonal regularization loss: {ortho_loss:.6f}\")\n",
    "\n",
    "# Test feature matching\n",
    "real_features = random.normal(random.PRNGKey(1003), (100, 256))\n",
    "fake_features = random.normal(random.PRNGKey(1004), (100, 256))\n",
    "fm_loss = feature_match(real_features, fake_features)\n",
    "print(f\"Feature matching loss: {fm_loss:.6f}\")\n",
    "\n",
    "# Test progressive training\n",
    "stages = ['warmup', 'normal', 'fine_tune']\n",
    "stage_lengths = [100, 500, 400]\n",
    "\n",
    "print(f\"\\nProgressive training stages:\")\n",
    "for step in [50, 300, 800]:\n",
    "    stage = progressive_schedule(step, stages, stage_lengths)\n",
    "    print(f\"Step {step}: Stage {stage}\")\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we explored advanced research tricks and techniques in JAX:\n",
    "\n",
    "**Gradient Manipulation:**\n",
    "- **Gradient Clipping**: Global norm clipping for training stability\n",
    "- **Gradient Centralization**: Centering gradients for better optimization\n",
    "- **Gradient Noise**: Adding noise for generalization\n",
    "- **Gradient Standardization**: Normalizing gradient distributions\n",
    "\n",
    "**Memory Optimization:**\n",
    "- **Gradient Checkpointing**: Trading computation for memory\n",
    "- **Mixed Precision**: Using float16 for memory efficiency\n",
    "- **Reversible Layers**: Invertible architectures\n",
    "- **Activation Checkpointing**: Strategic intermediate storage\n",
    "\n",
    "**Numerical Stability:**\n",
    "- **Stable Softmax**: Avoiding overflow in attention mechanisms  \n",
    "- **Log-Sum-Exp**: Numerically stable logarithmic computations\n",
    "- **Stable Activations**: GELU, Swish with numerical safeguards\n",
    "- **Stable Normalization**: Layer norm with epsilon protection\n",
    "\n",
    "**Debugging and Profiling:**\n",
    "- **Debug Callbacks**: Runtime tensor inspection\n",
    "- **Gradient Debugging**: Automatic gradient analysis\n",
    "- **Performance Profiling**: Execution time measurement\n",
    "- **NaN/Inf Detection**: Automatic numerical error catching\n",
    "\n",
    "**Advanced Optimization:**\n",
    "- **Learning Rate Schedules**: Cosine annealing with warmup\n",
    "- **Lookahead Optimizer**: Slow-fast weight interpolation\n",
    "- **Spectral Normalization**: Lipschitz constraint enforcement\n",
    "- **Orthogonal Regularization**: Weight matrix orthogonality\n",
    "- **Progressive Training**: Multi-stage training protocols\n",
    "\n",
    "**Research Applications:**\n",
    "- **GAN Training**: Feature matching and spectral normalization\n",
    "- **Large Model Training**: Memory optimization and numerical stability\n",
    "- **Transformer Training**: Gradient clipping and learning rate schedules\n",
    "- **Experimental Debugging**: Comprehensive diagnostic tools\n",
    "\n",
    "These techniques are essential for pushing the boundaries of what's possible in machine learning research while maintaining numerical stability and computational efficiency in JAX."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
