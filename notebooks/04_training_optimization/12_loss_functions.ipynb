{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eafde51c",
   "metadata": {},
   "source": [
    "# File: notebooks/04_training_optimization/12_loss_functions.ipynb\n",
    "\n",
    "## JAX Training Optimization: Loss Functions\n",
    "\n",
    "This notebook implements various loss functions from scratch in JAX, covering regression losses, classification losses, ranking losses, and regularization techniques. We'll explore their mathematical properties, numerical stability considerations, and practical applications.\n",
    "\n",
    "Loss functions define the objective that guides neural network training, making their proper implementation and selection crucial for model performance and training stability.\n",
    "\n",
    "## Setting Up the Environment\n",
    "\n",
    "```python\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, random\n",
    "from jax.nn import sigmoid, softmax, log_softmax, logsumexp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Callable\n",
    "import functools\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "```\n",
    "\n",
    "## Regression Losses\n",
    "\n",
    "### Mean Squared Error and Variants\n",
    "\n",
    "```python\n",
    "def mse_loss(predictions, targets, reduction='mean'):\n",
    "    \"\"\"Mean Squared Error loss\"\"\"\n",
    "    squared_errors = (predictions - targets) ** 2\n",
    "    \n",
    "    if reduction == 'mean':\n",
    "        return jnp.mean(squared_errors)\n",
    "    elif reduction == 'sum':\n",
    "        return jnp.sum(squared_errors)\n",
    "    elif reduction == 'none':\n",
    "        return squared_errors\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown reduction: {reduction}\")\n",
    "\n",
    "def rmse_loss(predictions, targets):\n",
    "    \"\"\"Root Mean Squared Error loss\"\"\"\n",
    "    return jnp.sqrt(mse_loss(predictions, targets))\n",
    "\n",
    "def mae_loss(predictions, targets, reduction='mean'):\n",
    "    \"\"\"Mean Absolute Error (L1) loss\"\"\"\n",
    "    absolute_errors = jnp.abs(predictions - targets)\n",
    "    \n",
    "    if reduction == 'mean':\n",
    "        return jnp.mean(absolute_errors)\n",
    "    elif reduction == 'sum':\n",
    "        return jnp.sum(absolute_errors)\n",
    "    elif reduction == 'none':\n",
    "        return absolute_errors\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown reduction: {reduction}\")\n",
    "\n",
    "def huber_loss(predictions, targets, delta=1.0, reduction='mean'):\n",
    "    \"\"\"Huber loss (smooth L1 loss) - robust to outliers\"\"\"\n",
    "    residual = jnp.abs(predictions - targets)\n",
    "    \n",
    "    # Quadratic for small errors, linear for large errors\n",
    "    loss = jnp.where(\n",
    "        residual <= delta,\n",
    "        0.5 * residual ** 2,\n",
    "        delta * (residual - 0.5 * delta)\n",
    "    )\n",
    "    \n",
    "    if reduction == 'mean':\n",
    "        return jnp.mean(loss)\n",
    "    elif reduction == 'sum':\n",
    "        return jnp.sum(loss)\n",
    "    elif reduction == 'none':\n",
    "        return loss\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown reduction: {reduction}\")\n",
    "\n",
    "def quantile_loss(predictions, targets, quantile=0.5, reduction='mean'):\n",
    "    \"\"\"Quantile loss for quantile regression\"\"\"\n",
    "    errors = targets - predictions\n",
    "    loss = jnp.where(\n",
    "        errors >= 0,\n",
    "        quantile * errors,\n",
    "        (quantile - 1) * errors\n",
    "    )\n",
    "    \n",
    "    if reduction == 'mean':\n",
    "        return jnp.mean(loss)\n",
    "    elif reduction == 'sum':\n",
    "        return jnp.sum(loss)\n",
    "    elif reduction == 'none':\n",
    "        return loss\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown reduction: {reduction}\")\n",
    "\n",
    "def test_regression_losses():\n",
    "    \"\"\"Test regression loss functions\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(42)\n",
    "    n_samples = 100\n",
    "    \n",
    "    # Create regression data with outliers\n",
    "    true_values = random.normal(key, (n_samples,))\n",
    "    predictions = true_values + 0.1 * random.normal(random.split(key)[1], (n_samples,))\n",
    "    \n",
    "    # Add some outliers\n",
    "    outlier_mask = random.bernoulli(random.split(key, 3)[2], 0.1, (n_samples,))\n",
    "    predictions = jnp.where(outlier_mask, predictions + 3.0, predictions)\n",
    "    \n",
    "    print(\"Regression Loss Functions:\")\n",
    "    print(f\"MSE Loss: {mse_loss(predictions, true_values):.4f}\")\n",
    "    print(f\"RMSE Loss: {rmse_loss(predictions, true_values):.4f}\")\n",
    "    print(f\"MAE Loss: {mae_loss(predictions, true_values):.4f}\")\n",
    "    print(f\"Huber Loss (δ=1.0): {huber_loss(predictions, true_values, delta=1.0):.4f}\")\n",
    "    print(f\"Huber Loss (δ=0.1): {huber_loss(predictions, true_values, delta=0.1):.4f}\")\n",
    "    print(f\"Quantile Loss (50%): {quantile_loss(predictions, true_values, quantile=0.5):.4f}\")\n",
    "    print(f\"Quantile Loss (90%): {quantile_loss(predictions, true_values, quantile=0.9):.4f}\")\n",
    "    \n",
    "    # Visualize loss behavior\n",
    "    errors = jnp.linspace(-3, 3, 100)\n",
    "    zero_targets = jnp.zeros_like(errors)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Compare different regression losses\n",
    "    ax1.plot(errors, mse_loss(errors, zero_targets, 'none'), label='MSE', linewidth=2)\n",
    "    ax1.plot(errors, mae_loss(errors, zero_targets, 'none'), label='MAE', linewidth=2)\n",
    "    ax1.plot(errors, huber_loss(errors, zero_targets, delta=1.0, reduction='none'), label='Huber (δ=1)', linewidth=2)\n",
    "    ax1.plot(errors, huber_loss(errors, zero_targets, delta=0.5, reduction='none'), label='Huber (δ=0.5)', linewidth=2)\n",
    "    ax1.set_xlabel('Error (prediction - target)')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Regression Loss Functions')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Compare quantile losses\n",
    "    for q in [0.1, 0.5, 0.9]:\n",
    "        ax2.plot(errors, quantile_loss(errors, zero_targets, quantile=q, reduction='none'), \n",
    "                label=f'Quantile {q}', linewidth=2)\n",
    "    ax2.set_xlabel('Error (prediction - target)')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_title('Quantile Loss Functions')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "test_regression_losses()\n",
    "```\n",
    "\n",
    "## Classification Losses\n",
    "\n",
    "### Cross-Entropy and Variants\n",
    "\n",
    "```python\n",
    "def binary_cross_entropy(logits, targets, from_logits=True):\n",
    "    \"\"\"Binary cross-entropy loss\"\"\"\n",
    "    if from_logits:\n",
    "        # Numerically stable implementation using log-sum-exp\n",
    "        loss = jnp.maximum(logits, 0) - logits * targets + jnp.log1p(jnp.exp(-jnp.abs(logits)))\n",
    "    else:\n",
    "        # Assumes probabilities, add epsilon for numerical stability\n",
    "        eps = 1e-15\n",
    "        probs = jnp.clip(logits, eps, 1 - eps)\n",
    "        loss = -(targets * jnp.log(probs) + (1 - targets) * jnp.log(1 - probs))\n",
    "    \n",
    "    return jnp.mean(loss)\n",
    "\n",
    "def categorical_cross_entropy(logits, targets, from_logits=True, label_smoothing=0.0):\n",
    "    \"\"\"Categorical cross-entropy loss with optional label smoothing\"\"\"\n",
    "    if from_logits:\n",
    "        log_probs = log_softmax(logits)\n",
    "    else:\n",
    "        # Assumes probabilities\n",
    "        eps = 1e-15\n",
    "        probs = jnp.clip(logits, eps, 1 - eps)\n",
    "        log_probs = jnp.log(probs)\n",
    "    \n",
    "    # Apply label smoothing if specified\n",
    "    if label_smoothing > 0.0:\n",
    "        num_classes = targets.shape[-1]\n",
    "        smooth_targets = (1 - label_smoothing) * targets + label_smoothing / num_classes\n",
    "    else:\n",
    "        smooth_targets = targets\n",
    "    \n",
    "    loss = -jnp.sum(smooth_targets * log_probs, axis=-1)\n",
    "    return jnp.mean(loss)\n",
    "\n",
    "def sparse_categorical_cross_entropy(logits, labels, from_logits=True):\n",
    "    \"\"\"Sparse categorical cross-entropy (integer labels)\"\"\"\n",
    "    if from_logits:\n",
    "        log_probs = log_softmax(logits)\n",
    "    else:\n",
    "        eps = 1e-15\n",
    "        probs = jnp.clip(logits, eps, 1 - eps)\n",
    "        log_probs = jnp.log(probs)\n",
    "    \n",
    "    # Convert to one-hot for indexing\n",
    "    num_classes = logits.shape[-1]\n",
    "    one_hot_labels = jax.nn.one_hot(labels, num_classes)\n",
    "    \n",
    "    loss = -jnp.sum(one_hot_labels * log_probs, axis=-1)\n",
    "    return jnp.mean(loss)\n",
    "\n",
    "def focal_loss(logits, targets, alpha=1.0, gamma=2.0, from_logits=True):\n",
    "    \"\"\"Focal loss for handling class imbalance\"\"\"\n",
    "    if from_logits:\n",
    "        probs = softmax(logits)\n",
    "        log_probs = log_softmax(logits)\n",
    "    else:\n",
    "        eps = 1e-15\n",
    "        probs = jnp.clip(logits, eps, 1 - eps)\n",
    "        log_probs = jnp.log(probs)\n",
    "    \n",
    "    # Compute p_t (probability of true class)\n",
    "    p_t = jnp.sum(targets * probs, axis=-1)\n",
    "    \n",
    "    # Compute alpha_t\n",
    "    alpha_t = jnp.sum(targets * alpha, axis=-1) if jnp.ndim(alpha) > 0 else alpha\n",
    "    \n",
    "    # Focal loss formula: -alpha_t * (1 - p_t)^gamma * log(p_t)\n",
    "    focal_weight = alpha_t * ((1 - p_t) ** gamma)\n",
    "    loss = -jnp.sum(targets * log_probs, axis=-1)\n",
    "    \n",
    "    return jnp.mean(focal_weight * loss)\n",
    "\n",
    "def test_classification_losses():\n",
    "    \"\"\"Test classification loss functions\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(123)\n",
    "    batch_size, num_classes = 32, 5\n",
    "    \n",
    "    # Generate random logits and targets\n",
    "    logits = random.normal(key, (batch_size, num_classes))\n",
    "    targets_one_hot = jax.nn.one_hot(\n",
    "        random.randint(random.split(key)[1], (batch_size,), 0, num_classes),\n",
    "        num_classes\n",
    "    )\n",
    "    targets_sparse = jnp.argmax(targets_one_hot, axis=1)\n",
    "    \n",
    "    print(\"Classification Loss Functions:\")\n",
    "    \n",
    "    # Test categorical cross-entropy\n",
    "    ce_loss = categorical_cross_entropy(logits, targets_one_hot)\n",
    "    ce_loss_smooth = categorical_cross_entropy(logits, targets_one_hot, label_smoothing=0.1)\n",
    "    sparse_ce_loss = sparse_categorical_cross_entropy(logits, targets_sparse)\n",
    "    \n",
    "    print(f\"Categorical CE: {ce_loss:.4f}\")\n",
    "    print(f\"Categorical CE (smoothed): {ce_loss_smooth:.4f}\")\n",
    "    print(f\"Sparse CE: {sparse_ce_loss:.4f}\")\n",
    "    print(f\"CE losses match: {jnp.allclose(ce_loss, sparse_ce_loss)}\")\n",
    "    \n",
    "    # Test focal loss\n",
    "    focal_loss_val = focal_loss(logits, targets_one_hot, alpha=1.0, gamma=2.0)\n",
    "    print(f\"Focal Loss (γ=2.0): {focal_loss_val:.4f}\")\n",
    "    \n",
    "    # Test binary cross-entropy\n",
    "    binary_logits = random.normal(random.split(key, 3)[2], (batch_size,))\n",
    "    binary_targets = random.bernoulli(random.split(key, 4)[3], 0.5, (batch_size,))\n",
    "    \n",
    "    bce_loss = binary_cross_entropy(binary_logits, binary_targets)\n",
    "    print(f\"Binary CE: {bce_loss:.4f}\")\n",
    "\n",
    "test_classification_losses()\n",
    "```\n",
    "\n",
    "### Margin-Based Losses\n",
    "\n",
    "```python\n",
    "def hinge_loss(logits, targets, margin=1.0):\n",
    "    \"\"\"Hinge loss for SVM-style classification\"\"\"\n",
    "    # Convert one-hot targets to {-1, 1} format\n",
    "    if targets.ndim > 1:  # One-hot encoded\n",
    "        y = 2 * targets - 1  # Convert {0,1} to {-1,1}\n",
    "        scores = jnp.sum(logits * y, axis=-1)\n",
    "    else:  # Binary labels\n",
    "        y = 2 * targets - 1\n",
    "        scores = logits * y\n",
    "    \n",
    "    loss = jnp.maximum(0, margin - scores)\n",
    "    return jnp.mean(loss)\n",
    "\n",
    "def squared_hinge_loss(logits, targets, margin=1.0):\n",
    "    \"\"\"Squared hinge loss\"\"\"\n",
    "    if targets.ndim > 1:\n",
    "        y = 2 * targets - 1\n",
    "        scores = jnp.sum(logits * y, axis=-1)\n",
    "    else:\n",
    "        y = 2 * targets - 1\n",
    "        scores = logits * y\n",
    "    \n",
    "    loss = jnp.maximum(0, margin - scores) ** 2\n",
    "    return jnp.mean(loss)\n",
    "\n",
    "def test_margin_losses():\n",
    "    \"\"\"Test margin-based loss functions\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(456)\n",
    "    batch_size = 50\n",
    "    \n",
    "    # Binary classification\n",
    "    binary_logits = random.normal(key, (batch_size,))\n",
    "    binary_targets = random.bernoulli(random.split(key)[1], 0.5, (batch_size,))\n",
    "    \n",
    "    hinge = hinge_loss(binary_logits, binary_targets)\n",
    "    sq_hinge = squared_hinge_loss(binary_logits, binary_targets)\n",
    "    \n",
    "    print(\"Margin-Based Losses:\")\n",
    "    print(f\"Hinge Loss: {hinge:.4f}\")\n",
    "    print(f\"Squared Hinge Loss: {sq_hinge:.4f}\")\n",
    "    \n",
    "    # Visualize hinge vs squared hinge\n",
    "    margins = jnp.linspace(-3, 3, 100)\n",
    "    hinge_vals = jnp.maximum(0, 1 - margins)\n",
    "    sq_hinge_vals = jnp.maximum(0, 1 - margins) ** 2\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(margins, hinge_vals, label='Hinge Loss', linewidth=2)\n",
    "    plt.plot(margins, sq_hinge_vals, label='Squared Hinge Loss', linewidth=2)\n",
    "    plt.xlabel('Margin (y * f(x))')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Hinge Loss vs Squared Hinge Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "test_margin_losses()\n",
    "```\n",
    "\n",
    "## Ranking and Contrastive Losses\n",
    "\n",
    "### Pairwise and Contrastive Losses\n",
    "\n",
    "```python\n",
    "def pairwise_ranking_loss(scores, targets, margin=1.0):\n",
    "    \"\"\"Pairwise ranking loss for learning to rank\"\"\"\n",
    "    # scores: (batch_size, num_items)\n",
    "    # targets: (batch_size, num_items) - relevance scores\n",
    "    \n",
    "    batch_size, num_items = scores.shape\n",
    "    \n",
    "    # Create all pairs\n",
    "    score_diff = scores[:, :, None] - scores[:, None, :]  # (batch, items, items)\n",
    "    target_diff = targets[:, :, None] - targets[:, None, :]  # (batch, items, items)\n",
    "    \n",
    "    # Only consider pairs where target_diff > 0 (first item more relevant)\n",
    "    valid_pairs = target_diff > 0\n",
    "    \n",
    "    # Ranking loss: max(0, margin - (s_i - s_j)) where target_i > target_j\n",
    "    loss = jnp.maximum(0, margin - score_diff)\n",
    "    loss = jnp.where(valid_pairs, loss, 0)\n",
    "    \n",
    "    num_valid_pairs = jnp.sum(valid_pairs)\n",
    "    return jnp.sum(loss) / jnp.maximum(num_valid_pairs, 1)\n",
    "\n",
    "def contrastive_loss(embeddings1, embeddings2, labels, margin=1.0):\n",
    "    \"\"\"Contrastive loss for siamese networks\"\"\"\n",
    "    # embeddings1, embeddings2: (batch_size, embedding_dim)\n",
    "    # labels: (batch_size,) - 1 for similar pairs, 0 for dissimilar\n",
    "    \n",
    "    # Compute Euclidean distance\n",
    "    distances = jnp.linalg.norm(embeddings1 - embeddings2, axis=1)\n",
    "    \n",
    "    # Contrastive loss\n",
    "    positive_loss = labels * (distances ** 2)\n",
    "    negative_loss = (1 - labels) * jnp.maximum(0, margin - distances) ** 2\n",
    "    \n",
    "    return jnp.mean(positive_loss + negative_loss)\n",
    "\n",
    "def triplet_loss(anchor, positive, negative, margin=1.0):\n",
    "    \"\"\"Triplet loss for metric learning\"\"\"\n",
    "    # anchor, positive, negative: (batch_size, embedding_dim)\n",
    "    \n",
    "    pos_dist = jnp.linalg.norm(anchor - positive, axis=1)\n",
    "    neg_dist = jnp.linalg.norm(anchor - negative, axis=1)\n",
    "    \n",
    "    loss = jnp.maximum(0, pos_dist - neg_dist + margin)\n",
    "    return jnp.mean(loss)\n",
    "\n",
    "def test_ranking_losses():\n",
    "    \"\"\"Test ranking and contrastive losses\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(789)\n",
    "    batch_size, num_items, embed_dim = 16, 5, 64\n",
    "    \n",
    "    # Test pairwise ranking\n",
    "    scores = random.normal(key, (batch_size, num_items))\n",
    "    targets = random.uniform(random.split(key)[1], (batch_size, num_items))\n",
    "    \n",
    "    ranking_loss_val = pairwise_ranking_loss(scores, targets)\n",
    "    print(\"Ranking and Contrastive Losses:\")\n",
    "    print(f\"Pairwise Ranking Loss: {ranking_loss_val:.4f}\")\n",
    "    \n",
    "    # Test contrastive loss\n",
    "    embed1 = random.normal(random.split(key, 3)[2], (batch_size, embed_dim))\n",
    "    embed2 = random.normal(random.split(key, 4)[3], (batch_size, embed_dim))\n",
    "    similarity_labels = random.bernoulli(random.split(key, 5)[4], 0.5, (batch_size,))\n",
    "    \n",
    "    contrastive_loss_val = contrastive_loss(embed1, embed2, similarity_labels)\n",
    "    print(f\"Contrastive Loss: {contrastive_loss_val:.4f}\")\n",
    "    \n",
    "    # Test triplet loss\n",
    "    anchor = random.normal(random.split(key, 6)[5], (batch_size, embed_dim))\n",
    "    positive = anchor + 0.1 * random.normal(random.split(key, 7)[6], (batch_size, embed_dim))\n",
    "    negative = random.normal(random.split(key, 8)[7], (batch_size, embed_dim))\n",
    "    \n",
    "    triplet_loss_val = triplet_loss(anchor, positive, negative)\n",
    "    print(f\"Triplet Loss: {triplet_loss_val:.4f}\")\n",
    "\n",
    "test_ranking_losses()\n",
    "```\n",
    "\n",
    "## Regularization Losses\n",
    "\n",
    "### Weight Decay and Regularization\n",
    "\n",
    "```python\n",
    "def l1_regularization(params, lambda_reg=0.01):\n",
    "    \"\"\"L1 regularization (Lasso)\"\"\"\n",
    "    l1_loss = 0.0\n",
    "    for param in jax.tree_leaves(params):\n",
    "        l1_loss += jnp.sum(jnp.abs(param))\n",
    "    return lambda_reg * l1_loss\n",
    "\n",
    "def l2_regularization(params, lambda_reg=0.01):\n",
    "    \"\"\"L2 regularization (Ridge)\"\"\"\n",
    "    l2_loss = 0.0\n",
    "    for param in jax.tree_leaves(params):\n",
    "        l2_loss += jnp.sum(param ** 2)\n",
    "    return lambda_reg * l2_loss\n",
    "\n",
    "def elastic_net_regularization(params, l1_ratio=0.5, lambda_reg=0.01):\n",
    "    \"\"\"Elastic Net regularization (L1 + L2)\"\"\"\n",
    "    l1_loss = 0.0\n",
    "    l2_loss = 0.0\n",
    "    \n",
    "    for param in jax.tree_leaves(params):\n",
    "        l1_loss += jnp.sum(jnp.abs(param))\n",
    "        l2_loss += jnp.sum(param ** 2)\n",
    "    \n",
    "    return lambda_reg * (l1_ratio * l1_loss + (1 - l1_ratio) * l2_loss)\n",
    "\n",
    "def orthogonality_loss(weight_matrix, lambda_reg=0.01):\n",
    "    \"\"\"Orthogonality regularization for weight matrices\"\"\"\n",
    "    WTW = jnp.dot(weight_matrix.T, weight_matrix)\n",
    "    I = jnp.eye(WTW.shape[0])\n",
    "    return lambda_reg * jnp.sum((WTW - I) ** 2)\n",
    "\n",
    "def test_regularization():\n",
    "    \"\"\"Test regularization functions\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(111)\n",
    "    params = {\n",
    "        'layer1': {'W': random.normal(key, (10, 5)), 'b': jnp.zeros(5)},\n",
    "        'layer2': {'W': random.normal(random.split(key)[1], (5, 3)), 'b': jnp.zeros(3)}\n",
    "    }\n",
    "    \n",
    "    print(\"Regularization Losses:\")\n",
    "    print(f\"L1 Regularization: {l1_regularization(params):.6f}\")\n",
    "    print(f\"L2 Regularization: {l2_regularization(params):.6f}\")\n",
    "    print(f\"Elastic Net (0.5): {elastic_net_regularization(params, l1_ratio=0.5):.6f}\")\n",
    "    \n",
    "    # Test orthogonality loss\n",
    "    W = random.normal(random.split(key, 3)[2], (20, 10))\n",
    "    ortho_loss = orthogonality_loss(W)\n",
    "    print(f\"Orthogonality Loss: {ortho_loss:.6f}\")\n",
    "\n",
    "test_regularization()\n",
    "```\n",
    "\n",
    "## Custom Loss Functions\n",
    "\n",
    "### Task-Specific Losses\n",
    "\n",
    "```python\n",
    "def dice_loss(predictions, targets, smooth=1e-6):\n",
    "    \"\"\"Dice loss for segmentation tasks\"\"\"\n",
    "    # Flatten predictions and targets\n",
    "    pred_flat = predictions.reshape(-1)\n",
    "    target_flat = targets.reshape(-1)\n",
    "    \n",
    "    intersection = jnp.sum(pred_flat * target_flat)\n",
    "    dice_coeff = (2 * intersection + smooth) / (jnp.sum(pred_flat) + jnp.sum(target_flat) + smooth)\n",
    "    \n",
    "    return 1 - dice_coeff\n",
    "\n",
    "def iou_loss(predictions, targets, smooth=1e-6):\n",
    "    \"\"\"IoU (Jaccard) loss for segmentation\"\"\"\n",
    "    pred_flat = predictions.reshape(-1)\n",
    "    target_flat = targets.reshape(-1)\n",
    "    \n",
    "    intersection = jnp.sum(pred_flat * target_flat)\n",
    "    union = jnp.sum(pred_flat) + jnp.sum(target_flat) - intersection\n",
    "    \n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    return 1 - iou\n",
    "\n",
    "def wasserstein_loss(real_scores, fake_scores):\n",
    "    \"\"\"Wasserstein loss for GANs\"\"\"\n",
    "    return jnp.mean(fake_scores) - jnp.mean(real_scores)\n",
    "\n",
    "def perceptual_loss(features_pred, features_target, feature_weights=None):\n",
    "    \"\"\"Perceptual loss using feature maps\"\"\"\n",
    "    if feature_weights is None:\n",
    "        feature_weights = [1.0] * len(features_pred)\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for i, (pred_feat, target_feat) in enumerate(zip(features_pred, features_target)):\n",
    "        loss = jnp.mean((pred_feat - target_feat) ** 2)\n",
    "        total_loss += feature_weights[i] * loss\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "def test_custom_losses():\n",
    "    \"\"\"Test custom loss functions\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(222)\n",
    "    \n",
    "    # Test segmentation losses\n",
    "    pred_mask = random.uniform(key, (32, 32))  # Predicted probabilities\n",
    "    true_mask = random.bernoulli(random.split(key)[1], 0.3, (32, 32))  # Binary mask\n",
    "    \n",
    "    dice_loss_val = dice_loss(pred_mask, true_mask)\n",
    "    iou_loss_val = iou_loss(pred_mask, true_mask)\n",
    "    \n",
    "    print(\"Custom Loss Functions:\")\n",
    "    print(f\"Dice Loss: {dice_loss_val:.4f}\")\n",
    "    print(f\"IoU Loss: {iou_loss_val:.4f}\")\n",
    "    \n",
    "    # Test Wasserstein loss\n",
    "    real_scores = random.normal(random.split(key, 3)[2], (100,))\n",
    "    fake_scores = random.normal(random.split(key, 4)[3], (100,))\n",
    "    \n",
    "    w_loss = wasserstein_loss(real_scores, fake_scores)\n",
    "    print(f\"Wasserstein Loss: {w_loss:.4f}\")\n",
    "    \n",
    "    # Test perceptual loss\n",
    "    features_pred = [random.normal(random.split(key, i+5)[i+4], (64, 32, 32)) for i in range(3)]\n",
    "    features_target = [random.normal(random.split(key, i+8)[i+7], (64, 32, 32)) for i in range(3)]\n",
    "    \n",
    "    perc_loss = perceptual_loss(features_pred, features_target)\n",
    "    print(f\"Perceptual Loss: {perc_loss:.4f}\")\n",
    "\n",
    "test_custom_losses()\n",
    "```\n",
    "\n",
    "## Loss Function Analysis\n",
    "\n",
    "### Gradient and Curvature Analysis\n",
    "\n",
    "```python\n",
    "def analyze_loss_properties(loss_fn, x_range=(-3, 3), num_points=100):\n",
    "    \"\"\"Analyze loss function properties\"\"\"\n",
    "    \n",
    "    x_vals = jnp.linspace(x_range[0], x_range[1], num_points)\n",
    "    targets = jnp.zeros_like(x_vals)  # Target at zero\n",
    "    \n",
    "    # Compute loss values\n",
    "    loss_vals = vmap(lambda x: loss_fn(x, 0.0))(x_vals)\n",
    "    \n",
    "    # Compute gradients\n",
    "    grad_fn = grad(lambda pred, target: loss_fn(pred, target))\n",
    "    grad_vals = vmap(lambda x: grad_fn(x, 0.0))(x_vals)\n",
    "    \n",
    "    # Compute second derivatives (curvature)\n",
    "    hess_fn = grad(grad_fn)\n",
    "    hess_vals = vmap(lambda x: hess_fn(x, 0.0))(x_vals)\n",
    "    \n",
    "    return x_vals, loss_vals, grad_vals, hess_vals\n",
    "\n",
    "def compare_loss_functions():\n",
    "    \"\"\"Compare properties of different loss functions\"\"\"\n",
    "    \n",
    "    loss_functions = {\n",
    "        'MSE': lambda pred, target: (pred - target) ** 2,\n",
    "        'MAE': lambda pred, target: jnp.abs(pred - target),\n",
    "        'Huber (δ=1)': lambda pred, target: huber_loss(pred, target, delta=1.0, reduction='none'),\n",
    "        'Quantile (0.1)': lambda pred, target: quantile_loss(pred, target, quantile=0.1, reduction='none')\n",
    "    }\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, (name, loss_fn) in enumerate(loss_functions.items()):\n",
    "        x_vals, loss_vals, grad_vals, hess_vals = analyze_loss_properties(loss_fn)\n",
    "        \n",
    "        # Plot loss function\n",
    "        ax = axes[i]\n",
    "        ax.plot(x_vals, loss_vals, 'b-', label='Loss', linewidth=2)\n",
    "        ax.plot(x_vals, grad_vals, 'r--', label='Gradient', linewidth=2)\n",
    "        ax.plot(x_vals, hess_vals, 'g:', label='2nd Derivative', linewidth=2)\n",
    "        \n",
    "        ax.set_title(f'{name} Loss Function')\n",
    "        ax.set_xlabel('Error (prediction - target)')\n",
    "        ax.set_ylabel('Value')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xlim(-3, 3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "compare_loss_functions()\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we've implemented comprehensive loss functions for various ML tasks:\n",
    "\n",
    "**Regression Losses:**\n",
    "1. **MSE/RMSE**: Standard squared error losses\n",
    "2. **MAE**: Robust L1 loss for outliers\n",
    "3. **Huber**: Smooth L1 combining MSE and MAE benefits\n",
    "4. **Quantile**: For quantile regression and uncertainty\n",
    "\n",
    "**Classification Losses:**\n",
    "1. **Cross-Entropy**: Standard for classification tasks\n",
    "2. **Focal Loss**: Handles class imbalance effectively\n",
    "3. **Hinge Losses**: SVM-style margin-based losses\n",
    "4. **Label Smoothing**: Regularization technique for overconfidence\n",
    "\n",
    "**Ranking/Contrastive Losses:**\n",
    "1. **Pairwise Ranking**: Learning to rank applications\n",
    "2. **Contrastive**: Siamese networks for similarity learning\n",
    "3. **Triplet**: Metric learning with anchor-positive-negative\n",
    "\n",
    "**Regularization:**\n",
    "1. **L1/L2**: Standard weight decay techniques\n",
    "2. **Elastic Net**: Combined L1+L2 regularization\n",
    "3. **Orthogonality**: Promoting orthogonal weight matrices\n",
    "\n",
    "**Custom Losses:**\n",
    "1. **Dice/IoU**: Segmentation task losses\n",
    "2. **Wasserstein**: GAN training losses  \n",
    "3. **Perceptual**: Feature-based losses for generation\n",
    "\n",
    "**Key Insights:**\n",
    "- Numerical stability crucial for cross-entropy losses\n",
    "- Different losses suit different problem characteristics\n",
    "- Regularization prevents overfitting and improves generalization\n",
    "- Custom losses enable task-specific optimization\n",
    "- Gradient analysis reveals optimization properties\n",
    "\n",
    "**Practical Guidelines:**\n",
    "- Use cross-entropy for classification, MSE for regression as defaults\n",
    "- Apply Huber loss when data contains outliers\n",
    "- Use focal loss for imbalanced classification\n",
    "- Implement custom losses for domain-specific requirements\n",
    "- Always consider numerical stability in implementation\n",
    "\n",
    "This comprehensive loss function library provides the foundation for training models across diverse machine learning applications."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
