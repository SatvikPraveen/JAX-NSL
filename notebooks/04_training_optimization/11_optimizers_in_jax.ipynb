{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a471018",
   "metadata": {},
   "source": [
    "# File: notebooks/04_training_optimization/11_optimizers_in_jax.ipynb\n",
    "\n",
    "## JAX Training Optimization: Optimizers in JAX\n",
    "\n",
    "This notebook implements various optimization algorithms from scratch in JAX, including SGD, momentum variants, Adam, AdamW, RMSprop, and advanced techniques like learning rate scheduling and gradient clipping. We'll analyze their behavior and provide practical guidance for choosing optimizers.\n",
    "\n",
    "Understanding optimization algorithms is crucial for effective neural network training, as the choice of optimizer significantly impacts convergence speed, stability, and final performance.\n",
    "\n",
    "## Setting Up the Environment\n",
    "\n",
    "```python\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, random, lax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Any, Optional, Callable, Tuple\n",
    "import functools\n",
    "from dataclasses import dataclass\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "```\n",
    "\n",
    "## Base Optimizer Interface\n",
    "\n",
    "### Optimizer Abstract Base\n",
    "\n",
    "```python\n",
    "class Optimizer:\n",
    "    \"\"\"Base class for optimizers\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate: float):\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def init_state(self, params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Initialize optimizer state\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def update(self, grads: Dict[str, Any], state: Dict[str, Any], params: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
    "        \"\"\"Update parameters given gradients\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def get_learning_rate(self, step: int) -> float:\n",
    "        \"\"\"Get learning rate for current step (can be overridden for scheduling)\"\"\"\n",
    "        return self.learning_rate\n",
    "\n",
    "def clip_gradients(grads, max_norm):\n",
    "    \"\"\"Clip gradients by global norm\"\"\"\n",
    "    global_norm = jnp.sqrt(sum(jnp.sum(g**2) for g in jax.tree_leaves(grads)))\n",
    "    clip_factor = jnp.minimum(max_norm / (global_norm + 1e-8), 1.0)\n",
    "    return jax.tree_map(lambda g: g * clip_factor, grads)\n",
    "```\n",
    "\n",
    "## Basic Optimizers\n",
    "\n",
    "### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "```python\n",
    "class SGD(Optimizer):\n",
    "    \"\"\"Stochastic Gradient Descent optimizer\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 0.01, momentum: float = 0.0, weight_decay: float = 0.0):\n",
    "        super().__init__(learning_rate)\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "    \n",
    "    def init_state(self, params):\n",
    "        \"\"\"Initialize SGD state\"\"\"\n",
    "        if self.momentum > 0:\n",
    "            return {\n",
    "                'velocity': jax.tree_map(jnp.zeros_like, params),\n",
    "                'step': 0\n",
    "            }\n",
    "        return {'step': 0}\n",
    "    \n",
    "    def update(self, grads, state, params):\n",
    "        \"\"\"SGD parameter update\"\"\"\n",
    "        step = state['step'] + 1\n",
    "        \n",
    "        # Add weight decay to gradients\n",
    "        if self.weight_decay > 0:\n",
    "            grads = jax.tree_map(\n",
    "                lambda g, p: g + self.weight_decay * p,\n",
    "                grads, params\n",
    "            )\n",
    "        \n",
    "        if self.momentum > 0:\n",
    "            # Momentum update\n",
    "            velocity = jax.tree_map(\n",
    "                lambda v, g: self.momentum * v + g,\n",
    "                state['velocity'], grads\n",
    "            )\n",
    "            \n",
    "            new_params = jax.tree_map(\n",
    "                lambda p, v: p - self.learning_rate * v,\n",
    "                params, velocity\n",
    "            )\n",
    "            \n",
    "            new_state = {'velocity': velocity, 'step': step}\n",
    "        else:\n",
    "            # Vanilla SGD\n",
    "            new_params = jax.tree_map(\n",
    "                lambda p, g: p - self.learning_rate * g,\n",
    "                params, grads\n",
    "            )\n",
    "            new_state = {'step': step}\n",
    "        \n",
    "        return new_params, new_state\n",
    "\n",
    "# Test SGD\n",
    "def test_sgd():\n",
    "    \"\"\"Test SGD optimizer\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(42)\n",
    "    params = {'w': random.normal(key, (3, 2)), 'b': jnp.zeros(2)}\n",
    "    grads = {'w': random.normal(random.split(key)[1], (3, 2)), 'b': jnp.ones(2)}\n",
    "    \n",
    "    # Test vanilla SGD\n",
    "    sgd = SGD(learning_rate=0.1)\n",
    "    state = sgd.init_state(params)\n",
    "    new_params, new_state = sgd.update(grads, state, params)\n",
    "    \n",
    "    print(\"SGD Test:\")\n",
    "    print(f\"Original params['w'][0,0]: {params['w'][0,0]:.4f}\")\n",
    "    print(f\"Updated params['w'][0,0]: {new_params['w'][0,0]:.4f}\")\n",
    "    print(f\"Step count: {new_state['step']}\")\n",
    "    \n",
    "    # Test SGD with momentum\n",
    "    sgd_momentum = SGD(learning_rate=0.1, momentum=0.9)\n",
    "    state_momentum = sgd_momentum.init_state(params)\n",
    "    new_params_momentum, new_state_momentum = sgd_momentum.update(grads, state_momentum, params)\n",
    "    \n",
    "    print(f\"SGD+Momentum params['w'][0,0]: {new_params_momentum['w'][0,0]:.4f}\")\n",
    "    print(f\"Velocity shape: {new_state_momentum['velocity']['w'].shape}\")\n",
    "\n",
    "test_sgd()\n",
    "```\n",
    "\n",
    "### Nesterov Accelerated Gradient\n",
    "\n",
    "```python\n",
    "class Nesterov(Optimizer):\n",
    "    \"\"\"Nesterov Accelerated Gradient optimizer\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 0.01, momentum: float = 0.9, weight_decay: float = 0.0):\n",
    "        super().__init__(learning_rate)\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "    \n",
    "    def init_state(self, params):\n",
    "        \"\"\"Initialize Nesterov state\"\"\"\n",
    "        return {\n",
    "            'velocity': jax.tree_map(jnp.zeros_like, params),\n",
    "            'step': 0\n",
    "        }\n",
    "    \n",
    "    def update(self, grads, state, params):\n",
    "        \"\"\"Nesterov parameter update\"\"\"\n",
    "        step = state['step'] + 1\n",
    "        \n",
    "        # Add weight decay\n",
    "        if self.weight_decay > 0:\n",
    "            grads = jax.tree_map(\n",
    "                lambda g, p: g + self.weight_decay * p,\n",
    "                grads, params\n",
    "            )\n",
    "        \n",
    "        # Nesterov update: look ahead\n",
    "        velocity = jax.tree_map(\n",
    "            lambda v, g: self.momentum * v + g,\n",
    "            state['velocity'], grads\n",
    "        )\n",
    "        \n",
    "        new_params = jax.tree_map(\n",
    "            lambda p, v, g: p - self.learning_rate * (self.momentum * v + g),\n",
    "            params, velocity, grads\n",
    "        )\n",
    "        \n",
    "        new_state = {'velocity': velocity, 'step': step}\n",
    "        return new_params, new_state\n",
    "\n",
    "class RMSprop(Optimizer):\n",
    "    \"\"\"RMSprop optimizer\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 0.001, rho: float = 0.9, eps: float = 1e-8, weight_decay: float = 0.0):\n",
    "        super().__init__(learning_rate)\n",
    "        self.rho = rho\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "    \n",
    "    def init_state(self, params):\n",
    "        \"\"\"Initialize RMSprop state\"\"\"\n",
    "        return {\n",
    "            'v': jax.tree_map(jnp.zeros_like, params),\n",
    "            'step': 0\n",
    "        }\n",
    "    \n",
    "    def update(self, grads, state, params):\n",
    "        \"\"\"RMSprop parameter update\"\"\"\n",
    "        step = state['step'] + 1\n",
    "        \n",
    "        # Add weight decay\n",
    "        if self.weight_decay > 0:\n",
    "            grads = jax.tree_map(\n",
    "                lambda g, p: g + self.weight_decay * p,\n",
    "                grads, params\n",
    "            )\n",
    "        \n",
    "        # Update squared gradient accumulator\n",
    "        v = jax.tree_map(\n",
    "            lambda v_prev, g: self.rho * v_prev + (1 - self.rho) * g**2,\n",
    "            state['v'], grads\n",
    "        )\n",
    "        \n",
    "        # Parameter update\n",
    "        new_params = jax.tree_map(\n",
    "            lambda p, g, v_val: p - self.learning_rate * g / (jnp.sqrt(v_val) + self.eps),\n",
    "            params, grads, v\n",
    "        )\n",
    "        \n",
    "        new_state = {'v': v, 'step': step}\n",
    "        return new_params, new_state\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    \"\"\"Adam optimizer\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 0.001, beta1: float = 0.9, beta2: float = 0.999, eps: float = 1e-8, weight_decay: float = 0.0):\n",
    "        super().__init__(learning_rate)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "    \n",
    "    def init_state(self, params):\n",
    "        \"\"\"Initialize Adam state\"\"\"\n",
    "        return {\n",
    "            'm': jax.tree_map(jnp.zeros_like, params),\n",
    "            'v': jax.tree_map(jnp.zeros_like, params),\n",
    "            'step': 0\n",
    "        }\n",
    "    \n",
    "    def update(self, grads, state, params):\n",
    "        \"\"\"Adam parameter update\"\"\"\n",
    "        step = state['step'] + 1\n",
    "        \n",
    "        # Add weight decay (L2 regularization)\n",
    "        if self.weight_decay > 0:\n",
    "            grads = jax.tree_map(\n",
    "                lambda g, p: g + self.weight_decay * p,\n",
    "                grads, params\n",
    "            )\n",
    "        \n",
    "        # Update biased first moment estimate\n",
    "        m = jax.tree_map(\n",
    "            lambda m_prev, g: self.beta1 * m_prev + (1 - self.beta1) * g,\n",
    "            state['m'], grads\n",
    "        )\n",
    "        \n",
    "        # Update biased second moment estimate\n",
    "        v = jax.tree_map(\n",
    "            lambda v_prev, g: self.beta2 * v_prev + (1 - self.beta2) * g**2,\n",
    "            state['v'], grads\n",
    "        )\n",
    "        \n",
    "        # Bias correction\n",
    "        m_hat = jax.tree_map(lambda m_val: m_val / (1 - self.beta1**step), m)\n",
    "        v_hat = jax.tree_map(lambda v_val: v_val / (1 - self.beta2**step), v)\n",
    "        \n",
    "        # Parameter update\n",
    "        new_params = jax.tree_map(\n",
    "            lambda p, m_val, v_val: p - self.learning_rate * m_val / (jnp.sqrt(v_val) + self.eps),\n",
    "            params, m_hat, v_hat\n",
    "        )\n",
    "        \n",
    "        new_state = {'m': m, 'v': v, 'step': step}\n",
    "        return new_params, new_state\n",
    "\n",
    "class AdamW(Optimizer):\n",
    "    \"\"\"AdamW optimizer (Adam with decoupled weight decay)\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 0.001, beta1: float = 0.9, beta2: float = 0.999, eps: float = 1e-8, weight_decay: float = 0.01):\n",
    "        super().__init__(learning_rate)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "    \n",
    "    def init_state(self, params):\n",
    "        \"\"\"Initialize AdamW state\"\"\"\n",
    "        return {\n",
    "            'm': jax.tree_map(jnp.zeros_like, params),\n",
    "            'v': jax.tree_map(jnp.zeros_like, params),\n",
    "            'step': 0\n",
    "        }\n",
    "    \n",
    "    def update(self, grads, state, params):\n",
    "        \"\"\"AdamW parameter update with decoupled weight decay\"\"\"\n",
    "        step = state['step'] + 1\n",
    "        \n",
    "        # Update biased first moment estimate\n",
    "        m = jax.tree_map(\n",
    "            lambda m_prev, g: self.beta1 * m_prev + (1 - self.beta1) * g,\n",
    "            state['m'], grads\n",
    "        )\n",
    "        \n",
    "        # Update biased second moment estimate\n",
    "        v = jax.tree_map(\n",
    "            lambda v_prev, g: self.beta2 * v_prev + (1 - self.beta2) * g**2,\n",
    "            state['v'], grads\n",
    "        )\n",
    "        \n",
    "        # Bias correction\n",
    "        m_hat = jax.tree_map(lambda m_val: m_val / (1 - self.beta1**step), m)\n",
    "        v_hat = jax.tree_map(lambda v_val: v_val / (1 - self.beta2**step), v)\n",
    "        \n",
    "        # Parameter update with decoupled weight decay\n",
    "        new_params = jax.tree_map(\n",
    "            lambda p, m_val, v_val: p - self.learning_rate * (m_val / (jnp.sqrt(v_val) + self.eps) + self.weight_decay * p),\n",
    "            params, m_hat, v_hat\n",
    "        )\n",
    "        \n",
    "        new_state = {'m': m, 'v': v, 'step': step}\n",
    "        return new_params, new_state\n",
    "\n",
    "def test_optimizers():\n",
    "    \"\"\"Test all optimizers on simple quadratic function\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(42)\n",
    "    \n",
    "    # Simple quadratic: f(x) = x^T A x + b^T x\n",
    "    A = jnp.array([[2.0, 0.5], [0.5, 1.0]])  # Positive definite\n",
    "    b = jnp.array([1.0, -0.5])\n",
    "    x_optimal = -jnp.linalg.solve(A, b) / 2  # Analytical optimum\n",
    "    \n",
    "    def objective(params):\n",
    "        x = params['x']\n",
    "        return jnp.dot(x, A @ x) + jnp.dot(b, x)\n",
    "    \n",
    "    grad_fn = grad(objective)\n",
    "    \n",
    "    # Test different optimizers\n",
    "    optimizers = {\n",
    "        'SGD': SGD(0.1),\n",
    "        'Momentum': SGD(0.1, momentum=0.9),\n",
    "        'Nesterov': Nesterov(0.1, momentum=0.9),\n",
    "        'RMSprop': RMSprop(0.1),\n",
    "        'Adam': Adam(0.1),\n",
    "        'AdamW': AdamW(0.1, weight_decay=0.01)\n",
    "    }\n",
    "    \n",
    "    n_steps = 100\n",
    "    results = {}\n",
    "    \n",
    "    for name, optimizer in optimizers.items():\n",
    "        params = {'x': jnp.array([2.0, -1.0])}  # Starting point\n",
    "        state = optimizer.init_state(params)\n",
    "        losses = []\n",
    "        \n",
    "        for step in range(n_steps):\n",
    "            grads = grad_fn(params)\n",
    "            params, state = optimizer.update(grads, state, params)\n",
    "            loss = objective(params)\n",
    "            losses.append(loss)\n",
    "        \n",
    "        results[name] = {\n",
    "            'losses': jnp.array(losses),\n",
    "            'final_params': params['x'],\n",
    "            'distance_to_optimal': jnp.linalg.norm(params['x'] - x_optimal)\n",
    "        }\n",
    "    \n",
    "    # Plot convergence\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    for name, result in results.items():\n",
    "        plt.semilogy(result['losses'], label=name)\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Convergence Comparison')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    names = list(results.keys())\n",
    "    distances = [results[name]['distance_to_optimal'] for name in names]\n",
    "    plt.bar(names, distances)\n",
    "    plt.ylabel('Distance to Optimal')\n",
    "    plt.title('Final Optimization Error')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Optimizer Comparison Results:\")\n",
    "    print(f\"Optimal solution: {x_optimal}\")\n",
    "    for name, result in results.items():\n",
    "        print(f\"{name:10}: final_x={result['final_params']}, error={result['distance_to_optimal']:.6f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "optimizer_results = test_optimizers()\n",
    "```\n",
    "\n",
    "## Learning Rate Scheduling\n",
    "\n",
    "### Learning Rate Schedulers\n",
    "\n",
    "```python\n",
    "class LRScheduler:\n",
    "    \"\"\"Base class for learning rate schedulers\"\"\"\n",
    "    \n",
    "    def __call__(self, step: int) -> float:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class StepLR(LRScheduler):\n",
    "    \"\"\"Step decay learning rate scheduler\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_lr: float, step_size: int, gamma: float = 0.1):\n",
    "        self.initial_lr = initial_lr\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def __call__(self, step: int) -> float:\n",
    "        return self.initial_lr * (self.gamma ** (step // self.step_size))\n",
    "\n",
    "class ExponentialLR(LRScheduler):\n",
    "    \"\"\"Exponential decay learning rate scheduler\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_lr: float, gamma: float):\n",
    "        self.initial_lr = initial_lr\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def __call__(self, step: int) -> float:\n",
    "        return self.initial_lr * (self.gamma ** step)\n",
    "\n",
    "class CosineAnnealingLR(LRScheduler):\n",
    "    \"\"\"Cosine annealing learning rate scheduler\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_lr: float, T_max: int, eta_min: float = 0):\n",
    "        self.initial_lr = initial_lr\n",
    "        self.T_max = T_max\n",
    "        self.eta_min = eta_min\n",
    "    \n",
    "    def __call__(self, step: int) -> float:\n",
    "        return self.eta_min + (self.initial_lr - self.eta_min) * (1 + jnp.cos(jnp.pi * step / self.T_max)) / 2\n",
    "\n",
    "class WarmupLR(LRScheduler):\n",
    "    \"\"\"Warmup + cosine learning rate scheduler\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_lr: float, warmup_steps: int, max_steps: int):\n",
    "        self.initial_lr = initial_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.max_steps = max_steps\n",
    "    \n",
    "    def __call__(self, step: int) -> float:\n",
    "        if step < self.warmup_steps:\n",
    "            return self.initial_lr * step / self.warmup_steps\n",
    "        else:\n",
    "            progress = (step - self.warmup_steps) / (self.max_steps - self.warmup_steps)\n",
    "            return self.initial_lr * 0.5 * (1 + jnp.cos(jnp.pi * progress))\n",
    "\n",
    "def test_schedulers():\n",
    "    \"\"\"Test learning rate schedulers\"\"\"\n",
    "    \n",
    "    schedulers = {\n",
    "        'Constant': lambda step: 0.1,\n",
    "        'Step': StepLR(0.1, step_size=30, gamma=0.5),\n",
    "        'Exponential': ExponentialLR(0.1, gamma=0.95),\n",
    "        'Cosine': CosineAnnealingLR(0.1, T_max=100),\n",
    "        'Warmup+Cosine': WarmupLR(0.1, warmup_steps=20, max_steps=100)\n",
    "    }\n",
    "    \n",
    "    steps = jnp.arange(100)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for name, scheduler in schedulers.items():\n",
    "        lrs = [scheduler(step) for step in steps]\n",
    "        plt.plot(steps, lrs, label=name, linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Learning Rate Schedules')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "test_schedulers()\n",
    "```\n",
    "\n",
    "## Advanced Optimizer Features\n",
    "\n",
    "### Gradient Clipping and Optimizer Wrappers\n",
    "\n",
    "```python\n",
    "class OptimWithClipping:\n",
    "    \"\"\"Optimizer wrapper with gradient clipping\"\"\"\n",
    "    \n",
    "    def __init__(self, optimizer: Optimizer, max_grad_norm: float = 1.0):\n",
    "        self.optimizer = optimizer\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "    \n",
    "    def init_state(self, params):\n",
    "        return self.optimizer.init_state(params)\n",
    "    \n",
    "    def update(self, grads, state, params):\n",
    "        # Clip gradients\n",
    "        clipped_grads = clip_gradients(grads, self.max_grad_norm)\n",
    "        return self.optimizer.update(clipped_grads, state, params)\n",
    "\n",
    "class OptimWithScheduler:\n",
    "    \"\"\"Optimizer wrapper with learning rate scheduling\"\"\"\n",
    "    \n",
    "    def __init__(self, optimizer: Optimizer, scheduler: LRScheduler):\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "    \n",
    "    def init_state(self, params):\n",
    "        state = self.optimizer.init_state(params)\n",
    "        state['scheduler_step'] = 0\n",
    "        return state\n",
    "    \n",
    "    def update(self, grads, state, params):\n",
    "        # Update learning rate\n",
    "        step = state.get('scheduler_step', 0)\n",
    "        self.optimizer.learning_rate = self.scheduler(step)\n",
    "        \n",
    "        # Update parameters\n",
    "        new_params, new_state = self.optimizer.update(grads, state, params)\n",
    "        new_state['scheduler_step'] = step + 1\n",
    "        \n",
    "        return new_params, new_state\n",
    "\n",
    "def test_advanced_features():\n",
    "    \"\"\"Test gradient clipping and scheduling\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(123)\n",
    "    \n",
    "    # Create problem with large gradients\n",
    "    def loss_fn(params):\n",
    "        x = params['x']\n",
    "        return jnp.sum(x**4)  # Fourth power creates large gradients\n",
    "    \n",
    "    grad_fn = grad(loss_fn)\n",
    "    \n",
    "    # Test with and without clipping\n",
    "    params = {'x': jnp.array([2.0, -1.5, 3.0])}\n",
    "    grads = grad_fn(params)\n",
    "    \n",
    "    print(\"Gradient Clipping Test:\")\n",
    "    print(f\"Original gradient norm: {jnp.sqrt(sum(jnp.sum(g**2) for g in jax.tree_leaves(grads))):.4f}\")\n",
    "    \n",
    "    clipped_grads = clip_gradients(grads, max_norm=1.0)\n",
    "    print(f\"Clipped gradient norm: {jnp.sqrt(sum(jnp.sum(g**2) for g in jax.tree_leaves(clipped_grads))):.4f}\")\n",
    "    \n",
    "    # Test optimizer with scheduling\n",
    "    base_optimizer = Adam(0.1)\n",
    "    scheduler = CosineAnnealingLR(0.1, T_max=50)\n",
    "    scheduled_optimizer = OptimWithScheduler(base_optimizer, scheduler)\n",
    "    \n",
    "    state = scheduled_optimizer.init_state(params)\n",
    "    \n",
    "    lrs = []\n",
    "    for step in range(20):\n",
    "        grads = grad_fn(params)\n",
    "        params, state = scheduled_optimizer.update(grads, state, params)\n",
    "        lrs.append(base_optimizer.learning_rate)\n",
    "    \n",
    "    print(f\"Learning rates over 20 steps: {lrs[:5]}...{lrs[-5:]}\")\n",
    "\n",
    "test_advanced_features()\n",
    "```\n",
    "\n",
    "## Optimizer Comparison on Neural Network\n",
    "\n",
    "### Training MLP with Different Optimizers\n",
    "\n",
    "```python\n",
    "def create_classification_data(key, n_samples=1000, n_features=20, n_classes=3, noise=0.1):\n",
    "    \"\"\"Create synthetic classification dataset\"\"\"\n",
    "    \n",
    "    # Generate class centers\n",
    "    centers = random.normal(key, (n_classes, n_features)) * 2\n",
    "    \n",
    "    # Generate samples\n",
    "    samples_per_class = n_samples // n_classes\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for class_idx in range(n_classes):\n",
    "        class_samples = centers[class_idx] + noise * random.normal(\n",
    "            random.split(key, n_classes + 1)[class_idx + 1], \n",
    "            (samples_per_class, n_features)\n",
    "        )\n",
    "        X.append(class_samples)\n",
    "        y.extend([class_idx] * samples_per_class)\n",
    "    \n",
    "    X = jnp.concatenate(X, axis=0)\n",
    "    y = jax.nn.one_hot(jnp.array(y), n_classes)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def simple_mlp_forward(params, x):\n",
    "    \"\"\"Simple 2-layer MLP forward pass\"\"\"\n",
    "    h = jax.nn.relu(x @ params['W1'] + params['b1'])\n",
    "    return h @ params['W2'] + params['b2']\n",
    "\n",
    "def cross_entropy_loss(params, x, y):\n",
    "    \"\"\"Cross-entropy loss\"\"\"\n",
    "    logits = simple_mlp_forward(params, x)\n",
    "    log_probs = jax.nn.log_softmax(logits)\n",
    "    return -jnp.mean(jnp.sum(y * log_probs, axis=1))\n",
    "\n",
    "def accuracy(params, x, y):\n",
    "    \"\"\"Classification accuracy\"\"\"\n",
    "    logits = simple_mlp_forward(params, x)\n",
    "    pred_classes = jnp.argmax(logits, axis=1)\n",
    "    true_classes = jnp.argmax(y, axis=1)\n",
    "    return jnp.mean(pred_classes == true_classes)\n",
    "\n",
    "def compare_optimizers_on_mlp():\n",
    "    \"\"\"Compare optimizers training an MLP\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(42)\n",
    "    \n",
    "    # Generate data\n",
    "    X, y = create_classification_data(key, n_samples=800, n_features=20, n_classes=3)\n",
    "    \n",
    "    # Split train/test\n",
    "    split_idx = int(0.8 * len(X))\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "    \n",
    "    # Initialize MLP parameters\n",
    "    def init_mlp_params(key, input_size, hidden_size, output_size):\n",
    "        keys = random.split(key, 4)\n",
    "        return {\n",
    "            'W1': random.normal(keys[0], (input_size, hidden_size)) * jnp.sqrt(2.0 / input_size),\n",
    "            'b1': jnp.zeros(hidden_size),\n",
    "            'W2': random.normal(keys[1], (hidden_size, output_size)) * jnp.sqrt(2.0 / hidden_size),\n",
    "            'b2': jnp.zeros(output_size)\n",
    "        }\n",
    "    \n",
    "    # Test optimizers\n",
    "    optimizers_to_test = {\n",
    "        'SGD': SGD(0.01),\n",
    "        'SGD+Momentum': SGD(0.01, momentum=0.9),\n",
    "        'Adam': Adam(0.001),\n",
    "        'AdamW': AdamW(0.001, weight_decay=0.01),\n",
    "        'RMSprop': RMSprop(0.001)\n",
    "    }\n",
    "    \n",
    "    n_epochs = 50\n",
    "    results = {}\n",
    "    \n",
    "    grad_fn = grad(cross_entropy_loss)\n",
    "    \n",
    "    for opt_name, optimizer in optimizers_to_test.items():\n",
    "        print(f\"Training with {opt_name}...\")\n",
    "        \n",
    "        # Initialize parameters and state\n",
    "        params = init_mlp_params(random.split(key)[1], 20, 64, 3)\n",
    "        state = optimizer.init_state(params)\n",
    "        \n",
    "        train_losses = []\n",
    "        test_accs = []\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            # Training step\n",
    "            grads = grad_fn(params, X_train, y_train)\n",
    "            params, state = optimizer.update(grads, state, params)\n",
    "            \n",
    "            # Record metrics\n",
    "            train_loss = cross_entropy_loss(params, X_train, y_train)\n",
    "            test_acc = accuracy(params, X_test, y_test)\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            test_accs.append(test_acc)\n",
    "        \n",
    "        results[opt_name] = {\n",
    "            'train_losses': jnp.array(train_losses),\n",
    "            'test_accs': jnp.array(test_accs),\n",
    "            'final_acc': test_acc\n",
    "        }\n",
    "    \n",
    "    # Plot results\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Training loss\n",
    "    for opt_name, result in results.items():\n",
    "        ax1.plot(result['train_losses'], label=opt_name, linewidth=2)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Training Loss')\n",
    "    ax1.set_title('Training Loss Comparison')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    # Test accuracy\n",
    "    for opt_name, result in results.items():\n",
    "        ax2.plot(result['test_accs'], label=opt_name, linewidth=2)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Test Accuracy')\n",
    "    ax2.set_title('Test Accuracy Comparison')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nFinal Results:\")\n",
    "    for opt_name, result in results.items():\n",
    "        print(f\"{opt_name:15}: {result['final_acc']:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "mlp_results = compare_optimizers_on_mlp()\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we've implemented and analyzed various optimization algorithms:\n",
    "\n",
    "**Core Optimizers:**\n",
    "\n",
    "1. **SGD**: Simple gradient descent with optional momentum\n",
    "2. **Nesterov**: Accelerated gradient with look-ahead\n",
    "3. **RMSprop**: Adaptive learning rates based on gradient magnitude\n",
    "4. **Adam**: Adaptive moments with bias correction\n",
    "5. **AdamW**: Adam with decoupled weight decay\n",
    "\n",
    "**Advanced Features:**\n",
    "- Gradient clipping for training stability\n",
    "- Learning rate scheduling (step, exponential, cosine, warmup)\n",
    "- Optimizer wrappers for modular functionality\n",
    "- Weight decay vs L2 regularization differences\n",
    "\n",
    "**Key Insights:**\n",
    "- Adam often converges faster than SGD on neural networks\n",
    "- AdamW provides better generalization than Adam with L2 regularization\n",
    "- Learning rate scheduling crucial for final performance\n",
    "- Gradient clipping prevents exploding gradients\n",
    "- Different optimizers suit different problem types\n",
    "\n",
    "**Practical Guidelines:**\n",
    "- Use Adam/AdamW as default for most neural networks\n",
    "- SGD+momentum for well-tuned scenarios or when batch size is large\n",
    "- RMSprop for RNNs and non-stationary objectives\n",
    "- Always use learning rate scheduling for best results\n",
    "- Clip gradients when training deep networks or RNNs\n",
    "\n",
    "**JAX Implementation Benefits:**\n",
    "- Functional approach with immutable state\n",
    "- Easy to compose with other transformations\n",
    "- JIT compilation for performance\n",
    "- Tree operations for complex parameter structures\n",
    "\n",
    "**Next Steps:**\n",
    "- The next notebook will cover loss functions in detail\n",
    "- We'll explore different loss formulations and their properties\n",
    "- Understanding optimizers enables effective training of complex models\n",
    "\n",
    "This comprehensive optimizer implementation provides the foundation for training neural networks effectively across various domains and architectures."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
