{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d6d2da0",
   "metadata": {},
   "source": [
    "# File: notebooks/03_neural_networks/10_attention_from_scratch.ipynb\n",
    "\n",
    "## JAX Neural Networks: Attention from Scratch\n",
    "\n",
    "This notebook implements attention mechanisms from scratch in JAX, including scaled dot-product attention, multi-head attention, and a basic transformer layer. We'll build these fundamental components that power modern language models and vision transformers.\n",
    "\n",
    "Attention mechanisms allow models to focus on relevant parts of the input when processing sequences, enabling better long-range dependencies and more interpretable models.\n",
    "\n",
    "## Setting Up the Environment\n",
    "\n",
    "```python\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, random, lax\n",
    "from jax.nn import softmax, gelu\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Dict, Any, Optional\n",
    "import functools\n",
    "import math\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "```\n",
    "\n",
    "## Scaled Dot-Product Attention\n",
    "\n",
    "### Basic Attention Implementation\n",
    "\n",
    "```python\n",
    "def scaled_dot_product_attention(query, key, value, mask=None, dropout_rate=0.0, key_dropout=None, training=True):\n",
    "    \"\"\"\n",
    "    Scaled dot-product attention mechanism\n",
    "    \n",
    "    Args:\n",
    "        query: Query tensor of shape (..., seq_len_q, d_k)\n",
    "        key: Key tensor of shape (..., seq_len_k, d_k)  \n",
    "        value: Value tensor of shape (..., seq_len_v, d_v)\n",
    "        mask: Optional attention mask\n",
    "        dropout_rate: Dropout rate for attention weights\n",
    "        key_dropout: Random key for dropout\n",
    "        training: Whether in training mode\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output of shape (..., seq_len_q, d_v)\n",
    "        attention_weights: Attention weights of shape (..., seq_len_q, seq_len_k)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute attention scores\n",
    "    d_k = query.shape[-1]\n",
    "    scores = jnp.matmul(query, jnp.swapaxes(key, -2, -1)) / jnp.sqrt(d_k)\n",
    "    \n",
    "    # Apply mask if provided\n",
    "    if mask is not None:\n",
    "        scores = jnp.where(mask, scores, -jnp.inf)\n",
    "    \n",
    "    # Apply softmax to get attention weights\n",
    "    attention_weights = softmax(scores, axis=-1)\n",
    "    \n",
    "    # Apply dropout to attention weights\n",
    "    if training and dropout_rate > 0.0 and key_dropout is not None:\n",
    "        keep_prob = 1.0 - dropout_rate\n",
    "        mask_dropout = random.bernoulli(key_dropout, keep_prob, attention_weights.shape)\n",
    "        attention_weights = jnp.where(mask_dropout, attention_weights / keep_prob, 0.0)\n",
    "    \n",
    "    # Apply attention to values\n",
    "    output = jnp.matmul(attention_weights, value)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "def test_basic_attention():\n",
    "    \"\"\"Test basic attention mechanism\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(42)\n",
    "    batch_size, seq_len, d_model = 2, 8, 64\n",
    "    \n",
    "    # Create random query, key, value tensors\n",
    "    query = random.normal(key, (batch_size, seq_len, d_model))\n",
    "    key_tensor = random.normal(random.split(key)[1], (batch_size, seq_len, d_model))\n",
    "    value = random.normal(random.split(key, 3)[2], (batch_size, seq_len, d_model))\n",
    "    \n",
    "    # Apply attention\n",
    "    output, weights = scaled_dot_product_attention(query, key_tensor, value)\n",
    "    \n",
    "    print(\"Basic Attention Test:\")\n",
    "    print(f\"Input shapes: Q{query.shape}, K{key_tensor.shape}, V{value.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Attention weights shape: {weights.shape}\")\n",
    "    print(f\"Attention weights sum along last axis: {jnp.sum(weights, axis=-1)[0, 0]:.6f}\")\n",
    "    \n",
    "    return output, weights\n",
    "\n",
    "attention_output, attention_weights = test_basic_attention()\n",
    "```\n",
    "\n",
    "### Causal (Masked) Attention\n",
    "\n",
    "```python\n",
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"Create causal mask for autoregressive attention\"\"\"\n",
    "    mask = jnp.tril(jnp.ones((seq_len, seq_len)))\n",
    "    return mask == 1  # Convert to boolean mask\n",
    "\n",
    "def create_padding_mask(lengths, max_len):\n",
    "    \"\"\"Create padding mask for variable length sequences\"\"\"\n",
    "    positions = jnp.arange(max_len)[None, :]  # (1, max_len)\n",
    "    lengths = lengths[:, None]  # (batch_size, 1)\n",
    "    return positions < lengths  # (batch_size, max_len)\n",
    "\n",
    "def test_masked_attention():\n",
    "    \"\"\"Test attention with causal masking\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(123)\n",
    "    batch_size, seq_len, d_model = 1, 6, 32\n",
    "    \n",
    "    # Create input tensors\n",
    "    query = random.normal(key, (batch_size, seq_len, d_model))\n",
    "    key_tensor = query  # Self-attention\n",
    "    value = query\n",
    "    \n",
    "    # Create causal mask\n",
    "    causal_mask = create_causal_mask(seq_len)\n",
    "    causal_mask = jnp.expand_dims(causal_mask, 0)  # Add batch dimension\n",
    "    \n",
    "    # Apply masked attention\n",
    "    output, weights = scaled_dot_product_attention(query, key_tensor, value, mask=causal_mask)\n",
    "    \n",
    "    print(\"Masked Attention Test:\")\n",
    "    print(f\"Causal mask shape: {causal_mask.shape}\")\n",
    "    print(f\"Causal mask:\\n{causal_mask[0].astype(int)}\")\n",
    "    print(f\"Attention weights (position 0): {weights[0, 0, :]}\")\n",
    "    print(f\"Attention weights (position 3): {weights[0, 3, :]}\")\n",
    "    \n",
    "    # Verify causality: later positions should have zero attention to future\n",
    "    future_attention = weights[0, 2, 4:]  # Position 2 attending to positions 4+\n",
    "    print(f\"Future attention (should be ~0): {future_attention}\")\n",
    "\n",
    "test_masked_attention()\n",
    "```\n",
    "\n",
    "## Multi-Head Attention\n",
    "\n",
    "### Multi-Head Attention Implementation\n",
    "\n",
    "```python\n",
    "class MultiHeadAttention:\n",
    "    \"\"\"Multi-head attention mechanism\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dropout_rate=0.1):\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "    \n",
    "    def init_params(self, key):\n",
    "        \"\"\"Initialize parameters for multi-head attention\"\"\"\n",
    "        keys = random.split(key, 4)\n",
    "        \n",
    "        # Xavier/Glorot initialization\n",
    "        def init_linear(key, shape):\n",
    "            fan_in, fan_out = shape[0], shape[1]\n",
    "            limit = jnp.sqrt(6.0 / (fan_in + fan_out))\n",
    "            return random.uniform(key, shape, minval=-limit, maxval=limit)\n",
    "        \n",
    "        params = {\n",
    "            'W_q': init_linear(keys[0], (self.d_model, self.d_model)),\n",
    "            'W_k': init_linear(keys[1], (self.d_model, self.d_model)),\n",
    "            'W_v': init_linear(keys[2], (self.d_model, self.d_model)),\n",
    "            'W_o': init_linear(keys[3], (self.d_model, self.d_model))\n",
    "        }\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"Split the last dimension into (num_heads, d_k)\"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        x = x.reshape(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        return jnp.transpose(x, (0, 2, 1, 3))  # (batch_size, num_heads, seq_len, d_k)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"Combine heads back into single dimension\"\"\"\n",
    "        batch_size, num_heads, seq_len, d_k = x.shape\n",
    "        x = jnp.transpose(x, (0, 2, 1, 3))  # (batch_size, seq_len, num_heads, d_k)\n",
    "        return x.reshape(batch_size, seq_len, self.d_model)\n",
    "    \n",
    "    def forward(self, params, query, key, value, mask=None, key_dropout=None, training=True):\n",
    "        \"\"\"Forward pass through multi-head attention\"\"\"\n",
    "        \n",
    "        batch_size, seq_len, _ = query.shape\n",
    "        \n",
    "        # Linear transformations\n",
    "        Q = jnp.dot(query, params['W_q'])\n",
    "        K = jnp.dot(key, params['W_k'])  \n",
    "        V = jnp.dot(value, params['W_v'])\n",
    "        \n",
    "        # Split into multiple heads\n",
    "        Q = self.split_heads(Q)  # (batch_size, num_heads, seq_len, d_k)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "        \n",
    "        # Apply scaled dot-product attention\n",
    "        attention_output, attention_weights = scaled_dot_product_attention(\n",
    "            Q, K, V, mask, self.dropout_rate, key_dropout, training\n",
    "        )\n",
    "        \n",
    "        # Combine heads\n",
    "        attention_output = self.combine_heads(attention_output)\n",
    "        \n",
    "        # Final linear transformation\n",
    "        output = jnp.dot(attention_output, params['W_o'])\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "    def __call__(self, params, query, key, value, mask=None, key_dropout=None, training=True):\n",
    "        \"\"\"Make the class callable\"\"\"\n",
    "        return self.forward(params, query, key, value, mask, key_dropout, training)\n",
    "\n",
    "def test_multihead_attention():\n",
    "    \"\"\"Test multi-head attention implementation\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(456)\n",
    "    batch_size, seq_len, d_model = 2, 10, 128\n",
    "    num_heads = 8\n",
    "    \n",
    "    # Create multi-head attention layer\n",
    "    mha = MultiHeadAttention(d_model, num_heads, dropout_rate=0.1)\n",
    "    params = mha.init_params(key)\n",
    "    \n",
    "    # Create input tensors\n",
    "    x = random.normal(random.split(key)[1], (batch_size, seq_len, d_model))\n",
    "    \n",
    "    # Self-attention\n",
    "    output, weights = mha(params, x, x, x, training=False)\n",
    "    \n",
    "    print(\"Multi-Head Attention Test:\")\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Attention weights shape: {weights.shape}\")\n",
    "    print(f\"Number of heads: {num_heads}\")\n",
    "    print(f\"d_k per head: {mha.d_k}\")\n",
    "    \n",
    "    # Test with causal mask\n",
    "    causal_mask = create_causal_mask(seq_len)\n",
    "    causal_mask = jnp.expand_dims(causal_mask, (0, 1))  # Add batch and head dimensions\n",
    "    \n",
    "    output_masked, weights_masked = mha(params, x, x, x, mask=causal_mask, training=False)\n",
    "    print(f\"Masked output shape: {output_masked.shape}\")\n",
    "    \n",
    "    return mha, params\n",
    "\n",
    "mha, mha_params = test_multihead_attention()\n",
    "```\n",
    "\n",
    "## Position Encoding\n",
    "\n",
    "### Positional Encoding Implementation\n",
    "\n",
    "```python\n",
    "def positional_encoding(seq_len, d_model):\n",
    "    \"\"\"Generate sinusoidal positional encoding\"\"\"\n",
    "    \n",
    "    position = jnp.arange(seq_len)[:, None]  # (seq_len, 1)\n",
    "    div_term = jnp.exp(jnp.arange(0, d_model, 2) * -(jnp.log(10000.0) / d_model))\n",
    "    \n",
    "    pe = jnp.zeros((seq_len, d_model))\n",
    "    pe = pe.at[:, 0::2].set(jnp.sin(position * div_term))\n",
    "    pe = pe.at[:, 1::2].set(jnp.cos(position * div_term))\n",
    "    \n",
    "    return pe\n",
    "\n",
    "def learned_positional_encoding(key, seq_len, d_model):\n",
    "    \"\"\"Generate learned positional embeddings\"\"\"\n",
    "    return random.normal(key, (seq_len, d_model)) * 0.1\n",
    "\n",
    "def test_positional_encoding():\n",
    "    \"\"\"Test positional encoding implementations\"\"\"\n",
    "    \n",
    "    seq_len, d_model = 50, 128\n",
    "    \n",
    "    # Sinusoidal encoding\n",
    "    sin_pe = positional_encoding(seq_len, d_model)\n",
    "    \n",
    "    # Learned encoding\n",
    "    key = random.PRNGKey(789)\n",
    "    learned_pe = learned_positional_encoding(key, seq_len, d_model)\n",
    "    \n",
    "    print(\"Positional Encoding Test:\")\n",
    "    print(f\"Sinusoidal PE shape: {sin_pe.shape}\")\n",
    "    print(f\"Learned PE shape: {learned_pe.shape}\")\n",
    "    \n",
    "    # Visualize first few dimensions\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Plot sinusoidal encoding\n",
    "    ax1.imshow(sin_pe[:, :20].T, cmap='RdBu', aspect='auto')\n",
    "    ax1.set_title('Sinusoidal Positional Encoding')\n",
    "    ax1.set_xlabel('Position')\n",
    "    ax1.set_ylabel('Dimension')\n",
    "    \n",
    "    # Plot learned encoding\n",
    "    ax2.imshow(learned_pe[:, :20].T, cmap='RdBu', aspect='auto')\n",
    "    ax2.set_title('Learned Positional Encoding')\n",
    "    ax2.set_xlabel('Position')\n",
    "    ax2.set_ylabel('Dimension')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return sin_pe, learned_pe\n",
    "\n",
    "sin_pe, learned_pe = test_positional_encoding()\n",
    "```\n",
    "\n",
    "## Transformer Layer Components\n",
    "\n",
    "### Feed-Forward Network\n",
    "\n",
    "```python\n",
    "class FeedForward:\n",
    "    \"\"\"Position-wise feed-forward network\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, d_ff, dropout_rate=0.1, activation='gelu'):\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.activation = gelu if activation == 'gelu' else jax.nn.relu\n",
    "    \n",
    "    def init_params(self, key):\n",
    "        \"\"\"Initialize feed-forward parameters\"\"\"\n",
    "        keys = random.split(key, 2)\n",
    "        \n",
    "        # Xavier initialization\n",
    "        def init_linear(key, shape):\n",
    "            fan_in, fan_out = shape[0], shape[1]\n",
    "            limit = jnp.sqrt(6.0 / (fan_in + fan_out))\n",
    "            return random.uniform(key, shape, minval=-limit, maxval=limit)\n",
    "        \n",
    "        params = {\n",
    "            'W1': init_linear(keys[0], (self.d_model, self.d_ff)),\n",
    "            'b1': jnp.zeros(self.d_ff),\n",
    "            'W2': init_linear(keys[1], (self.d_ff, self.d_model)),\n",
    "            'b2': jnp.zeros(self.d_model)\n",
    "        }\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def forward(self, params, x, key_dropout=None, training=True):\n",
    "        \"\"\"Forward pass through feed-forward network\"\"\"\n",
    "        \n",
    "        # First linear layer + activation\n",
    "        hidden = jnp.dot(x, params['W1']) + params['b1']\n",
    "        hidden = self.activation(hidden)\n",
    "        \n",
    "        # Dropout\n",
    "        if training and self.dropout_rate > 0.0 and key_dropout is not None:\n",
    "            keep_prob = 1.0 - self.dropout_rate\n",
    "            dropout_mask = random.bernoulli(key_dropout, keep_prob, hidden.shape)\n",
    "            hidden = jnp.where(dropout_mask, hidden / keep_prob, 0.0)\n",
    "        \n",
    "        # Second linear layer\n",
    "        output = jnp.dot(hidden, params['W2']) + params['b2']\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def __call__(self, params, x, key_dropout=None, training=True):\n",
    "        \"\"\"Make the class callable\"\"\"\n",
    "        return self.forward(params, x, key_dropout, training)\n",
    "\n",
    "def test_feedforward():\n",
    "    \"\"\"Test feed-forward network\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(111)\n",
    "    batch_size, seq_len, d_model, d_ff = 2, 10, 128, 512\n",
    "    \n",
    "    ff = FeedForward(d_model, d_ff, dropout_rate=0.1)\n",
    "    params = ff.init_params(key)\n",
    "    \n",
    "    x = random.normal(random.split(key)[1], (batch_size, seq_len, d_model))\n",
    "    output = ff(params, x, training=False)\n",
    "    \n",
    "    print(\"Feed-Forward Test:\")\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"d_ff (hidden size): {d_ff}\")\n",
    "    \n",
    "    return ff, params\n",
    "\n",
    "ff, ff_params = test_feedforward()\n",
    "```\n",
    "\n",
    "### Layer Normalization\n",
    "\n",
    "```python\n",
    "def layer_norm(x, gamma, beta, eps=1e-6):\n",
    "    \"\"\"Layer normalization\"\"\"\n",
    "    mean = jnp.mean(x, axis=-1, keepdims=True)\n",
    "    var = jnp.var(x, axis=-1, keepdims=True)\n",
    "    normalized = (x - mean) / jnp.sqrt(var + eps)\n",
    "    return gamma * normalized + beta\n",
    "\n",
    "def init_layer_norm(d_model):\n",
    "    \"\"\"Initialize layer norm parameters\"\"\"\n",
    "    gamma = jnp.ones(d_model)\n",
    "    beta = jnp.zeros(d_model)\n",
    "    return {'gamma': gamma, 'beta': beta}\n",
    "\n",
    "def test_layer_norm():\n",
    "    \"\"\"Test layer normalization\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(222)\n",
    "    batch_size, seq_len, d_model = 2, 10, 64\n",
    "    \n",
    "    x = random.normal(key, (batch_size, seq_len, d_model))\n",
    "    ln_params = init_layer_norm(d_model)\n",
    "    \n",
    "    normalized = layer_norm(x, ln_params['gamma'], ln_params['beta'])\n",
    "    \n",
    "    print(\"Layer Normalization Test:\")\n",
    "    print(f\"Input mean: {jnp.mean(x):.4f}, std: {jnp.std(x):.4f}\")\n",
    "    print(f\"Output mean: {jnp.mean(normalized):.4f}, std: {jnp.std(normalized):.4f}\")\n",
    "    print(f\"Per-sample output mean: {jnp.mean(normalized, axis=-1)[0, 0]:.6f}\")\n",
    "    print(f\"Per-sample output std: {jnp.std(normalized, axis=-1)[0, 0]:.6f}\")\n",
    "\n",
    "test_layer_norm()\n",
    "```\n",
    "\n",
    "## Complete Transformer Layer\n",
    "\n",
    "### Transformer Encoder Layer\n",
    "\n",
    "```python\n",
    "class TransformerLayer:\n",
    "    \"\"\"Complete transformer encoder layer\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout_rate=0.1):\n",
    "        self.d_model = d_model\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads, dropout_rate)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout_rate)\n",
    "        self.dropout_rate = dropout_rate\n",
    "    \n",
    "    def init_params(self, key):\n",
    "        \"\"\"Initialize all layer parameters\"\"\"\n",
    "        keys = random.split(key, 4)\n",
    "        \n",
    "        params = {\n",
    "            'mha': self.mha.init_params(keys[0]),\n",
    "            'ff': self.ff.init_params(keys[1]),\n",
    "            'ln1': init_layer_norm(self.d_model),\n",
    "            'ln2': init_layer_norm(self.d_model)\n",
    "        }\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def forward(self, params, x, mask=None, key_dropout=None, training=True):\n",
    "        \"\"\"Forward pass through transformer layer\"\"\"\n",
    "        \n",
    "        if key_dropout is not None:\n",
    "            key_dropout, key_dropout_ff = random.split(key_dropout)\n",
    "        else:\n",
    "            key_dropout_ff = None\n",
    "        \n",
    "        # Multi-head attention with residual connection and layer norm\n",
    "        attn_output, attn_weights = self.mha(\n",
    "            params['mha'], x, x, x, mask, key_dropout, training\n",
    "        )\n",
    "        \n",
    "        # Residual connection + layer norm (Pre-LN variant)\n",
    "        x = layer_norm(x + attn_output, params['ln1']['gamma'], params['ln1']['beta'])\n",
    "        \n",
    "        # Feed-forward with residual connection and layer norm\n",
    "        ff_output = self.ff(params['ff'], x, key_dropout_ff, training)\n",
    "        x = layer_norm(x + ff_output, params['ln2']['gamma'], params['ln2']['beta'])\n",
    "        \n",
    "        return x, attn_weights\n",
    "    \n",
    "    def __call__(self, params, x, mask=None, key_dropout=None, training=True):\n",
    "        \"\"\"Make the class callable\"\"\"\n",
    "        return self.forward(params, x, mask, key_dropout, training)\n",
    "\n",
    "def test_transformer_layer():\n",
    "    \"\"\"Test complete transformer layer\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(333)\n",
    "    batch_size, seq_len, d_model = 2, 12, 256\n",
    "    num_heads, d_ff = 8, 1024\n",
    "    \n",
    "    transformer = TransformerLayer(d_model, num_heads, d_ff, dropout_rate=0.1)\n",
    "    params = transformer.init_params(key)\n",
    "    \n",
    "    # Create input with positional encoding\n",
    "    x = random.normal(random.split(key)[1], (batch_size, seq_len, d_model))\n",
    "    pos_encoding = positional_encoding(seq_len, d_model)\n",
    "    x_with_pos = x + pos_encoding[None, :, :]  # Add batch dimension to pos encoding\n",
    "    \n",
    "    # Forward pass\n",
    "    output, attn_weights = transformer(params, x_with_pos, training=False)\n",
    "    \n",
    "    print(\"Transformer Layer Test:\")\n",
    "    print(f\"Input shape: {x_with_pos.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "    print(f\"Input/output same shape: {x_with_pos.shape == output.shape}\")\n",
    "    \n",
    "    # Test with causal mask\n",
    "    causal_mask = create_causal_mask(seq_len)\n",
    "    causal_mask = jnp.expand_dims(causal_mask, (0, 1))\n",
    "    \n",
    "    output_masked, attn_weights_masked = transformer(\n",
    "        params, x_with_pos, mask=causal_mask, training=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Masked output shape: {output_masked.shape}\")\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = 0\n",
    "    for component, component_params in params.items():\n",
    "        component_total = sum(p.size for p in jax.tree_leaves(component_params))\n",
    "        total_params += component_total\n",
    "        print(f\"{component}: {component_total:,} parameters\")\n",
    "    \n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    return transformer, params\n",
    "\n",
    "transformer, transformer_params = test_transformer_layer()\n",
    "```\n",
    "\n",
    "## Simple Sequence-to-Sequence Task\n",
    "\n",
    "### Training a Transformer on Copy Task\n",
    "\n",
    "```python\n",
    "def create_copy_task_data(key, seq_len=10, vocab_size=20, n_samples=1000):\n",
    "    \"\"\"Create simple copy task dataset\"\"\"\n",
    "    \n",
    "    # Generate random sequences\n",
    "    sequences = random.randint(key, (n_samples, seq_len), 0, vocab_size)\n",
    "    \n",
    "    # Input: [SOS, seq], Target: [seq, EOS]\n",
    "    sos_token = vocab_size  # Special start token\n",
    "    eos_token = vocab_size + 1  # Special end token\n",
    "    \n",
    "    # Input sequences with SOS\n",
    "    inputs = jnp.concatenate([\n",
    "        jnp.full((n_samples, 1), sos_token),\n",
    "        sequences\n",
    "    ], axis=1)\n",
    "    \n",
    "    # Target sequences with EOS\n",
    "    targets = jnp.concatenate([\n",
    "        sequences,\n",
    "        jnp.full((n_samples, 1), eos_token)\n",
    "    ], axis=1)\n",
    "    \n",
    "    return inputs, targets\n",
    "\n",
    "def train_copy_task():\n",
    "    \"\"\"Train transformer on simple copy task\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(42)\n",
    "    seq_len, vocab_size = 8, 16\n",
    "    d_model, num_heads, d_ff = 64, 4, 256\n",
    "    \n",
    "    # Create transformer\n",
    "    transformer = TransformerLayer(d_model, num_heads, d_ff)\n",
    "    transformer_params = transformer.init_params(key)\n",
    "    \n",
    "    # Create embedding layer\n",
    "    embedding_key = random.split(key)[1]\n",
    "    embedding_matrix = random.normal(embedding_key, (vocab_size + 2, d_model)) * 0.1\n",
    "    \n",
    "    # Create output projection\n",
    "    output_key = random.split(key, 3)[2]\n",
    "    output_matrix = random.normal(output_key, (d_model, vocab_size + 2)) * 0.1\n",
    "    \n",
    "    # Generate data\n",
    "    data_key = random.split(key, 4)[3]\n",
    "    inputs, targets = create_copy_task_data(data_key, seq_len, vocab_size, n_samples=500)\n",
    "    \n",
    "    # Simple training function\n",
    "    def model_fn(params, inputs):\n",
    "        # Embed inputs\n",
    "        embedded = embedding_matrix[inputs]  # (batch, seq_len+1, d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        pos_enc = positional_encoding(embedded.shape[1], d_model)\n",
    "        embedded = embedded + pos_enc[None, :, :]\n",
    "        \n",
    "        # Forward through transformer\n",
    "        output, _ = transformer(params, embedded, training=True)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        logits = jnp.dot(output, output_matrix)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    # Loss function\n",
    "    def loss_fn(params, inputs, targets):\n",
    "        logits = model_fn(params, inputs)\n",
    "        targets_one_hot = jax.nn.one_hot(targets, vocab_size + 2)\n",
    "        \n",
    "        log_probs = jax.nn.log_softmax(logits)\n",
    "        loss = -jnp.mean(jnp.sum(targets_one_hot * log_probs, axis=-1))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    # Simple training loop\n",
    "    learning_rate = 0.001\n",
    "    grad_fn = grad(loss_fn)\n",
    "    \n",
    "    # Split data\n",
    "    n_train = 400\n",
    "    train_inputs, test_inputs = inputs[:n_train], inputs[n_train:]\n",
    "    train_targets, test_targets = targets[:n_train], targets[n_train:]\n",
    "    \n",
    "    print(\"Training Copy Task:\")\n",
    "    print(f\"Vocab size: {vocab_size}, Seq length: {seq_len}\")\n",
    "    print(f\"Training samples: {n_train}, Test samples: {len(test_inputs)}\")\n",
    "    \n",
    "    # Training epochs\n",
    "    for epoch in range(20):\n",
    "        # Compute gradients\n",
    "        grads = grad_fn(transformer_params, train_inputs, train_targets)\n",
    "        \n",
    "        # Simple SGD update\n",
    "        transformer_params = jax.tree_map(\n",
    "            lambda p, g: p - learning_rate * g,\n",
    "            transformer_params, grads\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        train_loss = loss_fn(transformer_params, train_inputs, train_targets)\n",
    "        test_loss = loss_fn(transformer_params, test_inputs, test_targets)\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch:2d}: train_loss={train_loss:.4f}, test_loss={test_loss:.4f}\")\n",
    "    \n",
    "    # Test model predictions\n",
    "    test_logits = model_fn(transformer_params, test_inputs[:5])\n",
    "    predictions = jnp.argmax(test_logits, axis=-1)\n",
    "    \n",
    "    print(\"\\nSample Predictions:\")\n",
    "    for i in range(3):\n",
    "        print(f\"Input:  {test_inputs[i, 1:].tolist()}\")  # Remove SOS\n",
    "        print(f\"Target: {test_targets[i, :-1].tolist()}\")  # Remove EOS\n",
    "        print(f\"Pred:   {predictions[i, :-1].tolist()}\")  # Remove EOS position\n",
    "        print()\n",
    "\n",
    "train_copy_task()\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we've implemented attention mechanisms from scratch in JAX:\n",
    "\n",
    "**Core Components:**\n",
    "\n",
    "1. **Scaled Dot-Product Attention**: Foundation of all attention mechanisms\n",
    "2. **Multi-Head Attention**: Parallel attention heads for different representation subspaces\n",
    "3. **Positional Encoding**: Sinusoidal and learned position representations\n",
    "4. **Feed-Forward Networks**: Position-wise fully connected layers\n",
    "5. **Layer Normalization**: Stabilization technique for deep networks\n",
    "\n",
    "**Transformer Architecture:**\n",
    "- Complete transformer encoder layer with residual connections\n",
    "- Pre-layer normalization variant\n",
    "- Causal masking for autoregressive generation\n",
    "- Proper parameter initialization\n",
    "\n",
    "**Key Insights:**\n",
    "- Attention allows dynamic focus on relevant input positions\n",
    "- Multi-head attention captures different types of relationships\n",
    "- Positional encoding is crucial for sequence understanding\n",
    "- Layer normalization and residuals enable deep architectures\n",
    "\n",
    "**JAX Implementation Benefits:**\n",
    "- Efficient matrix operations for attention computation\n",
    "- Automatic differentiation through complex attention mechanics\n",
    "- JIT compilation for performance optimization\n",
    "- Functional programming with clean parameter management\n",
    "\n",
    "**Training Observations:**\n",
    "- Simple copy task tests basic sequence modeling capability\n",
    "- Proper initialization crucial for attention stability\n",
    "- Positional encoding essential for sequence order understanding\n",
    "- Masking enables causal language modeling\n",
    "\n",
    "**Next Steps:**\n",
    "- The next notebook will cover optimizers in detail\n",
    "- We'll explore advanced optimization techniques for training\n",
    "- Understanding attention enables modern transformer architectures\n",
    "\n",
    "This attention implementation provides the foundation for transformer models, which have revolutionized natural language processing and are increasingly used in computer vision and other domains."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
