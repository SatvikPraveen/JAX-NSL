{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e8e00f5",
   "metadata": {},
   "source": [
    "# File: notebooks/03_neural_networks/09_cnn_minimal.ipynb\n",
    "\n",
    "## JAX Neural Networks: Minimal CNN Implementation\n",
    "\n",
    "This notebook implements Convolutional Neural Networks (CNNs) from scratch using JAX's low-level convolution operations. We'll cover convolutional layers, pooling operations, and build a complete CNN for image classification using `lax.conv_general_dilated`.\n",
    "\n",
    "CNNs are fundamental for computer vision tasks, leveraging spatial locality and parameter sharing to efficiently process images and other grid-like data structures.\n",
    "\n",
    "## Setting Up the Environment\n",
    "\n",
    "```python\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, random, lax\n",
    "from jax.nn import relu, softmax, log_softmax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Dict, Any, List\n",
    "import functools\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "```\n",
    "\n",
    "## Core Convolution Operations\n",
    "\n",
    "### Low-Level Convolution Implementation\n",
    "\n",
    "```python\n",
    "def conv2d_basic(x, kernel, stride=1, padding='VALID'):\n",
    "    \"\"\"Basic 2D convolution using lax.conv_general_dilated\"\"\"\n",
    "    return lax.conv_general_dilated(\n",
    "        x, kernel,\n",
    "        window_strides=[stride, stride],\n",
    "        padding=padding\n",
    "    )\n",
    "\n",
    "def test_convolution():\n",
    "    \"\"\"Test basic convolution operation\"\"\"\n",
    "    \n",
    "    # Create test input: batch_size=1, height=5, width=5, channels=1\n",
    "    x = jnp.ones((1, 5, 5, 1))\n",
    "    \n",
    "    # Create 3x3 edge detection kernel\n",
    "    kernel = jnp.array([\n",
    "        [[-1, -1, -1],\n",
    "         [ 0,  0,  0],\n",
    "         [ 1,  1,  1]]\n",
    "    ]).reshape(3, 3, 1, 1)  # height, width, in_channels, out_channels\n",
    "    \n",
    "    # Apply convolution\n",
    "    output = conv2d_basic(x, kernel, stride=1, padding='VALID')\n",
    "    \n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Kernel shape: {kernel.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Output:\\n{output[0, :, :, 0]}\")\n",
    "    \n",
    "    return x, kernel, output\n",
    "\n",
    "x_test, kernel_test, output_test = test_convolution()\n",
    "```\n",
    "\n",
    "### Pooling Operations\n",
    "\n",
    "```python\n",
    "def max_pool2d(x, pool_size=2, stride=None):\n",
    "    \"\"\"2D max pooling operation\"\"\"\n",
    "    if stride is None:\n",
    "        stride = pool_size\n",
    "    \n",
    "    return lax.reduce_window(\n",
    "        x, -jnp.inf, lax.max,\n",
    "        [1, pool_size, pool_size, 1],\n",
    "        [1, stride, stride, 1],\n",
    "        'VALID'\n",
    "    )\n",
    "\n",
    "def avg_pool2d(x, pool_size=2, stride=None):\n",
    "    \"\"\"2D average pooling operation\"\"\"\n",
    "    if stride is None:\n",
    "        stride = pool_size\n",
    "    \n",
    "    pooled = lax.reduce_window(\n",
    "        x, 0.0, lax.add,\n",
    "        [1, pool_size, pool_size, 1],\n",
    "        [1, stride, stride, 1],\n",
    "        'VALID'\n",
    "    )\n",
    "    return pooled / (pool_size * pool_size)\n",
    "\n",
    "def test_pooling():\n",
    "    \"\"\"Test pooling operations\"\"\"\n",
    "    \n",
    "    # Create test input with pattern\n",
    "    x = jnp.arange(16).reshape(1, 4, 4, 1).astype(jnp.float32)\n",
    "    \n",
    "    max_pooled = max_pool2d(x, pool_size=2, stride=2)\n",
    "    avg_pooled = avg_pool2d(x, pool_size=2, stride=2)\n",
    "    \n",
    "    print(\"Pooling Operations Test:\")\n",
    "    print(f\"Input (4x4):\\n{x[0, :, :, 0]}\")\n",
    "    print(f\"Max pooled (2x2):\\n{max_pooled[0, :, :, 0]}\")\n",
    "    print(f\"Avg pooled (2x2):\\n{avg_pooled[0, :, :, 0]}\")\n",
    "\n",
    "test_pooling()\n",
    "```\n",
    "\n",
    "## CNN Layer Components\n",
    "\n",
    "### Convolutional Layer Class\n",
    "\n",
    "```python\n",
    "class ConvLayer:\n",
    "    \"\"\"Convolutional layer implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, out_channels, kernel_size=3, stride=1, padding='SAME', activation='relu'):\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.activation = relu if activation == 'relu' else lambda x: x\n",
    "    \n",
    "    def init_params(self, key, input_shape):\n",
    "        \"\"\"Initialize convolution parameters using He initialization\"\"\"\n",
    "        batch_size, height, width, in_channels = input_shape\n",
    "        kernel_shape = (self.kernel_size, self.kernel_size, in_channels, self.out_channels)\n",
    "        \n",
    "        # He initialization\n",
    "        fan_in = self.kernel_size * self.kernel_size * in_channels\n",
    "        std = jnp.sqrt(2.0 / fan_in)\n",
    "        \n",
    "        W = random.normal(key, kernel_shape) * std\n",
    "        b = jnp.zeros(self.out_channels)\n",
    "        \n",
    "        return {'W': W, 'b': b}\n",
    "    \n",
    "    def forward(self, params, x):\n",
    "        \"\"\"Forward pass through convolution layer\"\"\"\n",
    "        conv_out = lax.conv_general_dilated(\n",
    "            x, params['W'],\n",
    "            window_strides=[self.stride, self.stride],\n",
    "            padding=self.padding\n",
    "        )\n",
    "        \n",
    "        # Add bias (broadcast across spatial dimensions)\n",
    "        conv_out = conv_out + params['b']\n",
    "        \n",
    "        # Apply activation\n",
    "        return self.activation(conv_out)\n",
    "    \n",
    "    def output_shape(self, input_shape):\n",
    "        \"\"\"Calculate output shape after convolution\"\"\"\n",
    "        batch_size, height, width, in_channels = input_shape\n",
    "        \n",
    "        if self.padding == 'SAME':\n",
    "            out_height = height // self.stride\n",
    "            out_width = width // self.stride\n",
    "        else:  # VALID\n",
    "            out_height = (height - self.kernel_size) // self.stride + 1\n",
    "            out_width = (width - self.kernel_size) // self.stride + 1\n",
    "        \n",
    "        return (batch_size, out_height, out_width, self.out_channels)\n",
    "\n",
    "# Test ConvLayer\n",
    "def test_conv_layer():\n",
    "    \"\"\"Test ConvLayer implementation\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(42)\n",
    "    input_shape = (32, 28, 28, 1)  # Batch of MNIST-like images\n",
    "    \n",
    "    conv_layer = ConvLayer(out_channels=16, kernel_size=3, stride=1, padding='SAME')\n",
    "    params = conv_layer.init_params(key, input_shape)\n",
    "    \n",
    "    # Create test input\n",
    "    x = random.normal(random.split(key)[1], input_shape)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = conv_layer.forward(params, x)\n",
    "    expected_shape = conv_layer.output_shape(input_shape)\n",
    "    \n",
    "    print(f\"Conv Layer Test:\")\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Expected output shape: {expected_shape}\")\n",
    "    print(f\"Actual output shape: {output.shape}\")\n",
    "    print(f\"Shapes match: {output.shape == expected_shape}\")\n",
    "    \n",
    "    return conv_layer, params\n",
    "\n",
    "conv_layer, conv_params = test_conv_layer()\n",
    "```\n",
    "\n",
    "### Pooling Layer Class\n",
    "\n",
    "```python\n",
    "class PoolingLayer:\n",
    "    \"\"\"Pooling layer implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, pool_size=2, stride=None, pool_type='max'):\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride if stride is not None else pool_size\n",
    "        self.pool_type = pool_type\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through pooling layer\"\"\"\n",
    "        if self.pool_type == 'max':\n",
    "            return max_pool2d(x, self.pool_size, self.stride)\n",
    "        elif self.pool_type == 'avg':\n",
    "            return avg_pool2d(x, self.pool_size, self.stride)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pooling type: {self.pool_type}\")\n",
    "    \n",
    "    def output_shape(self, input_shape):\n",
    "        \"\"\"Calculate output shape after pooling\"\"\"\n",
    "        batch_size, height, width, channels = input_shape\n",
    "        out_height = (height - self.pool_size) // self.stride + 1\n",
    "        out_width = (width - self.pool_size) // self.stride + 1\n",
    "        return (batch_size, out_height, out_width, channels)\n",
    "\n",
    "# Test PoolingLayer\n",
    "def test_pooling_layer():\n",
    "    \"\"\"Test PoolingLayer implementation\"\"\"\n",
    "    \n",
    "    input_shape = (32, 28, 28, 16)\n",
    "    pool_layer = PoolingLayer(pool_size=2, stride=2, pool_type='max')\n",
    "    \n",
    "    x = random.normal(random.PRNGKey(0), input_shape)\n",
    "    output = pool_layer.forward(x)\n",
    "    expected_shape = pool_layer.output_shape(input_shape)\n",
    "    \n",
    "    print(f\"Pooling Layer Test:\")\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Expected output shape: {expected_shape}\")\n",
    "    print(f\"Actual output shape: {output.shape}\")\n",
    "    print(f\"Shapes match: {output.shape == expected_shape}\")\n",
    "\n",
    "test_pooling_layer()\n",
    "```\n",
    "\n",
    "## Complete CNN Implementation\n",
    "\n",
    "### CNN Architecture\n",
    "\n",
    "```python\n",
    "class SimpleCNN:\n",
    "    \"\"\"Simple CNN implementation for image classification\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Define network architecture\n",
    "        self.conv1 = ConvLayer(out_channels=32, kernel_size=3, padding='SAME')\n",
    "        self.pool1 = PoolingLayer(pool_size=2, stride=2, pool_type='max')\n",
    "        \n",
    "        self.conv2 = ConvLayer(out_channels=64, kernel_size=3, padding='SAME')\n",
    "        self.pool2 = PoolingLayer(pool_size=2, stride=2, pool_type='max')\n",
    "        \n",
    "        self.conv3 = ConvLayer(out_channels=128, kernel_size=3, padding='SAME')\n",
    "        self.pool3 = PoolingLayer(pool_size=2, stride=2, pool_type='max')\n",
    "    \n",
    "    def init_params(self, key, input_shape):\n",
    "        \"\"\"Initialize all network parameters\"\"\"\n",
    "        keys = random.split(key, 4)  # Need keys for conv layers and dense layers\n",
    "        params = {}\n",
    "        \n",
    "        # Track shape through network\n",
    "        current_shape = input_shape\n",
    "        \n",
    "        # Conv1 + Pool1\n",
    "        params['conv1'] = self.conv1.init_params(keys[0], current_shape)\n",
    "        current_shape = self.conv1.output_shape(current_shape)\n",
    "        current_shape = self.pool1.output_shape(current_shape)\n",
    "        \n",
    "        # Conv2 + Pool2\n",
    "        params['conv2'] = self.conv2.init_params(keys[1], current_shape)\n",
    "        current_shape = self.conv2.output_shape(current_shape)\n",
    "        current_shape = self.pool2.output_shape(current_shape)\n",
    "        \n",
    "        # Conv3 + Pool3\n",
    "        params['conv3'] = self.conv3.init_params(keys[2], current_shape)\n",
    "        current_shape = self.conv3.output_shape(current_shape)\n",
    "        current_shape = self.pool3.output_shape(current_shape)\n",
    "        \n",
    "        # Flatten for dense layer\n",
    "        flattened_size = current_shape[1] * current_shape[2] * current_shape[3]\n",
    "        \n",
    "        # Dense layer for classification\n",
    "        dense_std = jnp.sqrt(2.0 / flattened_size)\n",
    "        params['dense'] = {\n",
    "            'W': random.normal(keys[3], (flattened_size, self.num_classes)) * dense_std,\n",
    "            'b': jnp.zeros(self.num_classes)\n",
    "        }\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def forward(self, params, x):\n",
    "        \"\"\"Forward pass through CNN\"\"\"\n",
    "        \n",
    "        # Conv1 + Pool1\n",
    "        x = self.conv1.forward(params['conv1'], x)\n",
    "        x = self.pool1.forward(x)\n",
    "        \n",
    "        # Conv2 + Pool2\n",
    "        x = self.conv2.forward(params['conv2'], x)\n",
    "        x = self.pool2.forward(x)\n",
    "        \n",
    "        # Conv3 + Pool3  \n",
    "        x = self.conv3.forward(params['conv3'], x)\n",
    "        x = self.pool3.forward(x)\n",
    "        \n",
    "        # Flatten\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.reshape(batch_size, -1)\n",
    "        \n",
    "        # Dense layer\n",
    "        x = x @ params['dense']['W'] + params['dense']['b']\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def __call__(self, params, x):\n",
    "        \"\"\"Make CNN callable\"\"\"\n",
    "        return self.forward(params, x)\n",
    "\n",
    "# Test CNN\n",
    "def test_cnn():\n",
    "    \"\"\"Test complete CNN implementation\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(123)\n",
    "    input_shape = (8, 32, 32, 3)  # Small batch of RGB images\n",
    "    \n",
    "    cnn = SimpleCNN(num_classes=10)\n",
    "    params = cnn.init_params(key, input_shape)\n",
    "    \n",
    "    # Create test input\n",
    "    x = random.normal(random.split(key)[1], input_shape)\n",
    "    \n",
    "    # Forward pass\n",
    "    logits = cnn.forward(params, x)\n",
    "    \n",
    "    print(f\"CNN Test:\")\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output logits shape: {logits.shape}\")\n",
    "    \n",
    "    # Test with softmax\n",
    "    probs = softmax(logits)\n",
    "    print(f\"Output probabilities shape: {probs.shape}\")\n",
    "    print(f\"Probabilities sum to 1: {jnp.allclose(jnp.sum(probs, axis=1), 1.0)}\")\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = 0\n",
    "    for layer_name, layer_params in params.items():\n",
    "        layer_total = sum(p.size for p in layer_params.values())\n",
    "        total_params += layer_total\n",
    "        print(f\"{layer_name}: {layer_total:,} parameters\")\n",
    "    \n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    return cnn, params\n",
    "\n",
    "cnn, cnn_params = test_cnn()\n",
    "```\n",
    "\n",
    "## Training Implementation\n",
    "\n",
    "### Loss Functions and Training Loop\n",
    "\n",
    "```python\n",
    "def cross_entropy_loss(logits, labels):\n",
    "    \"\"\"Cross-entropy loss for classification\"\"\"\n",
    "    log_probs = log_softmax(logits)\n",
    "    return -jnp.mean(jnp.sum(labels * log_probs, axis=1))\n",
    "\n",
    "def accuracy(logits, labels):\n",
    "    \"\"\"Compute classification accuracy\"\"\"\n",
    "    predicted_class = jnp.argmax(logits, axis=1)\n",
    "    true_class = jnp.argmax(labels, axis=1)\n",
    "    return jnp.mean(predicted_class == true_class)\n",
    "\n",
    "class Adam:\n",
    "    \"\"\"Adam optimizer for CNN training\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "    \n",
    "    def init_state(self, params):\n",
    "        \"\"\"Initialize optimizer state\"\"\"\n",
    "        return {\n",
    "            'm': jax.tree_map(jnp.zeros_like, params),\n",
    "            'v': jax.tree_map(jnp.zeros_like, params),\n",
    "            'step': 0\n",
    "        }\n",
    "    \n",
    "    def update(self, grads, state, params):\n",
    "        \"\"\"Update parameters using Adam\"\"\"\n",
    "        step = state['step'] + 1\n",
    "        \n",
    "        m = jax.tree_map(\n",
    "            lambda m_prev, g: self.beta1 * m_prev + (1 - self.beta1) * g,\n",
    "            state['m'], grads\n",
    "        )\n",
    "        v = jax.tree_map(\n",
    "            lambda v_prev, g: self.beta2 * v_prev + (1 - self.beta2) * g**2,\n",
    "            state['v'], grads\n",
    "        )\n",
    "        \n",
    "        # Bias correction\n",
    "        m_hat = jax.tree_map(lambda m_val: m_val / (1 - self.beta1**step), m)\n",
    "        v_hat = jax.tree_map(lambda v_val: v_val / (1 - self.beta2**step), v)\n",
    "        \n",
    "        # Parameter update\n",
    "        new_params = jax.tree_map(\n",
    "            lambda p, m_val, v_val: p - self.learning_rate * m_val / (jnp.sqrt(v_val) + self.eps),\n",
    "            params, m_hat, v_hat\n",
    "        )\n",
    "        \n",
    "        new_state = {'m': m, 'v': v, 'step': step}\n",
    "        return new_params, new_state\n",
    "\n",
    "def train_cnn(cnn, train_data, test_data, num_epochs=10, batch_size=32, learning_rate=0.001):\n",
    "    \"\"\"Train CNN on dataset\"\"\"\n",
    "    \n",
    "    X_train, y_train = train_data\n",
    "    X_test, y_test = test_data\n",
    "    \n",
    "    # Initialize parameters and optimizer\n",
    "    key = random.PRNGKey(42)\n",
    "    params = cnn.init_params(key, (batch_size,) + X_train.shape[1:])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    opt_state = optimizer.init_state(params)\n",
    "    \n",
    "    # JIT compile training step\n",
    "    @jit\n",
    "    def train_step(params, opt_state, batch_x, batch_y):\n",
    "        def loss_fn(params):\n",
    "            logits = cnn(params, batch_x)\n",
    "            return cross_entropy_loss(logits, batch_y)\n",
    "        \n",
    "        loss, grads = jax.value_and_grad(loss_fn)(params)\n",
    "        new_params, new_opt_state = optimizer.update(grads, opt_state, params)\n",
    "        return new_params, new_opt_state, loss\n",
    "    \n",
    "    # JIT compile evaluation\n",
    "    @jit \n",
    "    def eval_step(params, x, y):\n",
    "        logits = cnn(params, x)\n",
    "        loss = cross_entropy_loss(logits, y)\n",
    "        acc = accuracy(logits, y)\n",
    "        return loss, acc\n",
    "    \n",
    "    # Training loop\n",
    "    n_train = len(X_train)\n",
    "    n_batches = n_train // batch_size\n",
    "    \n",
    "    history = {'train_losses': [], 'test_losses': [], 'test_accuracies': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle training data\n",
    "        perm = random.permutation(key, n_train)\n",
    "        key = random.split(key)[0]\n",
    "        \n",
    "        X_shuffled = X_train[perm]\n",
    "        y_shuffled = y_train[perm]\n",
    "        \n",
    "        # Training batches\n",
    "        epoch_losses = []\n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            \n",
    "            batch_x = X_shuffled[start_idx:end_idx]\n",
    "            batch_y = y_shuffled[start_idx:end_idx]\n",
    "            \n",
    "            params, opt_state, batch_loss = train_step(params, opt_state, batch_x, batch_y)\n",
    "            epoch_losses.append(batch_loss)\n",
    "        \n",
    "        # Record metrics\n",
    "        avg_train_loss = jnp.mean(jnp.array(epoch_losses))\n",
    "        test_loss, test_acc = eval_step(params, X_test, y_test)\n",
    "        \n",
    "        history['train_losses'].append(avg_train_loss)\n",
    "        history['test_losses'].append(test_loss)\n",
    "        history['test_accuracies'].append(test_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:2d}: train_loss={avg_train_loss:.4f}, \"\n",
    "              f\"test_loss={test_loss:.4f}, test_acc={test_acc:.4f}\")\n",
    "    \n",
    "    return params, history\n",
    "```\n",
    "\n",
    "## Synthetic Dataset Example\n",
    "\n",
    "### Create and Train on Synthetic Data\n",
    "\n",
    "```python\n",
    "def create_synthetic_image_data(key, n_samples=1000, image_size=32, n_classes=5):\n",
    "    \"\"\"Create synthetic image classification dataset\"\"\"\n",
    "    \n",
    "    # Generate random images with class-dependent patterns\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for class_idx in range(n_classes):\n",
    "        class_key = random.split(key, n_classes)[class_idx]\n",
    "        n_class_samples = n_samples // n_classes\n",
    "        \n",
    "        for i in range(n_class_samples):\n",
    "            sample_key = random.split(class_key, n_class_samples)[i]\n",
    "            \n",
    "            # Base noise\n",
    "            img = 0.1 * random.normal(sample_key, (image_size, image_size, 3))\n",
    "            \n",
    "            # Add class-specific pattern\n",
    "            if class_idx == 0:  # Horizontal stripes\n",
    "                img = img.at[::4, :, 0].set(1.0)\n",
    "            elif class_idx == 1:  # Vertical stripes  \n",
    "                img = img.at[:, ::4, 1].set(1.0)\n",
    "            elif class_idx == 2:  # Checkerboard\n",
    "                img = img.at[::8, ::8, 2].set(1.0)\n",
    "                img = img.at[4::8, 4::8, 2].set(1.0)\n",
    "            elif class_idx == 3:  # Circular pattern\n",
    "                center = image_size // 2\n",
    "                y, x = jnp.mgrid[:image_size, :image_size]\n",
    "                mask = (x - center)**2 + (y - center)**2 < (image_size//4)**2\n",
    "                img = img.at[mask, :].set([1.0, 1.0, 0.0])\n",
    "            else:  # Random bright spots\n",
    "                bright_key = random.split(sample_key)[1]\n",
    "                bright_locs = random.randint(bright_key, (10, 2), 0, image_size)\n",
    "                for loc in bright_locs:\n",
    "                    img = img.at[loc[0], loc[1], :].set(1.0)\n",
    "            \n",
    "            images.append(img)\n",
    "            labels.append(class_idx)\n",
    "    \n",
    "    images = jnp.array(images)\n",
    "    labels = jax.nn.one_hot(jnp.array(labels), n_classes)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "# Create synthetic dataset\n",
    "key = random.PRNGKey(0)\n",
    "X_synth, y_synth = create_synthetic_image_data(key, n_samples=500, image_size=32, n_classes=5)\n",
    "\n",
    "# Split train/test\n",
    "split_idx = int(0.8 * len(X_synth))\n",
    "X_train = X_synth[:split_idx]\n",
    "y_train = y_synth[:split_idx]\n",
    "X_test = X_synth[split_idx:]\n",
    "y_test = y_synth[split_idx:]\n",
    "\n",
    "print(f\"Synthetic Dataset:\")\n",
    "print(f\"Training: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing: {X_test.shape[0]} samples\")\n",
    "print(f\"Image shape: {X_train.shape[1:]}\")\n",
    "\n",
    "# Train CNN on synthetic data\n",
    "print(f\"\\nTraining CNN...\")\n",
    "cnn_synthetic = SimpleCNN(num_classes=5)\n",
    "trained_params, training_history = train_cnn(\n",
    "    cnn_synthetic, \n",
    "    (X_train, y_train),\n",
    "    (X_test, y_test),\n",
    "    num_epochs=15,\n",
    "    batch_size=16,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal test accuracy: {training_history['test_accuracies'][-1]:.4f}\")\n",
    "```\n",
    "\n",
    "## Feature Visualization\n",
    "\n",
    "### Visualize Learned Filters\n",
    "\n",
    "```python\n",
    "def visualize_conv_filters(params, layer_name='conv1', max_filters=8):\n",
    "    \"\"\"Visualize learned convolutional filters\"\"\"\n",
    "    \n",
    "    filters = params[layer_name]['W']  # Shape: (H, W, in_channels, out_channels)\n",
    "    n_filters = min(filters.shape[-1], max_filters)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i in range(n_filters):\n",
    "        filter_weights = filters[:, :, 0, i]  # Take first input channel\n",
    "        \n",
    "        axes[i].imshow(filter_weights, cmap='RdBu', vmin=-0.5, vmax=0.5)\n",
    "        axes[i].set_title(f'Filter {i+1}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'{layer_name.upper()} Learned Filters')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize learned filters\n",
    "visualize_conv_filters(trained_params, 'conv1', max_filters=8)\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we've implemented a complete CNN from scratch in JAX:\n",
    "\n",
    "**Core Components:**\n",
    "\n",
    "1. **Convolution Operations**: Using `lax.conv_general_dilated` for efficient convolutions\n",
    "2. **Pooling Operations**: Max and average pooling with `lax.reduce_window`  \n",
    "3. **Layer Classes**: Modular ConvLayer and PoolingLayer implementations\n",
    "4. **CNN Architecture**: Complete SimpleCNN with multiple conv-pool blocks\n",
    "\n",
    "**Key Features:**\n",
    "- Proper weight initialization (He initialization for ReLU)\n",
    "- Flexible layer configuration (kernel size, stride, padding)\n",
    "- Efficient JAX operations with JIT compilation\n",
    "- Complete training loop with Adam optimizer\n",
    "\n",
    "**JAX Advantages:**\n",
    "- Automatic differentiation through convolutions\n",
    "- JIT compilation for performance\n",
    "- Functional programming with immutable parameters\n",
    "- Easy vectorization for batch processing\n",
    "\n",
    "**Training Insights:**\n",
    "- CNNs learn hierarchical features (edges → patterns → objects)\n",
    "- Pooling reduces spatial dimensions while preserving important features\n",
    "- Proper initialization crucial for training stability\n",
    "- Batch processing enables efficient GPU utilization\n",
    "\n",
    "**Next Steps:**\n",
    "- The next notebook will implement attention mechanisms\n",
    "- We'll explore self-attention and transformer architectures\n",
    "- Understanding CNNs provides foundation for modern computer vision\n",
    "\n",
    "This minimal CNN implementation demonstrates JAX's capability for implementing complex neural architectures while maintaining clarity and performance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
