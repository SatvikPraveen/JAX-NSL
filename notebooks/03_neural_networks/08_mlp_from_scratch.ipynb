{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f799c65f",
   "metadata": {},
   "source": [
    "# File: notebooks/03_neural_networks/08_mlp_from_scratch.ipynb\n",
    "\n",
    "## JAX Neural Networks: MLP from Scratch\n",
    "\n",
    "Welcome to the neural networks section! This notebook implements a Multi-Layer Perceptron (MLP) from scratch using pure JAX. We'll cover weight initialization, forward propagation, backpropagation, and training loops while leveraging JAX's autodiff capabilities.\n",
    "\n",
    "Building neural networks from scratch in JAX provides deep understanding of the underlying mechanics while showcasing JAX's functional programming approach and automatic differentiation.\n",
    "\n",
    "## Setting Up the Environment\n",
    "\n",
    "```python\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, random, lax\n",
    "from jax.nn import relu, sigmoid, softmax, log_softmax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Callable, Dict, Any\n",
    "import functools\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "```\n",
    "\n",
    "## Neural Network Components\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "```python\n",
    "# Custom activation functions\n",
    "def swish(x):\n",
    "    \"\"\"Swish activation: x * sigmoid(x)\"\"\"\n",
    "    return x * sigmoid(x)\n",
    "\n",
    "def gelu(x):\n",
    "    \"\"\"Gaussian Error Linear Unit\"\"\"\n",
    "    return 0.5 * x * (1 + jnp.tanh(jnp.sqrt(2/jnp.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    \"\"\"Leaky ReLU with negative slope alpha\"\"\"\n",
    "    return jnp.where(x > 0, x, alpha * x)\n",
    "\n",
    "# Activation function registry\n",
    "ACTIVATIONS = {\n",
    "    'relu': relu,\n",
    "    'sigmoid': sigmoid,\n",
    "    'tanh': jnp.tanh,\n",
    "    'swish': swish,\n",
    "    'gelu': gelu,\n",
    "    'leaky_relu': leaky_relu,\n",
    "    'linear': lambda x: x\n",
    "}\n",
    "\n",
    "def test_activations():\n",
    "    \"\"\"Test and visualize activation functions\"\"\"\n",
    "    x = jnp.linspace(-3, 3, 100)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, (name, func) in enumerate(ACTIVATIONS.items()):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "        y = func(x)\n",
    "        axes[i].plot(x, y, label=name, linewidth=2)\n",
    "        axes[i].set_title(f'{name.capitalize()} Activation')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        axes[i].set_xlabel('x')\n",
    "        axes[i].set_ylabel('f(x)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Test activations\n",
    "test_activations()\n",
    "```\n",
    "\n",
    "### Weight Initialization\n",
    "\n",
    "```python\n",
    "def xavier_uniform_init(key, shape, gain=1.0):\n",
    "    \"\"\"Xavier/Glorot uniform initialization\"\"\"\n",
    "    fan_in, fan_out = shape[0], shape[1]\n",
    "    limit = gain * jnp.sqrt(6.0 / (fan_in + fan_out))\n",
    "    return random.uniform(key, shape, minval=-limit, maxval=limit)\n",
    "\n",
    "def xavier_normal_init(key, shape, gain=1.0):\n",
    "    \"\"\"Xavier/Glorot normal initialization\"\"\"\n",
    "    fan_in, fan_out = shape[0], shape[1]\n",
    "    std = gain * jnp.sqrt(2.0 / (fan_in + fan_out))\n",
    "    return random.normal(key, shape) * std\n",
    "\n",
    "def he_uniform_init(key, shape, gain=1.0):\n",
    "    \"\"\"He uniform initialization (good for ReLU)\"\"\"\n",
    "    fan_in = shape[0]\n",
    "    limit = gain * jnp.sqrt(6.0 / fan_in)\n",
    "    return random.uniform(key, shape, minval=-limit, maxval=limit)\n",
    "\n",
    "def he_normal_init(key, shape, gain=1.0):\n",
    "    \"\"\"He normal initialization (good for ReLU)\"\"\"\n",
    "    fan_in = shape[0]\n",
    "    std = gain * jnp.sqrt(2.0 / fan_in)\n",
    "    return random.normal(key, shape) * std\n",
    "\n",
    "def lecun_normal_init(key, shape, gain=1.0):\n",
    "    \"\"\"LeCun normal initialization\"\"\"\n",
    "    fan_in = shape[0]\n",
    "    std = gain * jnp.sqrt(1.0 / fan_in)\n",
    "    return random.normal(key, shape) * std\n",
    "\n",
    "# Initialization registry\n",
    "INITIALIZERS = {\n",
    "    'xavier_uniform': xavier_uniform_init,\n",
    "    'xavier_normal': xavier_normal_init,\n",
    "    'he_uniform': he_uniform_init,\n",
    "    'he_normal': he_normal_init,\n",
    "    'lecun_normal': lecun_normal_init,\n",
    "    'zeros': lambda key, shape: jnp.zeros(shape),\n",
    "    'ones': lambda key, shape: jnp.ones(shape),\n",
    "    'normal': lambda key, shape: random.normal(key, shape) * 0.01\n",
    "}\n",
    "\n",
    "def compare_initializations():\n",
    "    \"\"\"Compare different weight initialization schemes\"\"\"\n",
    "    key = random.PRNGKey(42)\n",
    "    shape = (784, 256)  # Input to hidden layer\n",
    "    \n",
    "    print(\"Weight Initialization Comparison:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for name, init_func in INITIALIZERS.items():\n",
    "        if name in ['zeros', 'ones']:\n",
    "            continue  # Skip trivial cases\n",
    "            \n",
    "        weights = init_func(key, shape)\n",
    "        \n",
    "        mean_val = jnp.mean(weights)\n",
    "        std_val = jnp.std(weights)\n",
    "        min_val = jnp.min(weights)\n",
    "        max_val = jnp.max(weights)\n",
    "        \n",
    "        print(f\"{name:15}: mean={mean_val:7.4f}, std={std_val:.4f}, \"\n",
    "              f\"range=[{min_val:6.3f}, {max_val:6.3f}]\")\n",
    "\n",
    "compare_initializations()\n",
    "```\n",
    "\n",
    "## MLP Implementation\n",
    "\n",
    "### Core MLP Class\n",
    "\n",
    "```python\n",
    "class MLP:\n",
    "    \"\"\"Multi-Layer Perceptron implementation in JAX\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 layer_sizes: List[int],\n",
    "                 activation: str = 'relu',\n",
    "                 output_activation: str = 'linear',\n",
    "                 weight_init: str = 'he_normal',\n",
    "                 bias_init: str = 'zeros'):\n",
    "        \n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activation = ACTIVATIONS[activation]\n",
    "        self.output_activation = ACTIVATIONS[output_activation]\n",
    "        self.weight_init = INITIALIZERS[weight_init]\n",
    "        self.bias_init = INITIALIZERS[bias_init]\n",
    "        self.num_layers = len(layer_sizes) - 1\n",
    "    \n",
    "    def init_params(self, key: jax.random.PRNGKey) -> Dict[str, Any]:\n",
    "        \"\"\"Initialize network parameters\"\"\"\n",
    "        keys = random.split(key, 2 * self.num_layers)\n",
    "        params = {}\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            layer_name = f'layer_{i}'\n",
    "            input_size = self.layer_sizes[i]\n",
    "            output_size = self.layer_sizes[i + 1]\n",
    "            \n",
    "            # Initialize weights and biases\n",
    "            W = self.weight_init(keys[2*i], (input_size, output_size))\n",
    "            b = self.bias_init(keys[2*i + 1], (output_size,))\n",
    "            \n",
    "            params[layer_name] = {'W': W, 'b': b}\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def forward(self, params: Dict[str, Any], x: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        for i in range(self.num_layers):\n",
    "            layer_name = f'layer_{i}'\n",
    "            W = params[layer_name]['W']\n",
    "            b = params[layer_name]['b']\n",
    "            \n",
    "            # Linear transformation\n",
    "            x = x @ W + b\n",
    "            \n",
    "            # Apply activation\n",
    "            if i < self.num_layers - 1:  # Hidden layers\n",
    "                x = self.activation(x)\n",
    "            else:  # Output layer\n",
    "                x = self.output_activation(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def __call__(self, params: Dict[str, Any], x: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"Make the MLP callable\"\"\"\n",
    "        return self.forward(params, x)\n",
    "\n",
    "# Test MLP creation and forward pass\n",
    "def test_mlp():\n",
    "    \"\"\"Test MLP initialization and forward pass\"\"\"\n",
    "    \n",
    "    # Create MLP for MNIST-like classification\n",
    "    mlp = MLP(layer_sizes=[784, 256, 128, 10], \n",
    "              activation='relu',\n",
    "              output_activation='linear',\n",
    "              weight_init='he_normal')\n",
    "    \n",
    "    # Initialize parameters\n",
    "    key = random.PRNGKey(0)\n",
    "    params = mlp.init_params(key)\n",
    "    \n",
    "    print(\"MLP Architecture:\")\n",
    "    print(f\"Layers: {mlp.layer_sizes}\")\n",
    "    print(f\"Activation: {mlp.activation.__name__}\")\n",
    "    \n",
    "    # Test forward pass\n",
    "    batch_size = 32\n",
    "    input_dim = 784\n",
    "    x = random.normal(random.split(key)[1], (batch_size, input_dim))\n",
    "    \n",
    "    output = mlp.forward(params, x)\n",
    "    print(f\"\\nForward pass test:\")\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = 0\n",
    "    for layer_name, layer_params in params.items():\n",
    "        W_params = layer_params['W'].size\n",
    "        b_params = layer_params['b'].size\n",
    "        layer_total = W_params + b_params\n",
    "        total_params += layer_total\n",
    "        print(f\"{layer_name}: W{layer_params['W'].shape} + b{layer_params['b'].shape} = {layer_total} params\")\n",
    "    \n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    return mlp, params\n",
    "\n",
    "mlp, params = test_mlp()\n",
    "```\n",
    "\n",
    "## Loss Functions\n",
    "\n",
    "### Common Loss Functions\n",
    "\n",
    "```python\n",
    "def mse_loss(predictions: jnp.ndarray, targets: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"Mean Squared Error loss\"\"\"\n",
    "    return jnp.mean((predictions - targets) ** 2)\n",
    "\n",
    "def mae_loss(predictions: jnp.ndarray, targets: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"Mean Absolute Error loss\"\"\"\n",
    "    return jnp.mean(jnp.abs(predictions - targets))\n",
    "\n",
    "def cross_entropy_loss(logits: jnp.ndarray, labels: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"Cross-entropy loss for classification\"\"\"\n",
    "    # Numerically stable implementation\n",
    "    log_probs = log_softmax(logits)\n",
    "    return -jnp.mean(jnp.sum(labels * log_probs, axis=1))\n",
    "\n",
    "def sparse_cross_entropy_loss(logits: jnp.ndarray, labels: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"Cross-entropy loss with integer labels\"\"\"\n",
    "    log_probs = log_softmax(logits)\n",
    "    num_classes = logits.shape[1]\n",
    "    one_hot_labels = jax.nn.one_hot(labels, num_classes)\n",
    "    return cross_entropy_loss(logits, one_hot_labels)\n",
    "\n",
    "def huber_loss(predictions: jnp.ndarray, targets: jnp.ndarray, delta: float = 1.0) -> jnp.ndarray:\n",
    "    \"\"\"Huber loss (smooth L1 loss)\"\"\"\n",
    "    residual = jnp.abs(predictions - targets)\n",
    "    return jnp.mean(\n",
    "        jnp.where(residual < delta,\n",
    "                  0.5 * residual ** 2,\n",
    "                  delta * (residual - 0.5 * delta))\n",
    "    )\n",
    "\n",
    "# Loss function registry\n",
    "LOSSES = {\n",
    "    'mse': mse_loss,\n",
    "    'mae': mae_loss,\n",
    "    'cross_entropy': cross_entropy_loss,\n",
    "    'sparse_cross_entropy': sparse_cross_entropy_loss,\n",
    "    'huber': huber_loss\n",
    "}\n",
    "\n",
    "def test_losses():\n",
    "    \"\"\"Test loss function implementations\"\"\"\n",
    "    key = random.PRNGKey(123)\n",
    "    \n",
    "    # Test regression losses\n",
    "    y_pred = random.normal(key, (100, 1))\n",
    "    y_true = random.normal(random.split(key)[1], (100, 1))\n",
    "    \n",
    "    print(\"Regression Loss Functions:\")\n",
    "    for name in ['mse', 'mae', 'huber']:\n",
    "        loss_val = LOSSES[name](y_pred, y_true)\n",
    "        print(f\"{name.upper():6}: {loss_val:.4f}\")\n",
    "    \n",
    "    # Test classification losses\n",
    "    logits = random.normal(random.split(key, 3)[2], (100, 10))\n",
    "    labels_one_hot = jax.nn.one_hot(random.randint(random.split(key, 4)[3], (100,), 0, 10), 10)\n",
    "    labels_sparse = jnp.argmax(labels_one_hot, axis=1)\n",
    "    \n",
    "    print(\"\\nClassification Loss Functions:\")\n",
    "    ce_loss = cross_entropy_loss(logits, labels_one_hot)\n",
    "    sce_loss = sparse_cross_entropy_loss(logits, labels_sparse)\n",
    "    \n",
    "    print(f\"Cross-entropy: {ce_loss:.4f}\")\n",
    "    print(f\"Sparse CE:     {sce_loss:.4f}\")\n",
    "    print(f\"Difference:    {jnp.abs(ce_loss - sce_loss):.6f}\")\n",
    "\n",
    "test_losses()\n",
    "```\n",
    "\n",
    "## Training Implementation\n",
    "\n",
    "### Optimizer Implementation\n",
    "\n",
    "```python\n",
    "class SGD:\n",
    "    \"\"\"Stochastic Gradient Descent optimizer\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 0.01, momentum: float = 0.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "    \n",
    "    def init_state(self, params):\n",
    "        \"\"\"Initialize optimizer state\"\"\"\n",
    "        if self.momentum > 0:\n",
    "            return jax.tree_map(jnp.zeros_like, params)\n",
    "        return {}\n",
    "    \n",
    "    def update(self, grads, state, params):\n",
    "        \"\"\"Update parameters using gradients\"\"\"\n",
    "        if self.momentum > 0:\n",
    "            # Momentum update\n",
    "            new_state = jax.tree_map(\n",
    "                lambda v, g: self.momentum * v + g,\n",
    "                state, grads\n",
    "            )\n",
    "            new_params = jax.tree_map(\n",
    "                lambda p, v: p - self.learning_rate * v,\n",
    "                params, new_state\n",
    "            )\n",
    "            return new_params, new_state\n",
    "        else:\n",
    "            # Simple SGD\n",
    "            new_params = jax.tree_map(\n",
    "                lambda p, g: p - self.learning_rate * g,\n",
    "                params, grads\n",
    "            )\n",
    "            return new_params, state\n",
    "\n",
    "class Adam:\n",
    "    \"\"\"Adam optimizer\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 0.001, \n",
    "                 beta1: float = 0.9, beta2: float = 0.999, \n",
    "                 eps: float = 1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "    \n",
    "    def init_state(self, params):\n",
    "        \"\"\"Initialize Adam state\"\"\"\n",
    "        return {\n",
    "            'm': jax.tree_map(jnp.zeros_like, params),  # First moment\n",
    "            'v': jax.tree_map(jnp.zeros_like, params),  # Second moment\n",
    "            'step': 0\n",
    "        }\n",
    "    \n",
    "    def update(self, grads, state, params):\n",
    "        \"\"\"Adam parameter update\"\"\"\n",
    "        step = state['step'] + 1\n",
    "        \n",
    "        # Update biased first and second moments\n",
    "        m = jax.tree_map(\n",
    "            lambda m_prev, g: self.beta1 * m_prev + (1 - self.beta1) * g,\n",
    "            state['m'], grads\n",
    "        )\n",
    "        v = jax.tree_map(\n",
    "            lambda v_prev, g: self.beta2 * v_prev + (1 - self.beta2) * g**2,\n",
    "            state['v'], grads\n",
    "        )\n",
    "        \n",
    "        # Bias correction\n",
    "        m_hat = jax.tree_map(lambda m_val: m_val / (1 - self.beta1**step), m)\n",
    "        v_hat = jax.tree_map(lambda v_val: v_val / (1 - self.beta2**step), v)\n",
    "        \n",
    "        # Parameter update\n",
    "        new_params = jax.tree_map(\n",
    "            lambda p, m_val, v_val: p - self.learning_rate * m_val / (jnp.sqrt(v_val) + self.eps),\n",
    "            params, m_hat, v_hat\n",
    "        )\n",
    "        \n",
    "        new_state = {'m': m, 'v': v, 'step': step}\n",
    "        return new_params, new_state\n",
    "```\n",
    "\n",
    "### Training Loop\n",
    "\n",
    "```python\n",
    "def train_mlp(mlp: MLP, \n",
    "              train_data: Tuple[jnp.ndarray, jnp.ndarray],\n",
    "              test_data: Tuple[jnp.ndarray, jnp.ndarray],\n",
    "              optimizer,\n",
    "              loss_fn: Callable,\n",
    "              num_epochs: int = 100,\n",
    "              batch_size: int = 32,\n",
    "              key: jax.random.PRNGKey = random.PRNGKey(0)):\n",
    "    \"\"\"Train MLP with given data and optimizer\"\"\"\n",
    "    \n",
    "    X_train, y_train = train_data\n",
    "    X_test, y_test = test_data\n",
    "    \n",
    "    # Initialize parameters and optimizer state\n",
    "    params = mlp.init_params(key)\n",
    "    opt_state = optimizer.init_state(params)\n",
    "    \n",
    "    # JIT compile training step\n",
    "    @jit\n",
    "    def train_step(params, opt_state, batch_x, batch_y):\n",
    "        def loss_fn_params(params):\n",
    "            predictions = mlp(params, batch_x)\n",
    "            return loss_fn(predictions, batch_y)\n",
    "        \n",
    "        loss, grads = jax.value_and_grad(loss_fn_params)(params)\n",
    "        new_params, new_opt_state = optimizer.update(grads, opt_state, params)\n",
    "        return new_params, new_opt_state, loss\n",
    "    \n",
    "    # JIT compile evaluation\n",
    "    @jit\n",
    "    def eval_step(params, x, y):\n",
    "        predictions = mlp(params, x)\n",
    "        loss = loss_fn(predictions, y)\n",
    "        \n",
    "        # Compute accuracy for classification\n",
    "        if len(y.shape) > 1 and y.shape[1] > 1:  # One-hot labels\n",
    "            pred_labels = jnp.argmax(predictions, axis=1)\n",
    "            true_labels = jnp.argmax(y, axis=1)\n",
    "        else:  # Regression or sparse labels\n",
    "            pred_labels = jnp.argmax(predictions, axis=1)\n",
    "            true_labels = y.flatten()\n",
    "        \n",
    "        accuracy = jnp.mean(pred_labels == true_labels)\n",
    "        return loss, accuracy\n",
    "    \n",
    "    # Training history\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    # Training loop\n",
    "    n_train = len(X_train)\n",
    "    n_batches = n_train // batch_size\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle training data\n",
    "        perm = random.permutation(key, n_train)\n",
    "        key = random.split(key)[0]\n",
    "        X_shuffled = X_train[perm]\n",
    "        y_shuffled = y_train[perm]\n",
    "        \n",
    "        # Mini-batch training\n",
    "        epoch_losses = []\n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            \n",
    "            batch_x = X_shuffled[start_idx:end_idx]\n",
    "            batch_y = y_shuffled[start_idx:end_idx]\n",
    "            \n",
    "            params, opt_state, batch_loss = train_step(params, opt_state, batch_x, batch_y)\n",
    "            epoch_losses.append(batch_loss)\n",
    "        \n",
    "        # Record training loss\n",
    "        avg_train_loss = jnp.mean(jnp.array(epoch_losses))\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_loss, test_accuracy = eval_step(params, X_test, y_test)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "            print(f\"Epoch {epoch:3d}: train_loss={avg_train_loss:.4f}, \"\n",
    "                  f\"test_loss={test_loss:.4f}, test_acc={test_accuracy:.4f}\")\n",
    "    \n",
    "    return params, {\n",
    "        'train_losses': jnp.array(train_losses),\n",
    "        'test_losses': jnp.array(test_losses),\n",
    "        'test_accuracies': jnp.array(test_accuracies)\n",
    "    }\n",
    "```\n",
    "\n",
    "## Practical Example: Classification\n",
    "\n",
    "### Generate Synthetic Dataset\n",
    "\n",
    "```python\n",
    "def generate_classification_data(key, n_samples=1000, n_features=20, n_classes=3):\n",
    "    \"\"\"Generate synthetic classification dataset\"\"\"\n",
    "    \n",
    "    # Generate class centers\n",
    "    centers = random.normal(key, (n_classes, n_features)) * 2\n",
    "    \n",
    "    # Generate samples\n",
    "    samples_per_class = n_samples // n_classes\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for class_idx in range(n_classes):\n",
    "        # Generate samples around class center\n",
    "        class_samples = centers[class_idx] + random.normal(\n",
    "            random.split(key, n_classes + 1)[class_idx + 1], \n",
    "            (samples_per_class, n_features)\n",
    "        )\n",
    "        X.append(class_samples)\n",
    "        y.extend([class_idx] * samples_per_class)\n",
    "    \n",
    "    X = jnp.concatenate(X, axis=0)\n",
    "    y = jnp.array(y)\n",
    "    \n",
    "    # Shuffle data\n",
    "    perm = random.permutation(random.split(key)[-1], len(X))\n",
    "    X = X[perm]\n",
    "    y = y[perm]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate dataset\n",
    "key = random.PRNGKey(42)\n",
    "X, y = generate_classification_data(key, n_samples=2000, n_features=20, n_classes=3)\n",
    "\n",
    "# Split into train/test\n",
    "split_idx = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "# Convert to one-hot labels\n",
    "y_train_oh = jax.nn.one_hot(y_train, 3)\n",
    "y_test_oh = jax.nn.one_hot(y_test, 3)\n",
    "\n",
    "print(f\"Dataset: {len(X)} samples, {X.shape[1]} features, {len(jnp.unique(y))} classes\")\n",
    "print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")\n",
    "```\n",
    "\n",
    "### Train and Evaluate\n",
    "\n",
    "```python\n",
    "# Create and train MLP\n",
    "mlp_classifier = MLP(\n",
    "    layer_sizes=[20, 64, 32, 3],\n",
    "    activation='relu',\n",
    "    output_activation='linear',\n",
    "    weight_init='he_normal'\n",
    ")\n",
    "\n",
    "# Train with SGD\n",
    "print(\"Training with SGD:\")\n",
    "sgd_optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
    "params_sgd, history_sgd = train_mlp(\n",
    "    mlp_classifier, \n",
    "    (X_train, y_train_oh), \n",
    "    (X_test, y_test_oh),\n",
    "    sgd_optimizer,\n",
    "    cross_entropy_loss,\n",
    "    num_epochs=100,\n",
    "    batch_size=32,\n",
    "    key=random.PRNGKey(123)\n",
    ")\n",
    "\n",
    "print(\"\\nTraining with Adam:\")\n",
    "adam_optimizer = Adam(learning_rate=0.001)\n",
    "params_adam, history_adam = train_mlp(\n",
    "    mlp_classifier,\n",
    "    (X_train, y_train_oh),\n",
    "    (X_test, y_test_oh), \n",
    "    adam_optimizer,\n",
    "    cross_entropy_loss,\n",
    "    num_epochs=100,\n",
    "    batch_size=32,\n",
    "    key=random.PRNGKey(123)\n",
    ")\n",
    "\n",
    "# Compare optimizers\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"SGD  - Test Accuracy: {history_sgd['test_accuracies'][-1]:.4f}\")\n",
    "print(f\"Adam - Test Accuracy: {history_adam['test_accuracies'][-1]:.4f}\")\n",
    "```\n",
    "\n",
    "## Advanced Features\n",
    "\n",
    "### Regularization\n",
    "\n",
    "```python\n",
    "def l2_regularization(params, weight_decay=0.01):\n",
    "    \"\"\"Compute L2 regularization penalty\"\"\"\n",
    "    l2_loss = 0.0\n",
    "    for layer_params in params.values():\n",
    "        l2_loss += weight_decay * jnp.sum(layer_params['W']**2)\n",
    "    return l2_loss\n",
    "\n",
    "def dropout(key, x, rate=0.5, training=True):\n",
    "    \"\"\"Apply dropout to layer activations\"\"\"\n",
    "    if not training or rate == 0.0:\n",
    "        return x\n",
    "    \n",
    "    keep_rate = 1.0 - rate\n",
    "    mask = random.bernoulli(key, keep_rate, x.shape)\n",
    "    return jnp.where(mask, x / keep_rate, 0.0)\n",
    "\n",
    "class RegularizedMLP(MLP):\n",
    "    \"\"\"MLP with regularization support\"\"\"\n",
    "    \n",
    "    def __init__(self, *args, dropout_rate=0.0, weight_decay=0.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.weight_decay = weight_decay\n",
    "    \n",
    "    def forward(self, params, x, key=None, training=True):\n",
    "        \"\"\"Forward pass with dropout\"\"\"\n",
    "        if key is None:\n",
    "            key = random.PRNGKey(0)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            layer_name = f'layer_{i}'\n",
    "            W = params[layer_name]['W']\n",
    "            b = params[layer_name]['b']\n",
    "            \n",
    "            # Linear transformation\n",
    "            x = x @ W + b\n",
    "            \n",
    "            # Apply activation\n",
    "            if i < self.num_layers - 1:\n",
    "                x = self.activation(x)\n",
    "                # Apply dropout to hidden layers\n",
    "                if self.dropout_rate > 0 and training:\n",
    "                    key, dropout_key = random.split(key)\n",
    "                    x = dropout(dropout_key, x, self.dropout_rate, training)\n",
    "            else:\n",
    "                x = self.output_activation(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def loss_with_regularization(self, params, x, y, key=None, training=True):\n",
    "        \"\"\"Compute loss with L2 regularization\"\"\"\n",
    "        predictions = self.forward(params, x, key, training)\n",
    "        base_loss = cross_entropy_loss(predictions, y)\n",
    "        reg_loss = l2_regularization(params, self.weight_decay)\n",
    "        return base_loss + reg_loss\n",
    "\n",
    "# Test regularized MLP\n",
    "regularized_mlp = RegularizedMLP(\n",
    "    layer_sizes=[20, 128, 64, 3],\n",
    "    activation='relu',\n",
    "    dropout_rate=0.3,\n",
    "    weight_decay=0.001\n",
    ")\n",
    "\n",
    "print(\"Regularized MLP created with dropout=0.3, weight_decay=0.001\")\n",
    "```\n",
    "\n",
    "### Batch Normalization\n",
    "\n",
    "```python\n",
    "def batch_norm(x, gamma, beta, running_mean, running_var, training=True, momentum=0.9, eps=1e-5):\n",
    "    \"\"\"Batch normalization implementation\"\"\"\n",
    "    \n",
    "    if training:\n",
    "        # Compute batch statistics\n",
    "        batch_mean = jnp.mean(x, axis=0, keepdims=True)\n",
    "        batch_var = jnp.var(x, axis=0, keepdims=True)\n",
    "        \n",
    "        # Update running statistics\n",
    "        new_running_mean = momentum * running_mean + (1 - momentum) * batch_mean\n",
    "        new_running_var = momentum * running_var + (1 - momentum) * batch_var\n",
    "        \n",
    "        # Normalize using batch stats\n",
    "        x_norm = (x - batch_mean) / jnp.sqrt(batch_var + eps)\n",
    "    else:\n",
    "        # Use running statistics for inference\n",
    "        x_norm = (x - running_mean) / jnp.sqrt(running_var + eps)\n",
    "        new_running_mean = running_mean\n",
    "        new_running_var = running_var\n",
    "    \n",
    "    # Scale and shift\n",
    "    out = gamma * x_norm + beta\n",
    "    \n",
    "    return out, new_running_mean, new_running_var\n",
    "\n",
    "class BatchNormMLP(MLP):\n",
    "    \"\"\"MLP with batch normalization\"\"\"\n",
    "    \n",
    "    def init_params(self, key):\n",
    "        \"\"\"Initialize parameters including batch norm params\"\"\"\n",
    "        params = super().init_params(key)\n",
    "        \n",
    "        # Add batch norm parameters for hidden layers\n",
    "        for i in range(self.num_layers - 1):  # Exclude output layer\n",
    "            layer_size = self.layer_sizes[i + 1]\n",
    "            bn_name = f'bn_{i}'\n",
    "            \n",
    "            params[bn_name] = {\n",
    "                'gamma': jnp.ones((1, layer_size)),\n",
    "                'beta': jnp.zeros((1, layer_size)),\n",
    "                'running_mean': jnp.zeros((1, layer_size)),\n",
    "                'running_var': jnp.ones((1, layer_size))\n",
    "            }\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def forward(self, params, x, training=True):\n",
    "        \"\"\"Forward pass with batch normalization\"\"\"\n",
    "        for i in range(self.num_layers):\n",
    "            layer_name = f'layer_{i}'\n",
    "            W = params[layer_name]['W']\n",
    "            b = params[layer_name]['b']\n",
    "            \n",
    "            # Linear transformation\n",
    "            x = x @ W + b\n",
    "            \n",
    "            # Apply batch norm to hidden layers\n",
    "            if i < self.num_layers - 1:\n",
    "                bn_name = f'bn_{i}'\n",
    "                bn_params = params[bn_name]\n",
    "                \n",
    "                x, new_running_mean, new_running_var = batch_norm(\n",
    "                    x, bn_params['gamma'], bn_params['beta'],\n",
    "                    bn_params['running_mean'], bn_params['running_var'],\n",
    "                    training=training\n",
    "                )\n",
    "                \n",
    "                # Update running statistics (in practice, this would be handled by the training loop)\n",
    "                params[bn_name]['running_mean'] = new_running_mean\n",
    "                params[bn_name]['running_var'] = new_running_var\n",
    "                \n",
    "                x = self.activation(x)\n",
    "            else:\n",
    "                x = self.output_activation(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"Batch Normalization MLP implementation ready\")\n",
    "```\n",
    "\n",
    "## Model Analysis and Visualization\n",
    "\n",
    "### Gradient Analysis\n",
    "\n",
    "```python\n",
    "def analyze_gradients(mlp, params, X, y, loss_fn):\n",
    "    \"\"\"Analyze gradient magnitudes across layers\"\"\"\n",
    "    \n",
    "    def loss_fn_params(params):\n",
    "        predictions = mlp(params, X)\n",
    "        return loss_fn(predictions, y)\n",
    "    \n",
    "    grads = grad(loss_fn_params)(params)\n",
    "    \n",
    "    print(\"Gradient Analysis:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    for layer_name in sorted(params.keys()):\n",
    "        if 'layer_' in layer_name:\n",
    "            W_grad = grads[layer_name]['W']\n",
    "            b_grad = grads[layer_name]['b']\n",
    "            \n",
    "            W_grad_norm = jnp.linalg.norm(W_grad)\n",
    "            b_grad_norm = jnp.linalg.norm(b_grad)\n",
    "            \n",
    "            print(f\"{layer_name}: W_grad_norm={W_grad_norm:.6f}, b_grad_norm={b_grad_norm:.6f}\")\n",
    "\n",
    "# Analyze gradients for trained model\n",
    "analyze_gradients(mlp_classifier, params_adam, X_test, y_test_oh, cross_entropy_loss)\n",
    "```\n",
    "\n",
    "### Learning Curves Visualization\n",
    "\n",
    "```python\n",
    "def plot_learning_curves(history_sgd, history_adam):\n",
    "    \"\"\"Plot training curves comparing optimizers\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    epochs = range(len(history_sgd['train_losses']))\n",
    "    \n",
    "    # Training loss\n",
    "    axes[0].plot(epochs, history_sgd['train_losses'], label='SGD', alpha=0.7)\n",
    "    axes[0].plot(epochs, history_adam['train_losses'], label='Adam', alpha=0.7)\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Training Loss')\n",
    "    axes[0].set_title('Training Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Test loss\n",
    "    axes[1].plot(epochs, history_sgd['test_losses'], label='SGD', alpha=0.7)\n",
    "    axes[1].plot(epochs, history_adam['test_losses'], label='Adam', alpha=0.7)\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Test Loss') \n",
    "    axes[1].set_title('Test Loss')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Test accuracy\n",
    "    axes[2].plot(epochs, history_sgd['test_accuracies'], label='SGD', alpha=0.7)\n",
    "    axes[2].plot(epochs, history_adam['test_accuracies'], label='Adam', alpha=0.7)\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('Test Accuracy')\n",
    "    axes[2].set_title('Test Accuracy')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot learning curves\n",
    "plot_learning_curves(history_sgd, history_adam)\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we've built a complete MLP implementation from scratch in JAX:\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "1. **Activation Functions**: ReLU, sigmoid, tanh, swish, GELU, leaky ReLU\n",
    "2. **Weight Initialization**: Xavier, He, LeCun initialization schemes\n",
    "3. **MLP Architecture**: Flexible multi-layer implementation\n",
    "4. **Loss Functions**: MSE, MAE, cross-entropy, sparse cross-entropy, Huber loss\n",
    "5. **Optimizers**: SGD with momentum, Adam optimizer\n",
    "\n",
    "**Advanced Features:**\n",
    "- L2 regularization and dropout for overfitting prevention\n",
    "- Batch normalization for training stability\n",
    "- Gradient analysis for debugging\n",
    "- Learning curve visualization\n",
    "\n",
    "**JAX-Specific Advantages:**\n",
    "- Automatic differentiation eliminates manual backpropagation\n",
    "- JIT compilation for performance optimization\n",
    "- Functional programming approach with immutable parameters\n",
    "- Easy vectorization with vmap for batch processing\n",
    "\n",
    "**Best Practices Demonstrated:**\n",
    "- Proper weight initialization for different activations\n",
    "- Numerically stable loss function implementations\n",
    "- Modular design with separate components\n",
    "- Comprehensive training and evaluation loops\n",
    "\n",
    "**Performance Insights:**\n",
    "- Adam typically converges faster than SGD\n",
    "- Proper initialization prevents vanishing/exploding gradients\n",
    "- Regularization improves generalization\n",
    "- Batch normalization stabilizes training\n",
    "\n",
    "**Next Steps:**\n",
    "- The next notebook will cover CNN implementation\n",
    "- We'll explore convolutional layers and image processing\n",
    "- Understanding MLPs provides foundation for more complex architectures\n",
    "\n",
    "This MLP implementation demonstrates JAX's power for neural network development, combining ease of use with high performance and automatic differentiation capabilities."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
