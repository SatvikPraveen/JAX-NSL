{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98d834e4",
   "metadata": {},
   "source": [
    "# File: notebooks/01_fundamentals/03_custom_vjp_jvp.ipynb\n",
    "\n",
    "## JAX Fundamentals: Custom VJP and JVP\n",
    "\n",
    "Welcome to the third notebook in the JAX-NSL series! In this notebook, we'll dive deep into custom Vector-Jacobian Products (VJP) and Jacobian-Vector Products (JVP) in JAX. These are advanced autodiff concepts that allow you to define custom differentiation rules for functions, which is essential for implementing special operations, numerical algorithms, and optimization techniques that require custom gradient behavior.\n",
    "\n",
    "Understanding VJP and JVP is crucial for implementing custom layers, special mathematical functions, and optimized gradient computations that go beyond JAX's automatic differentiation capabilities.\n",
    "\n",
    "## Setting Up the Environment\n",
    "\n",
    "```python\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jacfwd, jacrev, vmap, jvp, vjp\n",
    "from jax import custom_vjp, custom_jvp\n",
    "from jax import random, lax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, Tuple, Any\n",
    "import functools\n",
    "\n",
    "# Enable 64-bit precision\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Devices available: {jax.devices()}\")\n",
    "```\n",
    "\n",
    "## Understanding VJP and JVP\n",
    "\n",
    "### Vector-Jacobian Products (VJP) - Reverse Mode\n",
    "\n",
    "```python\n",
    "# Basic VJP example\n",
    "def simple_function(x):\n",
    "    \"\"\"f(x) = [x^2, sin(x), exp(x)]\"\"\"\n",
    "    return jnp.array([x**2, jnp.sin(x), jnp.exp(x)])\n",
    "\n",
    "# Compute VJP manually\n",
    "def demonstrate_vjp():\n",
    "    x = 2.0\n",
    "    \n",
    "    # Forward pass and VJP function\n",
    "    y, vjp_fn = vjp(simple_function, x)\n",
    "    \n",
    "    # Vector for left multiplication (cotangent)\n",
    "    v = jnp.array([1.0, 0.5, 0.1])  # Some cotangent vector\n",
    "    \n",
    "    # Compute VJP: v^T * J\n",
    "    vjp_result = vjp_fn(v)[0]  # Returns tuple, take first element\n",
    "    \n",
    "    print(f\"Input x: {x}\")\n",
    "    print(f\"Function output y: {y}\")\n",
    "    print(f\"Cotangent vector v: {v}\")\n",
    "    print(f\"VJP result (v^T * J): {vjp_result}\")\n",
    "    \n",
    "    # Manual verification\n",
    "    # J = [2x, cos(x), exp(x)] at x=2\n",
    "    manual_jacobian = jnp.array([2*x, jnp.cos(x), jnp.exp(x)])\n",
    "    manual_vjp = jnp.dot(v, manual_jacobian)\n",
    "    print(f\"Manual VJP calculation: {manual_vjp}\")\n",
    "    print(f\"Results match: {jnp.allclose(vjp_result, manual_vjp)}\")\n",
    "\n",
    "demonstrate_vjp()\n",
    "```\n",
    "\n",
    "### Jacobian-Vector Products (JVP) - Forward Mode\n",
    "\n",
    "```python\n",
    "# Basic JVP example  \n",
    "def demonstrate_jvp():\n",
    "    x = 2.0\n",
    "    \n",
    "    # Tangent vector (direction for directional derivative)\n",
    "    v = 1.5\n",
    "    \n",
    "    # Compute JVP: J * v\n",
    "    y, jvp_result = jvp(simple_function, (x,), (v,))\n",
    "    \n",
    "    print(f\"Input x: {x}\")\n",
    "    print(f\"Tangent vector v: {v}\")\n",
    "    print(f\"Function output y: {y}\")\n",
    "    print(f\"JVP result (J * v): {jvp_result}\")\n",
    "    \n",
    "    # Manual verification\n",
    "    # J = [2x, cos(x), exp(x)] at x=2\n",
    "    manual_jacobian = jnp.array([2*x, jnp.cos(x), jnp.exp(x)])\n",
    "    manual_jvp = manual_jacobian * v\n",
    "    print(f\"Manual JVP calculation: {manual_jvp}\")\n",
    "    print(f\"Results match: {jnp.allclose(jvp_result, manual_jvp)}\")\n",
    "\n",
    "demonstrate_jvp()\n",
    "```\n",
    "\n",
    "## Custom VJP Implementation\n",
    "\n",
    "### Basic Custom VJP\n",
    "\n",
    "```python\n",
    "# Example 1: Custom square function with custom VJP\n",
    "@custom_vjp\n",
    "def custom_square(x):\n",
    "    \"\"\"Square function with custom backward pass\"\"\"\n",
    "    return x * x\n",
    "\n",
    "def custom_square_fwd(x):\n",
    "    \"\"\"Forward pass: return (output, residual_data)\"\"\"\n",
    "    return x * x, x  # Save input for backward pass\n",
    "\n",
    "def custom_square_bwd(residual, cotangent):\n",
    "    \"\"\"Backward pass: compute VJP given residual and cotangent\"\"\"\n",
    "    x = residual  # Retrieve saved input\n",
    "    return (2 * x * cotangent,)  # Return tuple of partial derivatives\n",
    "\n",
    "# Register the custom VJP\n",
    "custom_square.defvjp(custom_square_fwd, custom_square_bwd)\n",
    "\n",
    "# Test the custom function\n",
    "x = 3.0\n",
    "result = custom_square(x)\n",
    "grad_result = grad(custom_square)(x)\n",
    "\n",
    "print(f\"custom_square({x}) = {result}\")\n",
    "print(f\"grad(custom_square)({x}) = {grad_result}\")\n",
    "print(f\"Expected gradient: {2 * x}\")\n",
    "```\n",
    "\n",
    "### Advanced Custom VJP Example: Matrix Square Root\n",
    "\n",
    "```python\n",
    "# Custom matrix square root with stable gradients\n",
    "@custom_vjp\n",
    "def matrix_sqrt(A):\n",
    "    \"\"\"Compute matrix square root A^(1/2)\"\"\"\n",
    "    # Use eigendecomposition for symmetric positive definite matrices\n",
    "    eigvals, eigvecs = jnp.linalg.eigh(A)\n",
    "    sqrt_eigvals = jnp.sqrt(jnp.maximum(eigvals, 1e-12))  # Numerical stability\n",
    "    return eigvecs @ jnp.diag(sqrt_eigvals) @ eigvecs.T\n",
    "\n",
    "def matrix_sqrt_fwd(A):\n",
    "    \"\"\"Forward pass for matrix square root\"\"\"\n",
    "    X = matrix_sqrt(A)\n",
    "    return X, (A, X)  # Return result and save both input and output\n",
    "\n",
    "def matrix_sqrt_bwd(residuals, cotangent):\n",
    "    \"\"\"Backward pass using Sylvester equation solution\"\"\"\n",
    "    A, X = residuals\n",
    "    G = cotangent  # Cotangent matrix\n",
    "    \n",
    "    # Solve Sylvester equation: XY + YX = G for Y\n",
    "    # This gives us the gradient dL/dA = Y\n",
    "    def sylvester_solve(X, G):\n",
    "        # For small matrices, use direct solve\n",
    "        # In practice, you'd use more sophisticated solvers\n",
    "        n = X.shape[0]\n",
    "        I = jnp.eye(n)\n",
    "        \n",
    "        # Vectorize: vec(XY + YX) = (I⊗X + X⊗I)vec(Y) = vec(G)\n",
    "        kron_term = jnp.kron(I, X) + jnp.kron(X, I)\n",
    "        vec_G = G.flatten()\n",
    "        vec_Y = jnp.linalg.solve(kron_term, vec_G)\n",
    "        Y = vec_Y.reshape(n, n)\n",
    "        return Y\n",
    "    \n",
    "    Y = sylvester_solve(X, G)\n",
    "    return (Y,)\n",
    "\n",
    "# Register custom VJP\n",
    "matrix_sqrt.defvjp(matrix_sqrt_fwd, matrix_sqrt_bwd)\n",
    "\n",
    "# Test with a simple symmetric positive definite matrix\n",
    "A = jnp.array([[4.0, 1.0], [1.0, 2.0]])\n",
    "X = matrix_sqrt(A)\n",
    "\n",
    "print(f\"Matrix A:\\n{A}\")\n",
    "print(f\"Matrix square root X:\\n{X}\")\n",
    "print(f\"Verification X @ X:\\n{X @ X}\")\n",
    "print(f\"Error: {jnp.max(jnp.abs(X @ X - A))}\")\n",
    "\n",
    "# Test gradient computation\n",
    "def trace_matrix_sqrt(A):\n",
    "    return jnp.trace(matrix_sqrt(A))\n",
    "\n",
    "grad_trace = grad(trace_matrix_sqrt)(A)\n",
    "print(f\"Gradient of trace(sqrt(A)):\\n{grad_trace}\")\n",
    "```\n",
    "\n",
    "## Custom JVP Implementation\n",
    "\n",
    "### Basic Custom JVP\n",
    "\n",
    "```python\n",
    "# Example: Custom exponential with custom JVP\n",
    "@custom_jvp\n",
    "def custom_exp(x):\n",
    "    \"\"\"Exponential function with custom forward mode\"\"\"\n",
    "    return jnp.exp(x)\n",
    "\n",
    "@custom_exp.defjvp\n",
    "def custom_exp_jvp(primals, tangents):\n",
    "    \"\"\"Custom JVP rule for exponential function\"\"\"\n",
    "    x, = primals\n",
    "    x_dot, = tangents\n",
    "    \n",
    "    # Forward pass\n",
    "    exp_x = jnp.exp(x)\n",
    "    \n",
    "    # JVP: d/dx[exp(x)] = exp(x), so JVP = exp(x) * x_dot\n",
    "    exp_x_dot = exp_x * x_dot\n",
    "    \n",
    "    return exp_x, exp_x_dot\n",
    "\n",
    "# Test custom JVP\n",
    "x = 1.0\n",
    "tangent = 0.5\n",
    "\n",
    "# Using JVP\n",
    "result, jvp_result = jvp(custom_exp, (x,), (tangent,))\n",
    "print(f\"custom_exp({x}) = {result}\")\n",
    "print(f\"JVP with tangent {tangent}: {jvp_result}\")\n",
    "\n",
    "# Compare with automatic differentiation\n",
    "auto_jvp = jvp(jnp.exp, (x,), (tangent,))\n",
    "print(f\"Automatic JVP: {auto_jvp}\")\n",
    "```\n",
    "\n",
    "### Advanced Custom JVP: Iterative Algorithm\n",
    "\n",
    "```python\n",
    "# Custom JVP for iterative square root computation\n",
    "@custom_jvp  \n",
    "def iterative_sqrt(x, num_iters=10):\n",
    "    \"\"\"Compute sqrt using Newton-Raphson iterations\"\"\"\n",
    "    # Newton-Raphson: x_{n+1} = 0.5 * (x_n + a/x_n)\n",
    "    estimate = x / 2.0  # Initial guess\n",
    "    \n",
    "    for _ in range(num_iters):\n",
    "        estimate = 0.5 * (estimate + x / estimate)\n",
    "    \n",
    "    return estimate\n",
    "\n",
    "@iterative_sqrt.defjvp\n",
    "def iterative_sqrt_jvp(primals, tangents):\n",
    "    \"\"\"Custom JVP for iterative sqrt\"\"\"\n",
    "    x, num_iters = primals\n",
    "    x_dot, _ = tangents  # num_iters is not differentiable\n",
    "    \n",
    "    # Forward pass\n",
    "    sqrt_x = iterative_sqrt(x, num_iters)\n",
    "    \n",
    "    # JVP: d/dx[sqrt(x)] = 1/(2*sqrt(x))\n",
    "    sqrt_x_dot = x_dot / (2 * sqrt_x)\n",
    "    \n",
    "    return sqrt_x, sqrt_x_dot\n",
    "\n",
    "# Test iterative sqrt\n",
    "x = 9.0\n",
    "tangent = 1.0\n",
    "\n",
    "result, jvp_result = jvp(iterative_sqrt, (x, 10), (tangent, 0))\n",
    "print(f\"iterative_sqrt({x}) = {result}\")\n",
    "print(f\"JVP result: {jvp_result}\")\n",
    "print(f\"True sqrt: {jnp.sqrt(x)}\")\n",
    "print(f\"True derivative: {1/(2*jnp.sqrt(x))}\")\n",
    "```\n",
    "\n",
    "## Combining Custom VJP and JVP\n",
    "\n",
    "### Bi-directional Custom Rules\n",
    "\n",
    "```python\n",
    "# Function that supports both custom VJP and JVP\n",
    "@custom_vjp\n",
    "@custom_jvp\n",
    "def special_function(x):\n",
    "    \"\"\"f(x) = x * log(1 + exp(x)) - numerically stable version\"\"\"\n",
    "    # This is a numerically stable version of x * log(1 + exp(x))\n",
    "    return jnp.where(x > 10, x**2, x * jnp.log1p(jnp.exp(x)))\n",
    "\n",
    "# Custom JVP\n",
    "@special_function.defjvp\n",
    "def special_function_jvp(primals, tangents):\n",
    "    x, = primals\n",
    "    x_dot, = tangents\n",
    "    \n",
    "    # Forward pass\n",
    "    y = special_function(x)\n",
    "    \n",
    "    # Derivative: d/dx[x * log(1 + exp(x))] = log(1 + exp(x)) + x * exp(x)/(1 + exp(x))\n",
    "    #           = log(1 + exp(x)) + x * sigmoid(x)\n",
    "    sigmoid_x = jax.nn.sigmoid(x)\n",
    "    dy_dx = jnp.log1p(jnp.exp(jnp.minimum(x, 10))) + x * sigmoid_x\n",
    "    \n",
    "    return y, dy_dx * x_dot\n",
    "\n",
    "# Custom VJP  \n",
    "def special_function_fwd(x):\n",
    "    return special_function(x), x\n",
    "\n",
    "def special_function_bwd(x, cotangent):\n",
    "    sigmoid_x = jax.nn.sigmoid(x)\n",
    "    dy_dx = jnp.log1p(jnp.exp(jnp.minimum(x, 10))) + x * sigmoid_x\n",
    "    return (dy_dx * cotangent,)\n",
    "\n",
    "special_function.defvjp(special_function_fwd, special_function_bwd)\n",
    "\n",
    "# Test both directions\n",
    "x = 2.0\n",
    "\n",
    "# Test JVP\n",
    "tangent = 1.5\n",
    "y, jvp_result = jvp(special_function, (x,), (tangent,))\n",
    "print(f\"JVP test: f({x}) = {y}, JVP = {jvp_result}\")\n",
    "\n",
    "# Test VJP via grad\n",
    "grad_result = grad(special_function)(x)\n",
    "print(f\"VJP test: grad(f)({x}) = {grad_result}\")\n",
    "\n",
    "# Verify consistency\n",
    "print(f\"JVP/VJP consistency: {jnp.allclose(jvp_result/tangent, grad_result)}\")\n",
    "```\n",
    "\n",
    "## Practical Applications\n",
    "\n",
    "### Custom Activation Function\n",
    "\n",
    "```python\n",
    "# Custom activation function with optimized gradients\n",
    "@custom_vjp\n",
    "def swish_custom(x):\n",
    "    \"\"\"Swish activation: x * sigmoid(x)\"\"\"\n",
    "    return x * jax.nn.sigmoid(x)\n",
    "\n",
    "def swish_fwd(x):\n",
    "    \"\"\"Forward pass for Swish\"\"\"\n",
    "    sigmoid_x = jax.nn.sigmoid(x)\n",
    "    return x * sigmoid_x, (x, sigmoid_x)\n",
    "\n",
    "def swish_bwd(residuals, cotangent):\n",
    "    \"\"\"Backward pass for Swish with optimized computation\"\"\"\n",
    "    x, sigmoid_x = residuals\n",
    "    \n",
    "    # Derivative: d/dx[x * sigmoid(x)] = sigmoid(x) + x * sigmoid(x) * (1 - sigmoid(x))\n",
    "    #           = sigmoid(x) * (1 + x * (1 - sigmoid(x)))\n",
    "    derivative = sigmoid_x * (1 + x * (1 - sigmoid_x))\n",
    "    \n",
    "    return (derivative * cotangent,)\n",
    "\n",
    "swish_custom.defvjp(swish_fwd, swish_bwd)\n",
    "\n",
    "# Compare with automatic differentiation\n",
    "def swish_auto(x):\n",
    "    return x * jax.nn.sigmoid(x)\n",
    "\n",
    "# Test both versions\n",
    "x_test = jnp.linspace(-5, 5, 100)\n",
    "\n",
    "# Forward pass\n",
    "y_custom = swish_custom(x_test)\n",
    "y_auto = swish_auto(x_test)\n",
    "\n",
    "print(f\"Forward pass error: {jnp.max(jnp.abs(y_custom - y_auto))}\")\n",
    "\n",
    "# Gradient computation\n",
    "grad_custom = vmap(grad(swish_custom))(x_test)\n",
    "grad_auto = vmap(grad(swish_auto))(x_test)\n",
    "\n",
    "print(f\"Gradient error: {jnp.max(jnp.abs(grad_custom - grad_auto))}\")\n",
    "```\n",
    "\n",
    "### Implicit Function Differentiation\n",
    "\n",
    "```python\n",
    "# Solve implicit equation F(x, y) = 0 and differentiate through the solution\n",
    "@custom_vjp\n",
    "def implicit_solve(x):\n",
    "    \"\"\"Solve y^3 + y - x = 0 for y given x using Newton-Raphson\"\"\"\n",
    "    \n",
    "    def residual(y, x):\n",
    "        return y**3 + y - x\n",
    "    \n",
    "    def residual_derivative(y):\n",
    "        return 3*y**2 + 1\n",
    "    \n",
    "    # Newton-Raphson iterations\n",
    "    y = x / 2  # Initial guess\n",
    "    for _ in range(10):  # Fixed number of iterations\n",
    "        f = residual(y, x)\n",
    "        df = residual_derivative(y)\n",
    "        y = y - f / df\n",
    "    \n",
    "    return y\n",
    "\n",
    "def implicit_solve_fwd(x):\n",
    "    \"\"\"Forward pass\"\"\"\n",
    "    y = implicit_solve(x)\n",
    "    return y, (x, y)\n",
    "\n",
    "def implicit_solve_bwd(residuals, cotangent):\n",
    "    \"\"\"Backward pass using implicit function theorem\"\"\"\n",
    "    x, y = residuals\n",
    "    \n",
    "    # From F(x, y) = 0, we have dF/dx + dF/dy * dy/dx = 0\n",
    "    # So dy/dx = -dF/dx / dF/dy\n",
    "    \n",
    "    # F(x, y) = y^3 + y - x\n",
    "    dF_dx = -1.0\n",
    "    dF_dy = 3*y**2 + 1\n",
    "    \n",
    "    dy_dx = -dF_dx / dF_dy  # = 1 / (3*y^2 + 1)\n",
    "    \n",
    "    return (dy_dx * cotangent,)\n",
    "\n",
    "implicit_solve.defvjp(implicit_solve_fwd, implicit_solve_bwd)\n",
    "\n",
    "# Test the implicit solver\n",
    "x_vals = jnp.array([1.0, 2.0, 5.0, 10.0])\n",
    "\n",
    "for x in x_vals:\n",
    "    y = implicit_solve(x)\n",
    "    \n",
    "    # Verify solution\n",
    "    residual = y**3 + y - x\n",
    "    \n",
    "    # Compute gradient\n",
    "    dy_dx = grad(implicit_solve)(x)\n",
    "    \n",
    "    print(f\"x = {x:4.1f}: y = {y:6.3f}, residual = {residual:8.2e}, dy/dx = {dy_dx:6.3f}\")\n",
    "```\n",
    "\n",
    "### Efficient Linear System Solver\n",
    "\n",
    "```python\n",
    "# Custom linear solver with efficient VJP\n",
    "@custom_vjp\n",
    "def solve_linear_system(A, b):\n",
    "    \"\"\"Solve Ax = b using Cholesky decomposition\"\"\"\n",
    "    return jnp.linalg.solve(A, b)\n",
    "\n",
    "def solve_fwd(A, b):\n",
    "    \"\"\"Forward pass for linear solve\"\"\"\n",
    "    x = jnp.linalg.solve(A, b)\n",
    "    return x, (A, b, x)\n",
    "\n",
    "def solve_bwd(residuals, x_cotangent):\n",
    "    \"\"\"Backward pass for linear solve using efficient method\"\"\"\n",
    "    A, b, x = residuals\n",
    "    \n",
    "    # Gradient computation:\n",
    "    # If x = A^(-1)b, then dx = -A^(-1) dA A^(-1) b + A^(-1) db\n",
    "    #                       = -A^(-1) dA x + A^(-1) db\n",
    "    \n",
    "    # For efficiency, solve A^T lambda = x_cotangent instead of computing A^(-1)\n",
    "    lambda_vec = jnp.linalg.solve(A.T, x_cotangent)\n",
    "    \n",
    "    # dL/dA = -lambda * x^T\n",
    "    dA = -jnp.outer(lambda_vec, x)\n",
    "    \n",
    "    # dL/db = lambda\n",
    "    db = lambda_vec\n",
    "    \n",
    "    return dA, db\n",
    "\n",
    "solve_linear_system.defvjp(solve_fwd, solve_bwd)\n",
    "\n",
    "# Test linear solver\n",
    "key = random.PRNGKey(42)\n",
    "n = 5\n",
    "\n",
    "# Generate a well-conditioned positive definite matrix\n",
    "A_base = random.normal(key, (n, n))\n",
    "A = A_base @ A_base.T + jnp.eye(n)  # Make positive definite\n",
    "b = random.normal(random.split(key)[1], (n,))\n",
    "\n",
    "# Solve system\n",
    "x = solve_linear_system(A, b)\n",
    "\n",
    "print(f\"Solution x: {x}\")\n",
    "print(f\"Residual ||Ax - b||: {jnp.linalg.norm(A @ x - b)}\")\n",
    "\n",
    "# Test gradient computation\n",
    "def objective(A, b):\n",
    "    x = solve_linear_system(A, b)\n",
    "    return jnp.sum(x**2)\n",
    "\n",
    "grad_A, grad_b = grad(objective, argnums=(0, 1))(A, b)\n",
    "print(f\"Gradient w.r.t. A shape: {grad_A.shape}\")\n",
    "print(f\"Gradient w.r.t. b shape: {grad_b.shape}\")\n",
    "```\n",
    "\n",
    "## Debugging and Validation\n",
    "\n",
    "### Gradient Checking\n",
    "\n",
    "```python\n",
    "def finite_difference_check(func, x, eps=1e-5):\n",
    "    \"\"\"Check gradients using finite differences\"\"\"\n",
    "    \n",
    "    def grad_component(x, i):\n",
    "        \"\"\"Compute i-th component of gradient using finite differences\"\"\"\n",
    "        x_plus = x.at[i].add(eps)\n",
    "        x_minus = x.at[i].add(-eps)\n",
    "        return (func(x_plus) - func(x_minus)) / (2 * eps)\n",
    "    \n",
    "    # Compute analytical gradient\n",
    "    analytical_grad = grad(func)(x)\n",
    "    \n",
    "    # Compute numerical gradient\n",
    "    numerical_grad = jnp.array([grad_component(x, i) for i in range(len(x))])\n",
    "    \n",
    "    # Compare\n",
    "    error = jnp.linalg.norm(analytical_grad - numerical_grad)\n",
    "    relative_error = error / (jnp.linalg.norm(analytical_grad) + 1e-8)\n",
    "    \n",
    "    return analytical_grad, numerical_grad, error, relative_error\n",
    "\n",
    "# Test custom functions\n",
    "def test_function(x):\n",
    "    return jnp.sum(swish_custom(x))\n",
    "\n",
    "x_test = jnp.array([1.0, -0.5, 2.0])\n",
    "anal_grad, num_grad, abs_error, rel_error = finite_difference_check(test_function, x_test)\n",
    "\n",
    "print(f\"Analytical gradient: {anal_grad}\")\n",
    "print(f\"Numerical gradient: {num_grad}\")\n",
    "print(f\"Absolute error: {abs_error:.2e}\")\n",
    "print(f\"Relative error: {rel_error:.2e}\")\n",
    "\n",
    "if rel_error < 1e-5:\n",
    "    print(\"✓ Gradient check passed\")\n",
    "else:\n",
    "    print(\"✗ Gradient check failed\")\n",
    "```\n",
    "\n",
    "### VJP/JVP Consistency Check\n",
    "\n",
    "```python\n",
    "def check_vjp_jvp_consistency(func, x, v, eps=1e-10):\n",
    "    \"\"\"Check that VJP and JVP are transposes of each other\"\"\"\n",
    "    \n",
    "    # Test the identity: v^T (Jx) = (v^T J) x for random vectors\n",
    "    key = random.PRNGKey(0)\n",
    "    \n",
    "    # Forward mode: compute J*x (JVP)\n",
    "    y, jvp_result = jvp(func, (x,), (v,))\n",
    "    \n",
    "    # Reverse mode: compute v^T*J (VJP)  \n",
    "    y_check, vjp_fn = vjp(func, x)\n",
    "    vjp_result = vjp_fn(v)[0]\n",
    "    \n",
    "    # For scalar functions, they should be equal\n",
    "    # For vector functions, we need to check the transpose relationship\n",
    "    \n",
    "    print(f\"Function output shape: {y.shape}\")\n",
    "    print(f\"JVP result shape: {jvp_result.shape}\")\n",
    "    print(f\"VJP result shape: {vjp_result.shape}\")\n",
    "    \n",
    "    if y.shape == ():  # Scalar function\n",
    "        consistency_error = abs(jvp_result - vjp_result)\n",
    "        print(f\"JVP result: {jvp_result}\")\n",
    "        print(f\"VJP result: {vjp_result}\")\n",
    "        print(f\"Consistency error: {consistency_error}\")\n",
    "        return consistency_error < eps\n",
    "    else:\n",
    "        # For vector functions, check <v, Jx> = <v^T J, x>\n",
    "        # This requires inner product consistency\n",
    "        inner_product_1 = jnp.dot(v, jvp_result)  # v^T (J x)\n",
    "        inner_product_2 = jnp.dot(vjp_result, x)  # (v^T J) x\n",
    "        \n",
    "        consistency_error = abs(inner_product_1 - inner_product_2)\n",
    "        print(f\"Inner product 1 (v^T Jx): {inner_product_1}\")\n",
    "        print(f\"Inner product 2 ((v^T J)x): {inner_product_2}\")\n",
    "        print(f\"Consistency error: {consistency_error}\")\n",
    "        return consistency_error < eps\n",
    "\n",
    "# Test consistency for our custom functions\n",
    "x = jnp.array([1.0, 2.0])\n",
    "v = jnp.array([0.5, -0.3])\n",
    "\n",
    "print(\"Testing VJP/JVP consistency for custom square:\")\n",
    "is_consistent = check_vjp_jvp_consistency(lambda x: custom_square(x[0]), x, v[0])\n",
    "print(f\"Consistent: {is_consistent}\\n\")\n",
    "```\n",
    "\n",
    "## Performance Comparison\n",
    "\n",
    "### Benchmarking Custom vs Automatic Differentiation\n",
    "\n",
    "```python\n",
    "import time\n",
    "\n",
    "def benchmark_differentiation():\n",
    "    \"\"\"Compare performance of custom vs automatic differentiation\"\"\"\n",
    "    \n",
    "    # Setup test data\n",
    "    key = random.PRNGKey(42)\n",
    "    x = random.normal(key, (1000,))\n",
    "    \n",
    "    # Functions to test\n",
    "    funcs = {\n",
    "        'swish_auto': lambda x: x * jax.nn.sigmoid(x),\n",
    "        'swish_custom': swish_custom\n",
    "    }\n",
    "    \n",
    "    # Warmup and benchmark\n",
    "    for name, func in funcs.items():\n",
    "        # Warmup\n",
    "        for _ in range(10):\n",
    "            _ = grad(func)(x[0])\n",
    "        \n",
    "        # Benchmark gradient computation\n",
    "        start_time = time.time()\n",
    "        for _ in range(100):\n",
    "            grad_result = vmap(grad(func))(x)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"{name}: {(end_time - start_time)*1000:.2f} ms for 100 iterations\")\n",
    "\n",
    "benchmark_differentiation()\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we've explored advanced automatic differentiation concepts in JAX:\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "1. **VJP (Vector-Jacobian Products)**: Reverse-mode differentiation for efficient backpropagation\n",
    "2. **JVP (Jacobian-Vector Products)**: Forward-mode differentiation for directional derivatives\n",
    "3. **Custom VJP**: Implementing custom backward passes with `@custom_vjp`\n",
    "4. **Custom JVP**: Implementing custom forward passes with `@custom_jvp`\n",
    "5. **Implicit Differentiation**: Differentiating through iterative algorithms and implicit equations\n",
    "\n",
    "**Practical Applications:**\n",
    "- Custom activation functions with optimized gradients\n",
    "- Matrix operations with numerically stable derivatives\n",
    "- Linear system solvers with efficient backpropagation\n",
    "- Implicit function differentiation for constrained optimization\n",
    "\n",
    "**Best Practices:**\n",
    "- Use custom VJP/JVP for numerical stability and performance\n",
    "- Save minimal residual data in forward passes\n",
    "- Validate custom derivatives with finite difference checks\n",
    "- Check VJP/JVP consistency for correctness\n",
    "\n",
    "**When to Use Custom Rules:**\n",
    "- Numerical stability issues with automatic differentiation\n",
    "- Performance optimization for specific operations\n",
    "- Implementing algorithms with known analytical derivatives\n",
    "- Differentiating through implicit functions or iterative solvers\n",
    "\n",
    "**Next Steps:**\n",
    "- The next notebook will cover control flow and scan operations\n",
    "- We'll explore how JAX handles differentiation through loops and conditionals\n",
    "- Understanding these concepts enables implementation of RNNs and iterative algorithms\n",
    "\n",
    "Custom VJP and JVP are powerful tools for advanced JAX programming, enabling fine-grained control over differentiation behavior while maintaining the benefits of automatic differentiation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
