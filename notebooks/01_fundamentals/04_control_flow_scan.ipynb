{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "152024c8",
   "metadata": {},
   "source": [
    "# File: notebooks/01_fundamentals/04_control_flow_scan.ipynb\n",
    "\n",
    "## JAX Fundamentals: Control Flow and Scan\n",
    "\n",
    "Welcome to the fourth notebook in the JAX-NSL series! In this notebook, we'll explore JAX's approach to control flow operations and the powerful `scan` function. These are essential for implementing recurrent neural networks, iterative algorithms, and any computation that involves loops or conditional logic while maintaining compatibility with JAX's transformations like `jit`, `grad`, and `vmap`.\n",
    "\n",
    "JAX requires special handling of control flow to enable automatic differentiation and compilation. We'll cover `lax.cond`, `lax.while_loop`, `lax.fori_loop`, and most importantly, `lax.scan` for efficient sequential computations.\n",
    "\n",
    "## Setting Up the Environment\n",
    "\n",
    "```python\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, lax\n",
    "from jax import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, Tuple, Any\n",
    "import functools\n",
    "\n",
    "# Enable 64-bit precision\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "```\n",
    "\n",
    "## Basic Control Flow Operations\n",
    "\n",
    "### Conditional Operations with lax.cond\n",
    "\n",
    "```python\n",
    "# Simple conditional logic\n",
    "def simple_conditional(x, threshold=0.0):\n",
    "    \"\"\"Return x^2 if x > threshold, else -x^2\"\"\"\n",
    "    return lax.cond(x > threshold,\n",
    "                    lambda x: x**2,      # true branch\n",
    "                    lambda x: -x**2,     # false branch  \n",
    "                    x)                   # operand\n",
    "\n",
    "# Test conditional\n",
    "x_values = jnp.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "\n",
    "print(\"Simple conditional results:\")\n",
    "for x in x_values:\n",
    "    result = simple_conditional(x)\n",
    "    print(f\"f({x:4.1f}) = {result:6.2f}\")\n",
    "\n",
    "# Gradient computation works through conditionals\n",
    "grad_conditional = grad(simple_conditional)\n",
    "print(\"\\nGradients:\")\n",
    "for x in x_values:\n",
    "    if x != 0.0:  # Avoid exactly zero where gradient is discontinuous\n",
    "        grad_result = grad_conditional(x)\n",
    "        print(f\"f'({x:4.1f}) = {grad_result:6.2f}\")\n",
    "```\n",
    "\n",
    "### Multiple Conditions with lax.switch\n",
    "\n",
    "```python\n",
    "# Multiple branch conditional\n",
    "def piecewise_function(x):\n",
    "    \"\"\"Piecewise function with three branches based on value\"\"\"\n",
    "    \n",
    "    # Define branch functions\n",
    "    def branch_0(x):  # x < -1\n",
    "        return x**3\n",
    "    \n",
    "    def branch_1(x):  # -1 <= x <= 1  \n",
    "        return x\n",
    "    \n",
    "    def branch_2(x):  # x > 1\n",
    "        return x**2\n",
    "    \n",
    "    # Determine which branch based on x value\n",
    "    index = lax.cond(x < -1.0,\n",
    "                     lambda _: 0,\n",
    "                     lambda _: lax.cond(x <= 1.0,\n",
    "                                        lambda _: 1,\n",
    "                                        lambda _: 2),\n",
    "                     None)\n",
    "    \n",
    "    branches = [branch_0, branch_1, branch_2]\n",
    "    return lax.switch(index, branches, x)\n",
    "\n",
    "# Test piecewise function\n",
    "x_test = jnp.linspace(-2, 2, 9)\n",
    "print(\"Piecewise function results:\")\n",
    "for x in x_test:\n",
    "    result = piecewise_function(x)\n",
    "    print(f\"f({x:4.1f}) = {result:6.2f}\")\n",
    "```\n",
    "\n",
    "## Loop Operations\n",
    "\n",
    "### Fixed Number of Iterations with lax.fori_loop\n",
    "\n",
    "```python\n",
    "# Simple iteration example: compute x^n using repeated multiplication\n",
    "def power_via_loop(base, exponent):\n",
    "    \"\"\"Compute base^exponent using fori_loop\"\"\"\n",
    "    \n",
    "    def body_fun(i, val):\n",
    "        return val * base\n",
    "    \n",
    "    # Start with 1, multiply by base 'exponent' times\n",
    "    return lax.fori_loop(0, exponent, body_fun, 1.0)\n",
    "\n",
    "# Test power computation\n",
    "base, exp = 2.0, 5\n",
    "result = power_via_loop(base, exp)\n",
    "print(f\"{base}^{exp} = {result} (expected: {base**exp})\")\n",
    "\n",
    "# More complex example: numerical integration using trapezoidal rule\n",
    "def trapezoidal_integration(func, a, b, n_intervals):\n",
    "    \"\"\"Numerical integration using trapezoidal rule with fori_loop\"\"\"\n",
    "    \n",
    "    h = (b - a) / n_intervals\n",
    "    \n",
    "    def body_fun(i, sum_val):\n",
    "        x = a + i * h\n",
    "        # Add f(x) * h, but first and last points get weight 0.5\n",
    "        weight = lax.cond((i == 0) | (i == n_intervals),\n",
    "                         lambda _: 0.5,\n",
    "                         lambda _: 1.0,\n",
    "                         None)\n",
    "        return sum_val + weight * func(x) * h\n",
    "    \n",
    "    return lax.fori_loop(0, n_intervals + 1, body_fun, 0.0)\n",
    "\n",
    "# Test integration\n",
    "def test_function(x):\n",
    "    return x**2\n",
    "\n",
    "integral_result = trapezoidal_integration(test_function, 0.0, 1.0, 1000)\n",
    "analytical_result = 1.0/3.0  # Integral of x^2 from 0 to 1\n",
    "print(f\"Numerical integral: {integral_result:.6f}\")\n",
    "print(f\"Analytical result: {analytical_result:.6f}\")\n",
    "print(f\"Error: {abs(integral_result - analytical_result):.2e}\")\n",
    "```\n",
    "\n",
    "### Dynamic Loops with lax.while_loop\n",
    "\n",
    "```python\n",
    "# Newton-Raphson method using while_loop\n",
    "def newton_raphson(func, dfunc, x0, tolerance=1e-8, max_iters=50):\n",
    "    \"\"\"Solve f(x) = 0 using Newton-Raphson method\"\"\"\n",
    "    \n",
    "    def cond_fun(state):\n",
    "        x, error, iteration = state\n",
    "        return (error > tolerance) & (iteration < max_iters)\n",
    "    \n",
    "    def body_fun(state):\n",
    "        x, _, iteration = state\n",
    "        fx = func(x)\n",
    "        dfx = dfunc(x)\n",
    "        \n",
    "        # Newton step\n",
    "        x_new = x - fx / dfx\n",
    "        error = jnp.abs(fx)\n",
    "        \n",
    "        return x_new, error, iteration + 1\n",
    "    \n",
    "    # Initial state: (x, error, iteration)\n",
    "    initial_state = (x0, jnp.inf, 0)\n",
    "    final_x, final_error, final_iter = lax.while_loop(cond_fun, body_fun, initial_state)\n",
    "    \n",
    "    return final_x, final_error, final_iter\n",
    "\n",
    "# Example: Find square root of 2 by solving x^2 - 2 = 0\n",
    "def f(x):\n",
    "    return x**2 - 2.0\n",
    "\n",
    "def df(x):\n",
    "    return 2 * x\n",
    "\n",
    "sqrt_2, error, iterations = newton_raphson(f, df, x0=1.0)\n",
    "print(f\"Square root of 2: {sqrt_2:.10f}\")\n",
    "print(f\"True value: {jnp.sqrt(2.0):.10f}\")\n",
    "print(f\"Error: {error:.2e}\")\n",
    "print(f\"Iterations: {iterations}\")\n",
    "```\n",
    "\n",
    "## The Scan Operation\n",
    "\n",
    "### Basic Scan Usage\n",
    "\n",
    "```python\n",
    "# Simple cumulative sum using scan\n",
    "def cumulative_sum_scan(arr):\n",
    "    \"\"\"Compute cumulative sum using lax.scan\"\"\"\n",
    "    \n",
    "    def scan_fun(carry, x):\n",
    "        # carry: running sum\n",
    "        # x: current element\n",
    "        new_carry = carry + x\n",
    "        output = new_carry  # output this intermediate result\n",
    "        return new_carry, output\n",
    "    \n",
    "    # Initial carry value\n",
    "    init_carry = 0.0\n",
    "    final_carry, outputs = lax.scan(scan_fun, init_carry, arr)\n",
    "    \n",
    "    return final_carry, outputs\n",
    "\n",
    "# Test cumulative sum\n",
    "arr = jnp.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "final_sum, cumsum_result = cumulative_sum_scan(arr)\n",
    "\n",
    "print(f\"Array: {arr}\")\n",
    "print(f\"Cumulative sum: {cumsum_result}\")\n",
    "print(f\"Final sum: {final_sum}\")\n",
    "print(f\"NumPy cumsum: {jnp.cumsum(arr)}\")\n",
    "```\n",
    "\n",
    "### Scan with State: Running Statistics\n",
    "\n",
    "```python\n",
    "# Compute running mean and variance using scan\n",
    "def running_statistics(data):\n",
    "    \"\"\"Compute running mean and variance using Welford's online algorithm\"\"\"\n",
    "    \n",
    "    def welford_update(state, x):\n",
    "        count, mean, M2 = state\n",
    "        \n",
    "        count = count + 1\n",
    "        delta = x - mean\n",
    "        mean = mean + delta / count\n",
    "        delta2 = x - mean\n",
    "        M2 = M2 + delta * delta2\n",
    "        \n",
    "        # Variance calculation\n",
    "        variance = lax.cond(count < 2,\n",
    "                           lambda _: 0.0,\n",
    "                           lambda _: M2 / (count - 1),\n",
    "                           None)\n",
    "        \n",
    "        new_state = (count, mean, M2)\n",
    "        output = (mean, variance)  # Output running mean and variance\n",
    "        \n",
    "        return new_state, output\n",
    "    \n",
    "    # Initial state: (count, mean, M2)\n",
    "    init_state = (0.0, 0.0, 0.0)\n",
    "    final_state, outputs = lax.scan(welford_update, init_state, data)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# Test with random data\n",
    "key = random.PRNGKey(42)\n",
    "data = random.normal(key, (100,))\n",
    "\n",
    "running_stats = running_statistics(data)\n",
    "running_means, running_vars = running_stats\n",
    "\n",
    "print(f\"Final running mean: {running_means[-1]:.4f}\")\n",
    "print(f\"True mean: {jnp.mean(data):.4f}\")\n",
    "print(f\"Final running variance: {running_vars[-1]:.4f}\")\n",
    "print(f\"True variance: {jnp.var(data, ddof=1):.4f}\")\n",
    "```\n",
    "\n",
    "### Scan for Recurrent Neural Networks\n",
    "\n",
    "```python\n",
    "# Simple RNN cell implementation using scan\n",
    "def simple_rnn_cell(params, hidden_state, input_x):\n",
    "    \"\"\"Simple RNN cell: h_t = tanh(W_hh @ h_{t-1} + W_xh @ x_t + b)\"\"\"\n",
    "    W_hh, W_xh, b = params\n",
    "    \n",
    "    new_hidden = jnp.tanh(W_hh @ hidden_state + W_xh @ input_x + b)\n",
    "    return new_hidden, new_hidden  # return (new_carry, output)\n",
    "\n",
    "def rnn_forward(params, inputs, initial_hidden):\n",
    "    \"\"\"Forward pass through RNN using scan\"\"\"\n",
    "    \n",
    "    def rnn_step(hidden_state, input_x):\n",
    "        return simple_rnn_cell(params, hidden_state, input_x)\n",
    "    \n",
    "    final_hidden, all_hidden = lax.scan(rnn_step, initial_hidden, inputs)\n",
    "    return all_hidden, final_hidden\n",
    "\n",
    "# Initialize RNN parameters\n",
    "key = random.PRNGKey(123)\n",
    "hidden_size = 4\n",
    "input_size = 3\n",
    "seq_length = 10\n",
    "\n",
    "# Random parameters\n",
    "W_hh = random.normal(key, (hidden_size, hidden_size)) * 0.1\n",
    "W_xh = random.normal(random.split(key)[1], (hidden_size, input_size)) * 0.1\n",
    "b = jnp.zeros(hidden_size)\n",
    "params = (W_hh, W_xh, b)\n",
    "\n",
    "# Random input sequence\n",
    "inputs = random.normal(random.split(key, 3)[2], (seq_length, input_size))\n",
    "initial_hidden = jnp.zeros(hidden_size)\n",
    "\n",
    "# Forward pass\n",
    "hidden_states, final_hidden = rnn_forward(params, inputs, initial_hidden)\n",
    "\n",
    "print(f\"Input sequence shape: {inputs.shape}\")\n",
    "print(f\"Hidden states shape: {hidden_states.shape}\")\n",
    "print(f\"Final hidden state: {final_hidden}\")\n",
    "\n",
    "# Test gradient computation through RNN\n",
    "def rnn_loss(params, inputs, initial_hidden, targets):\n",
    "    hidden_states, _ = rnn_forward(params, inputs, initial_hidden)\n",
    "    # Simple loss: sum of squared hidden states\n",
    "    return jnp.sum(hidden_states**2)\n",
    "\n",
    "# Compute gradients\n",
    "grad_rnn = grad(rnn_loss)\n",
    "targets = jnp.zeros_like(hidden_states)  # Dummy targets\n",
    "grads = grad_rnn(params, inputs, initial_hidden, targets)\n",
    "\n",
    "print(f\"Gradient shapes: W_hh={grads[0].shape}, W_xh={grads[1].shape}, b={grads[2].shape}\")\n",
    "```\n",
    "\n",
    "## Advanced Scan Patterns\n",
    "\n",
    "### Scan with Multiple Sequences\n",
    "\n",
    "```python\n",
    "# Process multiple input sequences simultaneously\n",
    "def multi_sequence_scan(sequences):\n",
    "    \"\"\"Process multiple sequences with shared state\"\"\"\n",
    "    \n",
    "    def scan_fun(state, inputs):\n",
    "        x1, x2, x3 = inputs  # Three input sequences\n",
    "        \n",
    "        # Update state based on all inputs\n",
    "        new_state = state + x1 * 0.5 + x2 * 0.3 + x3 * 0.2\n",
    "        \n",
    "        # Compute outputs\n",
    "        output1 = new_state * 2\n",
    "        output2 = jnp.sin(new_state)\n",
    "        \n",
    "        return new_state, (output1, output2)\n",
    "    \n",
    "    seq1, seq2, seq3 = sequences\n",
    "    inputs = (seq1, seq2, seq3)\n",
    "    \n",
    "    init_state = 0.0\n",
    "    final_state, (outputs1, outputs2) = lax.scan(scan_fun, init_state, inputs)\n",
    "    \n",
    "    return final_state, outputs1, outputs2\n",
    "\n",
    "# Test with multiple sequences\n",
    "key = random.PRNGKey(0)\n",
    "n_steps = 20\n",
    "\n",
    "seq1 = random.normal(key, (n_steps,))\n",
    "seq2 = random.normal(random.split(key)[1], (n_steps,))\n",
    "seq3 = random.normal(random.split(key, 3)[2], (n_steps,))\n",
    "\n",
    "final_state, out1, out2 = multi_sequence_scan((seq1, seq2, seq3))\n",
    "print(f\"Final state: {final_state:.4f}\")\n",
    "print(f\"Output 1 shape: {out1.shape}\")\n",
    "print(f\"Output 2 shape: {out2.shape}\")\n",
    "```\n",
    "\n",
    "### Reverse Mode Scan\n",
    "\n",
    "```python\n",
    "# Scan in reverse order\n",
    "def reverse_cumsum(arr):\n",
    "    \"\"\"Cumulative sum from right to left\"\"\"\n",
    "    \n",
    "    def scan_fun(carry, x):\n",
    "        new_carry = carry + x\n",
    "        return new_carry, new_carry\n",
    "    \n",
    "    # Reverse the array, scan, then reverse the results\n",
    "    reversed_arr = jnp.flip(arr)\n",
    "    _, reverse_outputs = lax.scan(scan_fun, 0.0, reversed_arr)\n",
    "    \n",
    "    return jnp.flip(reverse_outputs)\n",
    "\n",
    "# Test reverse cumulative sum\n",
    "arr = jnp.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "reverse_result = reverse_cumsum(arr)\n",
    "\n",
    "print(f\"Original array: {arr}\")\n",
    "print(f\"Reverse cumsum: {reverse_result}\")\n",
    "print(f\"Expected: {jnp.flip(jnp.cumsum(jnp.flip(arr)))}\")\n",
    "```\n",
    "\n",
    "## Performance Optimizations\n",
    "\n",
    "### Scan vs Python Loops\n",
    "\n",
    "```python\n",
    "import time\n",
    "\n",
    "def python_loop_cumsum(arr):\n",
    "    \"\"\"Cumulative sum using Python loop (not JIT-able)\"\"\"\n",
    "    result = []\n",
    "    cumsum = 0.0\n",
    "    for x in arr:\n",
    "        cumsum += x\n",
    "        result.append(cumsum)\n",
    "    return jnp.array(result)\n",
    "\n",
    "def scan_cumsum(arr):\n",
    "    \"\"\"Cumulative sum using scan (JIT-able)\"\"\"\n",
    "    def scan_fun(carry, x):\n",
    "        carry = carry + x\n",
    "        return carry, carry\n",
    "    \n",
    "    _, outputs = lax.scan(scan_fun, 0.0, arr)\n",
    "    return outputs\n",
    "\n",
    "# JIT compile the scan version\n",
    "scan_cumsum_jit = jit(scan_cumsum)\n",
    "\n",
    "# Generate test data\n",
    "key = random.PRNGKey(42)\n",
    "large_array = random.normal(key, (10000,))\n",
    "\n",
    "# Warmup JIT compilation\n",
    "_ = scan_cumsum_jit(large_array[:10])\n",
    "\n",
    "# Benchmark\n",
    "n_trials = 100\n",
    "\n",
    "# Python loop version (can't JIT this)\n",
    "start = time.time()\n",
    "for _ in range(n_trials):\n",
    "    result_loop = python_loop_cumsum(large_array[:1000])  # Smaller for fairness\n",
    "end = time.time()\n",
    "time_loop = end - start\n",
    "\n",
    "# Scan version with JIT\n",
    "start = time.time()\n",
    "for _ in range(n_trials):\n",
    "    result_scan = scan_cumsum_jit(large_array)\n",
    "end = time.time()\n",
    "time_scan = end - start\n",
    "\n",
    "print(f\"Python loop time: {time_loop:.4f}s\")\n",
    "print(f\"Scan + JIT time: {time_scan:.4f}s\")\n",
    "print(f\"Speedup: {time_loop / time_scan:.1f}x\")\n",
    "```\n",
    "\n",
    "### Memory Efficient Scan\n",
    "\n",
    "```python\n",
    "# Scan with minimal memory usage\n",
    "def memory_efficient_scan(init_state, inputs, chunk_size=1000):\n",
    "    \"\"\"Process large sequences in chunks to save memory\"\"\"\n",
    "    \n",
    "    def process_chunk(state, chunk):\n",
    "        def scan_fun(carry, x):\n",
    "            # Simple processing function\n",
    "            carry = carry * 0.99 + x * 0.01  # Exponential moving average\n",
    "            return carry, carry\n",
    "        \n",
    "        final_carry, outputs = lax.scan(scan_fun, state, chunk)\n",
    "        return final_carry, outputs\n",
    "    \n",
    "    # Split inputs into chunks\n",
    "    n_total = len(inputs)\n",
    "    n_chunks = (n_total + chunk_size - 1) // chunk_size\n",
    "    \n",
    "    state = init_state\n",
    "    all_outputs = []\n",
    "    \n",
    "    for i in range(n_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = min((i + 1) * chunk_size, n_total)\n",
    "        chunk = inputs[start_idx:end_idx]\n",
    "        \n",
    "        state, chunk_outputs = process_chunk(state, chunk)\n",
    "        all_outputs.append(chunk_outputs)\n",
    "    \n",
    "    return state, jnp.concatenate(all_outputs)\n",
    "\n",
    "# Test with large sequence\n",
    "key = random.PRNGKey(0)\n",
    "large_sequence = random.normal(key, (50000,))\n",
    "\n",
    "final_state, outputs = memory_efficient_scan(0.0, large_sequence, chunk_size=5000)\n",
    "print(f\"Processed sequence length: {len(large_sequence)}\")\n",
    "print(f\"Final state: {final_state:.6f}\")\n",
    "print(f\"Output shape: {outputs.shape}\")\n",
    "```\n",
    "\n",
    "## Differentiating Through Scan\n",
    "\n",
    "### Gradient Flow Through Time\n",
    "\n",
    "```python\n",
    "# Example: Gradient flow through recurrent computation\n",
    "def recurrent_computation(params, sequence):\n",
    "    \"\"\"Simple recurrent computation with parameters\"\"\"\n",
    "    \n",
    "    def step_fun(state, input_val):\n",
    "        # state evolves based on parameters and input\n",
    "        new_state = params[0] * state + params[1] * input_val + params[2]\n",
    "        output = new_state**2  # Some nonlinear output\n",
    "        return new_state, output\n",
    "    \n",
    "    init_state = 0.0\n",
    "    final_state, outputs = lax.scan(step_fun, init_state, sequence)\n",
    "    \n",
    "    # Loss is sum of outputs\n",
    "    return jnp.sum(outputs)\n",
    "\n",
    "# Test gradient computation\n",
    "params = jnp.array([0.9, 0.1, 0.05])  # [state_coeff, input_coeff, bias]\n",
    "sequence = jnp.array([1.0, 0.5, -0.5, 1.5, -1.0])\n",
    "\n",
    "# Compute loss and gradients\n",
    "loss = recurrent_computation(params, sequence)\n",
    "grads = grad(recurrent_computation)(params, sequence)\n",
    "\n",
    "print(f\"Loss: {loss:.4f}\")\n",
    "print(f\"Gradients: {grads}\")\n",
    "\n",
    "# Test stability of gradients\n",
    "def gradient_norm_vs_sequence_length():\n",
    "    \"\"\"Check how gradient norms scale with sequence length\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(42)\n",
    "    lengths = [10, 50, 100, 500, 1000]\n",
    "    \n",
    "    for length in lengths:\n",
    "        seq = random.normal(key, (length,))\n",
    "        grads = grad(recurrent_computation)(params, seq)\n",
    "        grad_norm = jnp.linalg.norm(grads)\n",
    "        print(f\"Length {length:4d}: Gradient norm = {grad_norm:.4f}\")\n",
    "\n",
    "gradient_norm_vs_sequence_length()\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we've explored JAX's control flow and scan operations:\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "1. **Conditional Logic**: Using `lax.cond` and `lax.switch` for differentiable conditionals\n",
    "2. **Loops**: `lax.fori_loop` for fixed iterations, `lax.while_loop` for dynamic loops\n",
    "3. **Scan Operation**: `lax.scan` for efficient sequential processing with state\n",
    "4. **RNN Implementation**: Using scan for recurrent neural networks\n",
    "5. **Performance**: JIT compilation and memory efficiency considerations\n",
    "\n",
    "**Practical Applications:**\n",
    "- Recurrent neural networks and sequence models  \n",
    "- Iterative algorithms (Newton-Raphson, gradient descent)\n",
    "- Online statistics computation\n",
    "- Numerical integration and differential equations\n",
    "- Time series processing\n",
    "\n",
    "**Best Practices:**\n",
    "- Use scan instead of Python loops for JIT compilation\n",
    "- Minimize state size in scan operations\n",
    "- Consider chunking for very long sequences\n",
    "- Test gradient flow through recurrent computations\n",
    "\n",
    "**Performance Benefits:**\n",
    "- JIT compilation of control flow operations\n",
    "- Automatic differentiation through loops and conditionals\n",
    "- Vectorization and parallelization support\n",
    "- Memory-efficient processing of long sequences\n",
    "\n",
    "**Next Steps:**\n",
    "- The next notebook will cover linear algebra operations\n",
    "- We'll explore matrix operations, decompositions, and solvers\n",
    "- Understanding scan enables efficient implementation of iterative linear algebra algorithms\n",
    "\n",
    "Control flow and scan are fundamental for implementing sophisticated algorithms that maintain JAX's functional programming paradigm while enabling automatic differentiation and compilation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
