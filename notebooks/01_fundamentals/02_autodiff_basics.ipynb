{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "817c6a7b",
   "metadata": {},
   "source": [
    "# File: notebooks/01_fundamentals/02_autodiff_basics.ipynb\n",
    "\n",
    "## JAX Fundamentals: Automatic Differentiation Basics\n",
    "\n",
    "Welcome to the second notebook in the JAX-NSL series! Automatic differentiation (autodiff) is one of JAX's most powerful features, enabling efficient gradient computation for machine learning and scientific computing. In this notebook, we'll explore JAX's autodiff capabilities from basic gradients to more advanced concepts like Jacobians and Hessians.\n",
    "\n",
    "Automatic differentiation in JAX is implemented through program transformation - JAX can take a Python function and automatically generate its derivative function. This is fundamental for optimization algorithms, neural network training, and scientific computing applications that require gradients.\n",
    "\n",
    "## Setting Up the Environment\n",
    "\n",
    "```python\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jacfwd, jacrev, hessian, vmap\n",
    "from jax import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, Tuple\n",
    "import functools\n",
    "\n",
    "# Enable 64-bit precision for numerical accuracy\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "```\n",
    "\n",
    "## Basic Gradient Computation\n",
    "\n",
    "### Single-Variable Functions\n",
    "\n",
    "```python\n",
    "# Define simple functions for gradient computation\n",
    "def f1(x):\n",
    "    \"\"\"f(x) = x^2\"\"\"\n",
    "    return x**2\n",
    "\n",
    "def f2(x):\n",
    "    \"\"\"f(x) = sin(x) + cos(x)\"\"\"\n",
    "    return jnp.sin(x) + jnp.cos(x)\n",
    "\n",
    "def f3(x):\n",
    "    \"\"\"f(x) = exp(x) * log(x + 1)\"\"\"\n",
    "    return jnp.exp(x) * jnp.log(x + 1)\n",
    "\n",
    "# Compute gradients using jax.grad\n",
    "grad_f1 = grad(f1)  # Should be 2x\n",
    "grad_f2 = grad(f2)  # Should be cos(x) - sin(x)\n",
    "grad_f3 = grad(f3)  # More complex derivative\n",
    "\n",
    "# Test at specific points\n",
    "x_vals = jnp.array([1.0, 2.0, 3.0])\n",
    "\n",
    "for x in x_vals:\n",
    "    print(f\"x = {x}\")\n",
    "    print(f\"  f1(x) = {f1(x):.4f}, f1'(x) = {grad_f1(x):.4f}\")\n",
    "    print(f\"  f2(x) = {f2(x):.4f}, f2'(x) = {grad_f2(x):.4f}\")\n",
    "    print(f\"  f3(x) = {f3(x):.4f}, f3'(x) = {grad_f3(x):.4f}\")\n",
    "    print()\n",
    "```\n",
    "\n",
    "### Multi-Variable Functions\n",
    "\n",
    "```python\n",
    "# Multi-variable function examples\n",
    "def g1(x):\n",
    "    \"\"\"f(x, y) = x^2 + y^2 (input x is a vector [x, y])\"\"\"\n",
    "    return jnp.sum(x**2)\n",
    "\n",
    "def g2(x):\n",
    "    \"\"\"f(x, y) = x*y + sin(x) * cos(y)\"\"\"\n",
    "    return x[0] * x[1] + jnp.sin(x[0]) * jnp.cos(x[1])\n",
    "\n",
    "def g3(x):\n",
    "    \"\"\"Rosenbrock function: f(x, y) = (1-x)^2 + 100*(y-x^2)^2\"\"\"\n",
    "    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n",
    "\n",
    "# Compute gradients\n",
    "grad_g1 = grad(g1)\n",
    "grad_g2 = grad(g2)\n",
    "grad_g3 = grad(g3)\n",
    "\n",
    "# Test points\n",
    "test_points = [jnp.array([1.0, 1.0]), \n",
    "               jnp.array([0.5, 2.0]), \n",
    "               jnp.array([2.0, 1.5])]\n",
    "\n",
    "for i, point in enumerate(test_points):\n",
    "    print(f\"Point {i+1}: {point}\")\n",
    "    print(f\"  g1 gradient: {grad_g1(point)}\")\n",
    "    print(f\"  g2 gradient: {grad_g2(point)}\")\n",
    "    print(f\"  g3 gradient: {grad_g3(point)}\")\n",
    "    print()\n",
    "```\n",
    "\n",
    "## Jacobians: Handling Vector-Valued Functions\n",
    "\n",
    "### Forward vs Reverse Mode\n",
    "\n",
    "```python\n",
    "# Vector-valued function example\n",
    "def vector_function(x):\n",
    "    \"\"\"Maps R^2 -> R^3\"\"\"\n",
    "    return jnp.array([\n",
    "        x[0]**2 + x[1],           # f1(x, y) = x^2 + y\n",
    "        x[0] * x[1],              # f2(x, y) = xy  \n",
    "        jnp.sin(x[0]) + jnp.cos(x[1])  # f3(x, y) = sin(x) + cos(y)\n",
    "    ])\n",
    "\n",
    "# Forward-mode Jacobian (efficient for tall Jacobians)\n",
    "jac_forward = jacfwd(vector_function)\n",
    "\n",
    "# Reverse-mode Jacobian (efficient for wide Jacobians)  \n",
    "jac_reverse = jacrev(vector_function)\n",
    "\n",
    "# Test both methods\n",
    "x = jnp.array([1.0, 2.0])\n",
    "jac_fwd_result = jac_forward(x)\n",
    "jac_rev_result = jac_reverse(x)\n",
    "\n",
    "print(f\"Input point: {x}\")\n",
    "print(f\"Function output: {vector_function(x)}\")\n",
    "print(f\"Forward-mode Jacobian:\\n{jac_fwd_result}\")\n",
    "print(f\"Reverse-mode Jacobian:\\n{jac_rev_result}\")\n",
    "print(f\"Results match: {jnp.allclose(jac_fwd_result, jac_rev_result)}\")\n",
    "```\n",
    "\n",
    "### When to Use Forward vs Reverse Mode\n",
    "\n",
    "```python\n",
    "# Efficiency demonstration\n",
    "def tall_jacobian_function(x):\n",
    "    \"\"\"R^2 -> R^10 (more outputs than inputs)\"\"\"\n",
    "    return jnp.array([x[0]**i + x[1]**(i+1) for i in range(10)])\n",
    "\n",
    "def wide_jacobian_function(x):\n",
    "    \"\"\"R^10 -> R^2 (more inputs than outputs)\"\"\"\n",
    "    return jnp.array([jnp.sum(x**2), jnp.prod(x[:5])])\n",
    "\n",
    "# For tall Jacobians, forward mode is more efficient\n",
    "x_small = jnp.array([1.0, 2.0])\n",
    "jac_tall = jacfwd(tall_jacobian_function)(x_small)\n",
    "print(f\"Tall Jacobian shape: {jac_tall.shape}\")\n",
    "\n",
    "# For wide Jacobians, reverse mode is more efficient  \n",
    "x_large = jnp.ones(10)\n",
    "jac_wide = jacrev(wide_jacobian_function)(x_large)\n",
    "print(f\"Wide Jacobian shape: {jac_wide.shape}\")\n",
    "```\n",
    "\n",
    "## Hessians: Second-Order Derivatives\n",
    "\n",
    "### Computing Hessian Matrices\n",
    "\n",
    "```python\n",
    "# Functions for Hessian computation\n",
    "def quadratic_form(x):\n",
    "    \"\"\"f(x) = x^T A x + b^T x + c\"\"\"\n",
    "    A = jnp.array([[2, 1], [1, 3]])  # Positive definite matrix\n",
    "    b = jnp.array([1, -1])\n",
    "    c = 5\n",
    "    return jnp.dot(x, jnp.dot(A, x)) + jnp.dot(b, x) + c\n",
    "\n",
    "def rosenbrock_2d(x):\n",
    "    \"\"\"Standard Rosenbrock function\"\"\"\n",
    "    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n",
    "\n",
    "# Compute Hessians\n",
    "hess_quad = hessian(quadratic_form)\n",
    "hess_rosen = hessian(rosenbrock_2d)\n",
    "\n",
    "# Evaluate at test points\n",
    "x1 = jnp.array([1.0, 1.0])\n",
    "x2 = jnp.array([0.0, 0.0])\n",
    "\n",
    "print(\"Quadratic Form Hessians:\")\n",
    "print(f\"At {x1}: \\n{hess_quad(x1)}\")\n",
    "print(f\"At {x2}: \\n{hess_quad(x2)}\")\n",
    "print(\"Note: Hessian is constant for quadratic functions\")\n",
    "\n",
    "print(\"\\nRosenbrock Hessians:\")\n",
    "print(f\"At {x1}: \\n{hess_rosen(x1)}\")\n",
    "print(f\"At {x2}: \\n{hess_rosen(x2)}\")\n",
    "```\n",
    "\n",
    "### Analyzing Function Curvature\n",
    "\n",
    "```python\n",
    "def analyze_critical_point(func, point):\n",
    "    \"\"\"Analyze the nature of a critical point using the Hessian\"\"\"\n",
    "    grad_func = grad(func)\n",
    "    hess_func = hessian(func)\n",
    "    \n",
    "    gradient = grad_func(point)\n",
    "    hessian_matrix = hess_func(point)\n",
    "    \n",
    "    # Check if it's actually a critical point\n",
    "    grad_norm = jnp.linalg.norm(gradient)\n",
    "    \n",
    "    # Compute eigenvalues for classification\n",
    "    eigenvals = jnp.linalg.eigvals(hessian_matrix)\n",
    "    \n",
    "    print(f\"Point: {point}\")\n",
    "    print(f\"Gradient norm: {grad_norm:.6f}\")\n",
    "    print(f\"Hessian eigenvalues: {eigenvals}\")\n",
    "    \n",
    "    if grad_norm < 1e-6:  # Close to critical point\n",
    "        if jnp.all(eigenvals > 0):\n",
    "            print(\"Classification: Local minimum\")\n",
    "        elif jnp.all(eigenvals < 0):\n",
    "            print(\"Classification: Local maximum\")  \n",
    "        else:\n",
    "            print(\"Classification: Saddle point\")\n",
    "    else:\n",
    "        print(\"Not a critical point\")\n",
    "    \n",
    "    return gradient, hessian_matrix\n",
    "\n",
    "# Test with Rosenbrock function\n",
    "print(\"Analyzing Rosenbrock function:\")\n",
    "# The global minimum is at (1, 1)\n",
    "analyze_critical_point(rosenbrock_2d, jnp.array([1.0, 1.0]))\n",
    "print()\n",
    "\n",
    "# Test at saddle point of another function\n",
    "def saddle_function(x):\n",
    "    return x[0]**2 - x[1]**2\n",
    "\n",
    "print(\"Analyzing saddle function at origin:\")\n",
    "analyze_critical_point(saddle_function, jnp.array([0.0, 0.0]))\n",
    "```\n",
    "\n",
    "## Gradient-Based Optimization\n",
    "\n",
    "### Simple Gradient Descent\n",
    "\n",
    "```python\n",
    "def gradient_descent(func, x0, learning_rate=0.01, num_steps=100, tolerance=1e-6):\n",
    "    \"\"\"Simple gradient descent implementation\"\"\"\n",
    "    grad_func = grad(func)\n",
    "    x = x0\n",
    "    history = [x]\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        gradient = grad_func(x)\n",
    "        x = x - learning_rate * gradient\n",
    "        history.append(x)\n",
    "        \n",
    "        # Check convergence\n",
    "        if jnp.linalg.norm(gradient) < tolerance:\n",
    "            print(f\"Converged at step {step}\")\n",
    "            break\n",
    "    \n",
    "    return x, jnp.array(history)\n",
    "\n",
    "# Optimize the quadratic function\n",
    "x0 = jnp.array([5.0, 5.0])\n",
    "solution, path = gradient_descent(quadratic_form, x0, learning_rate=0.1)\n",
    "\n",
    "print(f\"Starting point: {x0}\")\n",
    "print(f\"Final solution: {solution}\")\n",
    "print(f\"Final function value: {quadratic_form(solution):.6f}\")\n",
    "print(f\"Number of steps: {len(path)-1}\")\n",
    "\n",
    "# Verify against analytical solution for quadratic form\n",
    "# For f(x) = x^T A x + b^T x + c, minimum is at x* = -0.5 * A^(-1) * b\n",
    "A = jnp.array([[2, 1], [1, 3]])\n",
    "b = jnp.array([1, -1])\n",
    "analytical_solution = -0.5 * jnp.linalg.solve(A, b)\n",
    "print(f\"Analytical solution: {analytical_solution}\")\n",
    "print(f\"Error: {jnp.linalg.norm(solution - analytical_solution):.6f}\")\n",
    "```\n",
    "\n",
    "### Newton's Method\n",
    "\n",
    "```python\n",
    "def newton_method(func, x0, num_steps=20, tolerance=1e-8):\n",
    "    \"\"\"Newton's method using automatic differentiation\"\"\"\n",
    "    grad_func = grad(func)\n",
    "    hess_func = hessian(func)\n",
    "    \n",
    "    x = x0\n",
    "    history = [x]\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        gradient = grad_func(x)\n",
    "        hessian_matrix = hess_func(x)\n",
    "        \n",
    "        # Newton step: x_{k+1} = x_k - H^{-1} * g\n",
    "        try:\n",
    "            newton_step = jnp.linalg.solve(hessian_matrix, gradient)\n",
    "            x = x - newton_step\n",
    "            history.append(x)\n",
    "            \n",
    "            if jnp.linalg.norm(gradient) < tolerance:\n",
    "                print(f\"Converged at step {step}\")\n",
    "                break\n",
    "        except:\n",
    "            print(f\"Hessian not invertible at step {step}\")\n",
    "            break\n",
    "    \n",
    "    return x, jnp.array(history)\n",
    "\n",
    "# Compare with gradient descent on Rosenbrock\n",
    "x0 = jnp.array([0.0, 0.0])\n",
    "\n",
    "# Newton's method\n",
    "newton_sol, newton_path = newton_method(rosenbrock_2d, x0)\n",
    "\n",
    "# Gradient descent with smaller learning rate\n",
    "gd_sol, gd_path = gradient_descent(rosenbrock_2d, x0, learning_rate=0.001, num_steps=1000)\n",
    "\n",
    "print(f\"Newton's method solution: {newton_sol}\")\n",
    "print(f\"Newton's method steps: {len(newton_path)-1}\")\n",
    "print(f\"Gradient descent solution: {gd_sol}\")\n",
    "print(f\"Gradient descent steps: {len(gd_path)-1}\")\n",
    "```\n",
    "\n",
    "## Advanced Autodiff Patterns\n",
    "\n",
    "### Higher-Order Derivatives\n",
    "\n",
    "```python\n",
    "# Computing higher-order derivatives\n",
    "def polynomial(x):\n",
    "    return x**4 - 2*x**3 + x**2 - 5*x + 3\n",
    "\n",
    "# First through fourth derivatives\n",
    "first_deriv = grad(polynomial)\n",
    "second_deriv = grad(first_deriv)\n",
    "third_deriv = grad(second_deriv)  \n",
    "fourth_deriv = grad(third_deriv)\n",
    "\n",
    "x = 2.0\n",
    "print(f\"At x = {x}:\")\n",
    "print(f\"f(x) = {polynomial(x)}\")\n",
    "print(f\"f'(x) = {first_deriv(x)}\")\n",
    "print(f\"f''(x) = {second_deriv(x)}\")\n",
    "print(f\"f'''(x) = {third_deriv(x)}\")\n",
    "print(f\"f''''(x) = {fourth_deriv(x)}\")\n",
    "\n",
    "# Analytical verification for x^4 - 2x^3 + x^2 - 5x + 3\n",
    "# f'(x) = 4x^3 - 6x^2 + 2x - 5\n",
    "print(f\"\\nAnalytical f'({x}) = {4*x**3 - 6*x**2 + 2*x - 5}\")\n",
    "```\n",
    "\n",
    "### Gradients of Vector Functions\n",
    "\n",
    "```python\n",
    "# Using vmap for efficient batch gradients\n",
    "def batch_function(x_batch):\n",
    "    \"\"\"Function that operates on a batch of inputs\"\"\"\n",
    "    return jnp.sum(x_batch**2, axis=1)  # Sum of squares for each input\n",
    "\n",
    "# Compute gradient for each input in batch\n",
    "batch_grad = vmap(grad(lambda x: jnp.sum(x**2)))\n",
    "\n",
    "# Test with batch data\n",
    "x_batch = jnp.array([[1.0, 2.0], \n",
    "                     [3.0, 4.0], \n",
    "                     [5.0, 6.0]])\n",
    "\n",
    "gradients = batch_grad(x_batch)\n",
    "print(f\"Batch inputs:\\n{x_batch}\")\n",
    "print(f\"Gradients:\\n{gradients}\")\n",
    "```\n",
    "\n",
    "### Gradient Through Control Flow\n",
    "\n",
    "```python\n",
    "# JAX can differentiate through control flow (with limitations)\n",
    "def conditional_function(x):\n",
    "    \"\"\"Function with conditional logic\"\"\"\n",
    "    return jnp.where(x > 0, x**2, -x**2)\n",
    "\n",
    "def iterative_function(x, n_iters=5):\n",
    "    \"\"\"Function with iterative computation\"\"\"\n",
    "    result = x\n",
    "    for i in range(n_iters):\n",
    "        result = result + 0.1 * jnp.sin(result)\n",
    "    return result\n",
    "\n",
    "# Compute gradients\n",
    "grad_conditional = grad(conditional_function)\n",
    "grad_iterative = grad(iterative_function)\n",
    "\n",
    "# Test\n",
    "x_vals = jnp.array([-1.0, 0.0, 1.0, 2.0])\n",
    "\n",
    "print(\"Conditional function gradients:\")\n",
    "for x in x_vals:\n",
    "    print(f\"x = {x:4.1f}: grad = {grad_conditional(x):6.3f}\")\n",
    "\n",
    "print(f\"\\nIterative function gradient at x=1.0: {grad_iterative(1.0):.6f}\")\n",
    "```\n",
    "\n",
    "## Practical Applications\n",
    "\n",
    "### Linear Regression with Automatic Differentiation\n",
    "\n",
    "```python\n",
    "# Generate synthetic data\n",
    "key = random.PRNGKey(42)\n",
    "n_samples, n_features = 100, 3\n",
    "\n",
    "X = random.normal(key, (n_samples, n_features))\n",
    "true_weights = jnp.array([2.0, -1.5, 3.0])\n",
    "y = X @ true_weights + 0.1 * random.normal(random.split(key)[1], (n_samples,))\n",
    "\n",
    "def mse_loss(weights, X, y):\n",
    "    \"\"\"Mean squared error loss\"\"\"\n",
    "    predictions = X @ weights\n",
    "    return jnp.mean((predictions - y)**2)\n",
    "\n",
    "def train_linear_regression(X, y, learning_rate=0.01, num_steps=100):\n",
    "    \"\"\"Train linear regression using gradient descent\"\"\"\n",
    "    # Initialize weights\n",
    "    weights = jnp.zeros(X.shape[1])\n",
    "    \n",
    "    # Loss and gradient functions\n",
    "    loss_fn = lambda w: mse_loss(w, X, y)\n",
    "    grad_fn = grad(loss_fn)\n",
    "    \n",
    "    losses = []\n",
    "    for step in range(num_steps):\n",
    "        loss = loss_fn(weights)\n",
    "        gradient = grad_fn(weights)\n",
    "        weights = weights - learning_rate * gradient\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if step % 20 == 0:\n",
    "            print(f\"Step {step}: Loss = {loss:.6f}\")\n",
    "    \n",
    "    return weights, jnp.array(losses)\n",
    "\n",
    "# Train the model\n",
    "learned_weights, loss_history = train_linear_regression(X, y)\n",
    "\n",
    "print(f\"\\nTrue weights: {true_weights}\")\n",
    "print(f\"Learned weights: {learned_weights}\")\n",
    "print(f\"Weight error: {jnp.linalg.norm(learned_weights - true_weights):.6f}\")\n",
    "```\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "```python\n",
    "# Generate classification data\n",
    "def generate_classification_data(key, n_samples=200):\n",
    "    X = random.normal(key, (n_samples, 2))\n",
    "    # Create linearly separable classes with some noise\n",
    "    true_weights = jnp.array([1.5, -2.0])\n",
    "    bias = 0.5\n",
    "    logits = X @ true_weights + bias + 0.1 * random.normal(random.split(key)[1], (n_samples,))\n",
    "    y = (logits > 0).astype(jnp.float32)\n",
    "    return X, y\n",
    "\n",
    "key = random.PRNGKey(123)\n",
    "X_cls, y_cls = generate_classification_data(key)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + jnp.exp(-x))\n",
    "\n",
    "def logistic_loss(params, X, y):\n",
    "    \"\"\"Binary cross-entropy loss\"\"\"\n",
    "    weights, bias = params\n",
    "    logits = X @ weights + bias\n",
    "    probs = sigmoid(logits)\n",
    "    # Numerical stability\n",
    "    probs = jnp.clip(probs, 1e-15, 1 - 1e-15)\n",
    "    return -jnp.mean(y * jnp.log(probs) + (1 - y) * jnp.log(1 - probs))\n",
    "\n",
    "def train_logistic_regression(X, y, learning_rate=0.1, num_steps=200):\n",
    "    \"\"\"Train logistic regression\"\"\"\n",
    "    # Initialize parameters\n",
    "    weights = jnp.zeros(X.shape[1])\n",
    "    bias = 0.0\n",
    "    params = (weights, bias)\n",
    "    \n",
    "    grad_fn = grad(logistic_loss)\n",
    "    losses = []\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        loss = logistic_loss(params, X, y)\n",
    "        grads = grad_fn(params, X, y)\n",
    "        \n",
    "        # Update parameters\n",
    "        weights = weights - learning_rate * grads[0]\n",
    "        bias = bias - learning_rate * grads[1]\n",
    "        params = (weights, bias)\n",
    "        \n",
    "        losses.append(loss)\n",
    "        \n",
    "        if step % 50 == 0:\n",
    "            print(f\"Step {step}: Loss = {loss:.6f}\")\n",
    "    \n",
    "    return params, jnp.array(losses)\n",
    "\n",
    "# Train logistic regression\n",
    "final_params, cls_losses = train_logistic_regression(X_cls, y_cls)\n",
    "final_weights, final_bias = final_params\n",
    "\n",
    "print(f\"Final weights: {final_weights}\")\n",
    "print(f\"Final bias: {final_bias:.4f}\")\n",
    "\n",
    "# Compute accuracy\n",
    "logits = X_cls @ final_weights + final_bias\n",
    "predictions = (sigmoid(logits) > 0.5).astype(jnp.float32)\n",
    "accuracy = jnp.mean(predictions == y_cls)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "```\n",
    "\n",
    "## Performance Tips and Gotchas\n",
    "\n",
    "### Efficient Gradient Computation\n",
    "\n",
    "```python\n",
    "# Avoid recomputing gradients unnecessarily\n",
    "def inefficient_training_step(params, data):\n",
    "    \"\"\"Recomputes gradient function each time\"\"\"\n",
    "    loss_fn = lambda p: jnp.sum((p - data)**2)\n",
    "    grad_fn = grad(loss_fn)  # BAD: Creates new gradient function\n",
    "    return grad_fn(params)\n",
    "\n",
    "def efficient_training_step(grad_fn, params, data):\n",
    "    \"\"\"Reuses precomputed gradient function\"\"\"  \n",
    "    return grad_fn(params, data)\n",
    "\n",
    "# Define gradient function once\n",
    "def loss_with_data(params, data):\n",
    "    return jnp.sum((params - data)**2)\n",
    "\n",
    "efficient_grad_fn = grad(loss_with_data)\n",
    "\n",
    "# Example usage\n",
    "key = random.PRNGKey(0)\n",
    "params = random.normal(key, (1000,))\n",
    "data = random.normal(random.split(key)[1], (1000,))\n",
    "\n",
    "# Both give same results but efficient version is faster\n",
    "grad1 = inefficient_training_step(params, data)\n",
    "grad2 = efficient_grad_fn(params, data)\n",
    "print(f\"Gradients match: {jnp.allclose(grad1, grad2)}\")\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we've explored JAX's automatic differentiation capabilities:\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "1. **Basic Gradients**: Using `jax.grad` for scalar-valued functions\n",
    "2. **Jacobians**: Forward (`jacfwd`) vs reverse (`jacrev`) mode for vector-valued functions  \n",
    "3. **Hessians**: Second-order derivatives for optimization and analysis\n",
    "4. **Higher-order derivatives**: Composing `grad` multiple times\n",
    "5. **Batch gradients**: Using `vmap` for efficient batch processing\n",
    "\n",
    "**Practical Applications:**\n",
    "- Gradient descent optimization\n",
    "- Newton's method\n",
    "- Linear and logistic regression\n",
    "- Function analysis and critical point classification\n",
    "\n",
    "**Performance Best Practices:**\n",
    "- Choose appropriate differentiation mode (forward vs reverse)\n",
    "- Avoid recomputing gradient functions\n",
    "- Use vectorization with `vmap` when possible\n",
    "- Consider numerical stability in loss functions\n",
    "\n",
    "**Next Steps:**\n",
    "- The next notebook will cover custom VJP/JVP for advanced differentiation\n",
    "- We'll explore how to implement custom derivatives for special functions\n",
    "- Understanding the mathematical foundations will help with more complex autodiff scenarios\n",
    "\n",
    "Automatic differentiation is fundamental to modern machine learning and scientific computing, and JAX's implementation provides both ease of use and high performance for research and production applications."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
