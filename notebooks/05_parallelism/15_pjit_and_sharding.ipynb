{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ac11e1a",
   "metadata": {},
   "source": [
    "# Location: notebooks/05_parallelism/15_pjit_and_sharding.ipynb\n",
    "\n",
    "## Advanced Parallelism with pjit and Sharding\n",
    "\n",
    "This notebook covers JAX's `pjit` (parallel just-in-time) compilation and advanced sharding strategies for model parallelism, data parallelism, and mixed parallelism patterns.\n",
    "\n",
    "## Setting up Device Mesh\n",
    "\n",
    "```python\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.experimental import pjit, PartitionSpec as P\n",
    "from jax.experimental.maps import mesh\n",
    "import numpy as np\n",
    "\n",
    "# Create device mesh\n",
    "devices = jax.devices()\n",
    "n_devices = len(devices)\n",
    "print(f\"Available devices: {n_devices}\")\n",
    "\n",
    "# Create 1D mesh for simple sharding\n",
    "mesh_1d = mesh(devices, ('x',))\n",
    "print(f\"1D Mesh: {mesh_1d}\")\n",
    "\n",
    "# Create 2D mesh if we have enough devices\n",
    "if n_devices >= 4:\n",
    "    devices_2d = np.array(devices[:4]).reshape(2, 2)\n",
    "    mesh_2d = mesh(devices_2d, ('data', 'model'))\n",
    "    print(f\"2D Mesh shape: {devices_2d.shape}\")\n",
    "else:\n",
    "    # Fallback to 1D mesh\n",
    "    mesh_2d = mesh(devices, ('data',))\n",
    "    print(\"Using 1D mesh as fallback\")\n",
    "```\n",
    "\n",
    "## Basic pjit with Sharding Specifications\n",
    "\n",
    "```python\n",
    "def simple_computation(x, y):\n",
    "    return jnp.dot(x, y)\n",
    "\n",
    "# Define sharding specifications\n",
    "with mesh_1d:\n",
    "    # Shard first array along first dimension\n",
    "    # Replicate second array\n",
    "    # Shard output along first dimension\n",
    "    pjit_simple = pjit.pjit(\n",
    "        simple_computation,\n",
    "        in_axis_resources=(P('x'), P()),\n",
    "        out_axis_resources=P('x')\n",
    "    )\n",
    "    \n",
    "    # Create test data\n",
    "    x = jax.random.normal(jax.random.PRNGKey(42), (n_devices * 4, 64))\n",
    "    y = jax.random.normal(jax.random.PRNGKey(43), (64, 32))\n",
    "    \n",
    "    result = pjit_simple(x, y)\n",
    "    print(f\"Input x shape: {x.shape}\")\n",
    "    print(f\"Input y shape: {y.shape}\")\n",
    "    print(f\"Output shape: {result.shape}\")\n",
    "    print(f\"Output sharding: {result.sharding}\")\n",
    "```\n",
    "\n",
    "## Matrix Operations with Different Sharding Strategies\n",
    "\n",
    "```python\n",
    "def matrix_multiply_variants():\n",
    "    A = jax.random.normal(jax.random.PRNGKey(0), (128, 64))\n",
    "    B = jax.random.normal(jax.random.PRNGKey(1), (64, 96))\n",
    "    \n",
    "    with mesh_1d:\n",
    "        # Strategy 1: Shard A along rows\n",
    "        pjit_row_shard = pjit.pjit(\n",
    "            lambda a, b: jnp.dot(a, b),\n",
    "            in_axis_resources=(P('x'), P()),\n",
    "            out_axis_resources=P('x')\n",
    "        )\n",
    "        \n",
    "        result1 = pjit_row_shard(A, B)\n",
    "        print(f\"Row-sharded result shape: {result1.shape}\")\n",
    "        \n",
    "        # Strategy 2: Shard B along columns\n",
    "        pjit_col_shard = pjit.pjit(\n",
    "            lambda a, b: jnp.dot(a, b),\n",
    "            in_axis_resources=(P(), P(None, 'x')),\n",
    "            out_axis_resources=P(None, 'x')\n",
    "        )\n",
    "        \n",
    "        result2 = pjit_col_shard(A, B)\n",
    "        print(f\"Column-sharded result shape: {result2.shape}\")\n",
    "        \n",
    "        # Strategy 3: Replicated computation\n",
    "        pjit_replicated = pjit.pjit(\n",
    "            lambda a, b: jnp.dot(a, b),\n",
    "            in_axis_resources=(P(), P()),\n",
    "            out_axis_resources=P()\n",
    "        )\n",
    "        \n",
    "        result3 = pjit_replicated(A, B)\n",
    "        print(f\"Replicated result shape: {result3.shape}\")\n",
    "        \n",
    "        # Verify all results are equivalent\n",
    "        print(f\"Results match: {jnp.allclose(result1, result2) and jnp.allclose(result2, result3)}\")\n",
    "\n",
    "matrix_multiply_variants()\n",
    "```\n",
    "\n",
    "## Neural Network Layer with Model Parallelism\n",
    "\n",
    "```python\n",
    "def create_parallel_linear_layer(input_size, output_size, mesh_context):\n",
    "    \"\"\"Create a parallelized linear layer\"\"\"\n",
    "    \n",
    "    def init_params(key):\n",
    "        w_key, b_key = jax.random.split(key)\n",
    "        w = jax.random.normal(w_key, (input_size, output_size)) * 0.1\n",
    "        b = jnp.zeros(output_size)\n",
    "        return {'w': w, 'b': b}\n",
    "    \n",
    "    def linear_forward(params, x):\n",
    "        return jnp.dot(x, params['w']) + params['b']\n",
    "    \n",
    "    with mesh_context:\n",
    "        # Model parallel: split weights along output dimension\n",
    "        pjit_linear = pjit.pjit(\n",
    "            linear_forward,\n",
    "            in_axis_resources=({\n",
    "                'w': P(None, 'x'),  # Shard weights along output dim\n",
    "                'b': P('x')         # Shard bias along output dim  \n",
    "            }, P()),                # Replicate input\n",
    "            out_axis_resources=P(None, 'x')  # Output sharded along feature dim\n",
    "        )\n",
    "    \n",
    "    return init_params, pjit_linear\n",
    "\n",
    "# Create parallel linear layer\n",
    "init_fn, parallel_linear = create_parallel_linear_layer(256, 512, mesh_1d)\n",
    "\n",
    "# Initialize parameters\n",
    "params = init_fn(jax.random.PRNGKey(10))\n",
    "print(f\"Weight shape: {params['w'].shape}\")\n",
    "print(f\"Bias shape: {params['b'].shape}\")\n",
    "\n",
    "# Test forward pass\n",
    "x = jax.random.normal(jax.random.PRNGKey(11), (32, 256))\n",
    "output = parallel_linear(params, x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "```\n",
    "\n",
    "## Multi-Layer Network with Mixed Parallelism\n",
    "\n",
    "```python\n",
    "def create_mlp_with_sharding(layer_sizes, mesh_context):\n",
    "    \"\"\"Create MLP with different sharding strategies per layer\"\"\"\n",
    "    \n",
    "    def init_mlp(key):\n",
    "        keys = jax.random.split(key, len(layer_sizes) - 1)\n",
    "        params = []\n",
    "        for i, (in_size, out_size) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "            w_key, b_key = jax.random.split(keys[i])\n",
    "            params.append({\n",
    "                'w': jax.random.normal(w_key, (in_size, out_size)) * 0.1,\n",
    "                'b': jnp.zeros(out_size)\n",
    "            })\n",
    "        return params\n",
    "    \n",
    "    def mlp_forward(params, x):\n",
    "        # First layer: model parallel\n",
    "        x = jnp.dot(x, params[0]['w']) + params[0]['b']\n",
    "        x = jax.nn.relu(x)\n",
    "        \n",
    "        # Middle layers: data parallel\n",
    "        for i in range(1, len(params) - 1):\n",
    "            x = jnp.dot(x, params[i]['w']) + params[i]['b']\n",
    "            x = jax.nn.relu(x)\n",
    "        \n",
    "        # Final layer: model parallel\n",
    "        if len(params) > 1:\n",
    "            x = jnp.dot(x, params[-1]['w']) + params[-1]['b']\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    with mesh_context:\n",
    "        # Mixed sharding strategy\n",
    "        if len(layer_sizes) == 3:  # Simple case: input -> hidden -> output\n",
    "            param_sharding = [\n",
    "                {'w': P(None, 'x'), 'b': P('x')},      # First layer: model parallel\n",
    "                {'w': P('x', None), 'b': P()}           # Last layer: input parallel\n",
    "            ]\n",
    "        else:\n",
    "            # More layers\n",
    "            param_sharding = [\n",
    "                {'w': P(None, 'x'), 'b': P('x')}       # Model parallel for all\n",
    "                for _ in range(len(layer_sizes) - 1)\n",
    "            ]\n",
    "        \n",
    "        pjit_mlp = pjit.pjit(\n",
    "            mlp_forward,\n",
    "            in_axis_resources=(param_sharding, P()),    # Params sharded, input replicated\n",
    "            out_axis_resources=P(None, 'x')             # Output model parallel\n",
    "        )\n",
    "    \n",
    "    return init_mlp, pjit_mlp\n",
    "\n",
    "# Create MLP with mixed parallelism\n",
    "layer_sizes = [784, 512, 256, 10]\n",
    "init_mlp, parallel_mlp = create_mlp_with_sharding(layer_sizes, mesh_1d)\n",
    "\n",
    "# Initialize and test\n",
    "mlp_params = init_mlp(jax.random.PRNGKey(20))\n",
    "test_input = jax.random.normal(jax.random.PRNGKey(21), (16, 784))\n",
    "\n",
    "mlp_output = parallel_mlp(mlp_params, test_input)\n",
    "print(f\"MLP input shape: {test_input.shape}\")\n",
    "print(f\"MLP output shape: {mlp_output.shape}\")\n",
    "```\n",
    "\n",
    "## Advanced Sharding: 2D Parallelism\n",
    "\n",
    "```python\n",
    "# Only run if we have enough devices for 2D mesh\n",
    "if n_devices >= 4:\n",
    "    with mesh_2d:\n",
    "        def advanced_matmul(A, B):\n",
    "            return jnp.dot(A, B)\n",
    "        \n",
    "        # 2D sharding: shard A along both dimensions, B along first dimension\n",
    "        pjit_2d = pjit.pjit(\n",
    "            advanced_matmul,\n",
    "            in_axis_resources=(P('data', 'model'), P('model', None)),\n",
    "            out_axis_resources=P('data', None)\n",
    "        )\n",
    "        \n",
    "        # Create larger matrices for 2D sharding\n",
    "        A_large = jax.random.normal(jax.random.PRNGKey(30), (256, 256))\n",
    "        B_large = jax.random.normal(jax.random.PRNGKey(31), (256, 128))\n",
    "        \n",
    "        result_2d = pjit_2d(A_large, B_large)\n",
    "        print(f\"2D sharded result shape: {result_2d.shape}\")\n",
    "        \n",
    "        # Compare with sequential computation\n",
    "        sequential_result = jnp.dot(A_large, B_large)\n",
    "        print(f\"Results match: {jnp.allclose(result_2d, sequential_result, rtol=1e-5)}\")\n",
    "```\n",
    "\n",
    "## Training Step with Gradient Sharding\n",
    "\n",
    "```python\n",
    "def create_sharded_training_step(mesh_context):\n",
    "    \"\"\"Create training step with gradient accumulation and sharding\"\"\"\n",
    "    \n",
    "    def loss_fn(params, x, y):\n",
    "        pred = jnp.dot(x, params['w']) + params['b']\n",
    "        return jnp.mean((pred - y) ** 2)\n",
    "    \n",
    "    def training_step(params, x, y, lr):\n",
    "        loss, grads = jax.value_and_grad(loss_fn)(params, x, y)\n",
    "        \n",
    "        # Update parameters\n",
    "        new_params = {\n",
    "            'w': params['w'] - lr * grads['w'],\n",
    "            'b': params['b'] - lr * grads['b']\n",
    "        }\n",
    "        \n",
    "        return new_params, loss\n",
    "    \n",
    "    with mesh_context:\n",
    "        # Shard parameters and gradients\n",
    "        param_spec = {'w': P('x', None), 'b': P()}\n",
    "        \n",
    "        pjit_train_step = pjit.pjit(\n",
    "            training_step,\n",
    "            in_axis_resources=(param_spec, P(), P(), P()),  # params, x, y, lr\n",
    "            out_axis_resources=(param_spec, P())            # new_params, loss\n",
    "        )\n",
    "    \n",
    "    return pjit_train_step\n",
    "\n",
    "# Create sharded training step\n",
    "sharded_train_step = create_sharded_training_step(mesh_1d)\n",
    "\n",
    "# Initialize training data\n",
    "train_params = {\n",
    "    'w': jax.random.normal(jax.random.PRNGKey(40), (256, 64)),\n",
    "    'b': jnp.zeros(64)\n",
    "}\n",
    "train_x = jax.random.normal(jax.random.PRNGKey(41), (32, 256))\n",
    "train_y = jax.random.normal(jax.random.PRNGKey(42), (32, 64))\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Perform training step\n",
    "new_params, loss = sharded_train_step(train_params, train_x, train_y, learning_rate)\n",
    "print(f\"Training loss: {loss}\")\n",
    "print(f\"Parameter update successful: {not jnp.allclose(train_params['w'], new_params['w'])}\")\n",
    "```\n",
    "\n",
    "## Dynamic Sharding and Resharding\n",
    "\n",
    "```python\n",
    "def demonstrate_resharding():\n",
    "    \"\"\"Show how to change sharding patterns dynamically\"\"\"\n",
    "    \n",
    "    with mesh_1d:\n",
    "        # Create data with one sharding pattern\n",
    "        x = jax.random.normal(jax.random.PRNGKey(50), (n_devices * 8, 64))\n",
    "        \n",
    "        # Function that expects row-sharded input\n",
    "        pjit_row_op = pjit.pjit(\n",
    "            lambda x: jnp.sum(x, axis=1),\n",
    "            in_axis_resources=P('x'),\n",
    "            out_axis_resources=P('x')\n",
    "        )\n",
    "        \n",
    "        # Function that expects column-sharded input (requires resharding)\n",
    "        pjit_col_op = pjit.pjit(\n",
    "            lambda x: jnp.sum(x, axis=0),\n",
    "            in_axis_resources=P(None, 'x'),\n",
    "            out_axis_resources=P('x')\n",
    "        )\n",
    "        \n",
    "        # Apply row operation (no resharding needed)\n",
    "        result1 = pjit_row_op(x)\n",
    "        print(f\"Row operation result shape: {result1.shape}\")\n",
    "        \n",
    "        # Apply column operation (will trigger automatic resharding)\n",
    "        result2 = pjit_col_op(x)\n",
    "        print(f\"Column operation result shape: {result2.shape}\")\n",
    "        \n",
    "        # Explicit resharding using pjit\n",
    "        reshard_fn = pjit.pjit(\n",
    "            lambda x: x,  # Identity function\n",
    "            in_axis_resources=P('x'),      # Input: row-sharded\n",
    "            out_axis_resources=P(None, 'x')  # Output: column-sharded\n",
    "        )\n",
    "        \n",
    "        x_resharded = reshard_fn(x)\n",
    "        print(f\"Original sharding: row-wise\")\n",
    "        print(f\"After resharding: column-wise\")\n",
    "\n",
    "demonstrate_resharding()\n",
    "```\n",
    "\n",
    "## Performance Analysis and Optimization\n",
    "\n",
    "```python\n",
    "import time\n",
    "\n",
    "def benchmark_sharding_strategies():\n",
    "    \"\"\"Compare performance of different sharding strategies\"\"\"\n",
    "    \n",
    "    # Test data\n",
    "    A = jax.random.normal(jax.random.PRNGKey(60), (512, 512))\n",
    "    B = jax.random.normal(jax.random.PRNGKey(61), (512, 512))\n",
    "    \n",
    "    strategies = {\n",
    "        'replicated': (P(), P(), P()),\n",
    "        'row_sharded': (P('x'), P(), P('x')),\n",
    "        'col_sharded': (P(), P(None, 'x'), P(None, 'x')),\n",
    "    }\n",
    "    \n",
    "    def matmul(a, b):\n",
    "        return jnp.dot(a, b)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    with mesh_1d:\n",
    "        for name, (in1_spec, in2_spec, out_spec) in strategies.items():\n",
    "            pjit_fn = pjit.pjit(\n",
    "                matmul,\n",
    "                in_axis_resources=(in1_spec, in2_spec),\n",
    "                out_axis_resources=out_spec\n",
    "            )\n",
    "            \n",
    "            # Warmup\n",
    "            _ = pjit_fn(A, B)\n",
    "            \n",
    "            # Timing\n",
    "            n_trials = 5\n",
    "            start_time = time.time()\n",
    "            for _ in range(n_trials):\n",
    "                result = pjit_fn(A, B)\n",
    "                result.block_until_ready()  # Ensure completion\n",
    "            \n",
    "            avg_time = (time.time() - start_time) / n_trials\n",
    "            results[name] = avg_time\n",
    "            \n",
    "            print(f\"{name:15}: {avg_time:.4f}s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "performance_results = benchmark_sharding_strategies()\n",
    "\n",
    "# Find best strategy\n",
    "best_strategy = min(performance_results.items(), key=lambda x: x[1])\n",
    "print(f\"\\nBest strategy: {best_strategy[0]} ({best_strategy[1]:.4f}s)\")\n",
    "```\n",
    "\n",
    "## Memory-Efficient Large Model Patterns\n",
    "\n",
    "```python\n",
    "def create_memory_efficient_layer(input_size, output_size, mesh_context):\n",
    "    \"\"\"Create layer optimized for memory efficiency\"\"\"\n",
    "    \n",
    "    def efficient_linear(w, x):\n",
    "        # Use checkpoint to save memory during backprop\n",
    "        @jax.checkpoint\n",
    "        def chunked_computation(w_chunk, x):\n",
    "            return jnp.dot(x, w_chunk)\n",
    "        \n",
    "        return chunked_computation(w, x)\n",
    "    \n",
    "    with mesh_context:\n",
    "        # Shard weights to distribute memory load\n",
    "        pjit_efficient = pjit.pjit(\n",
    "            efficient_linear,\n",
    "            in_axis_resources=(P('x', None), P()),\n",
    "            out_axis_resources=P(None, 'x')\n",
    "        )\n",
    "    \n",
    "    return pjit_efficient\n",
    "\n",
    "# Create memory-efficient layer\n",
    "efficient_layer = create_memory_efficient_layer(1024, 2048, mesh_1d)\n",
    "\n",
    "# Test with large tensors\n",
    "large_w = jax.random.normal(jax.random.PRNGKey(70), (1024, 2048))\n",
    "large_x = jax.random.normal(jax.random.PRNGKey(71), (64, 1024))\n",
    "\n",
    "efficient_output = efficient_layer(large_w, large_x)\n",
    "print(f\"Efficient layer output shape: {efficient_output.shape}\")\n",
    "\n",
    "# Gradient computation test\n",
    "grad_fn = jax.grad(lambda w, x: jnp.sum(efficient_layer(w, x)))\n",
    "grads = grad_fn(large_w, large_x)\n",
    "print(f\"Gradients computed successfully: {grads.shape}\")\n",
    "```\n",
    "\n",
    "## Best Practices and Common Patterns\n",
    "\n",
    "```python\n",
    "# Pattern 1: Conditional sharding based on array size\n",
    "def adaptive_sharding(x, threshold=1000):\n",
    "    \"\"\"Use different sharding based on array size\"\"\"\n",
    "    if x.size > threshold:\n",
    "        # Large arrays: use sharding\n",
    "        with mesh_1d:\n",
    "            pjit_fn = pjit.pjit(\n",
    "                lambda arr: jnp.sum(arr ** 2),\n",
    "                in_axis_resources=P('x'),\n",
    "                out_axis_resources=P()\n",
    "            )\n",
    "    else:\n",
    "        # Small arrays: replicate\n",
    "        with mesh_1d:\n",
    "            pjit_fn = pjit.pjit(\n",
    "                lambda arr: jnp.sum(arr ** 2),\n",
    "                in_axis_resources=P(),\n",
    "                out_axis_resources=P()\n",
    "            )\n",
    "    \n",
    "    return pjit_fn(x)\n",
    "\n",
    "# Test adaptive sharding\n",
    "small_array = jax.random.normal(jax.random.PRNGKey(80), (100,))\n",
    "large_array = jax.random.normal(jax.random.PRNGKey(81), (10000,))\n",
    "\n",
    "result_small = adaptive_sharding(small_array)\n",
    "result_large = adaptive_sharding(large_array)\n",
    "print(f\"Small array result: {result_small}\")\n",
    "print(f\"Large array result: {result_large}\")\n",
    "\n",
    "# Pattern 2: Pipeline parallelism simulation\n",
    "def pipeline_stages(x):\n",
    "    \"\"\"Simulate pipeline parallelism with different sharding per stage\"\"\"\n",
    "    \n",
    "    with mesh_1d:\n",
    "        # Stage 1: Data parallel processing\n",
    "        stage1 = pjit.pjit(\n",
    "            lambda x: jax.nn.relu(x),\n",
    "            in_axis_resources=P('x'),\n",
    "            out_axis_resources=P('x')\n",
    "        )\n",
    "        \n",
    "        # Stage 2: Model parallel processing (reshape first)\n",
    "        stage2 = pjit.pjit(\n",
    "            lambda x: jnp.mean(x, axis=0),\n",
    "            in_axis_resources=P('x'),\n",
    "            out_axis_resources=P()\n",
    "        )\n",
    "        \n",
    "        # Execute pipeline\n",
    "        x = stage1(x)\n",
    "        x = stage2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test pipeline\n",
    "pipeline_input = jax.random.normal(jax.random.PRNGKey(90), (n_devices * 16, 64))\n",
    "pipeline_output = pipeline_stages(pipeline_input)\n",
    "print(f\"Pipeline output shape: {pipeline_output.shape}\")\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we explored advanced parallelism with `pjit` and sharding:\n",
    "\n",
    "**Key Concepts:**\n",
    "- `pjit` enables fine-grained control over data and computation placement\n",
    "- `PartitionSpec` defines how arrays are sharded across device dimensions\n",
    "- Device meshes organize devices for different parallelism strategies\n",
    "- Automatic resharding handles changes in sharding patterns\n",
    "\n",
    "**Sharding Strategies:**\n",
    "- **Data Parallelism**: Shard data, replicate parameters\n",
    "- **Model Parallelism**: Shard parameters, replicate data  \n",
    "- **Mixed Parallelism**: Combine data and model parallelism\n",
    "- **2D Parallelism**: Use multiple mesh dimensions for complex patterns\n",
    "\n",
    "**Best Practices:**\n",
    "- Choose sharding strategy based on memory constraints and computation patterns\n",
    "- Use explicit resharding for better control over data movement\n",
    "- Consider memory efficiency with gradient checkpointing\n",
    "- Profile different strategies to find optimal performance\n",
    "\n",
    "**Advanced Patterns:**\n",
    "- Adaptive sharding based on array sizes\n",
    "- Pipeline parallelism with different sharding per stage\n",
    "- Memory-efficient patterns for large models\n",
    "- Dynamic resharding for complex workflows\n",
    "\n",
    "`pjit` provides the most flexible and powerful parallelism capabilities in JAX, enabling efficient scaling from single devices to large clusters while maintaining automatic differentiation and compilation benefits."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
