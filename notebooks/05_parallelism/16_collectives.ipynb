{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83705c6e",
   "metadata": {},
   "source": [
    "# Location: notebooks/05_parallelism/16_collectives.ipynb\n",
    "\n",
    "## Collective Operations in JAX\n",
    "\n",
    "This notebook explores collective communication operations in JAX, including reductions, all-to-all communication, and synchronization primitives essential for distributed training and parallel algorithms.\n",
    "\n",
    "## Basic Collective Operations\n",
    "\n",
    "```python\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax, pmap\n",
    "import numpy as np\n",
    "\n",
    "# Setup devices\n",
    "n_devices = jax.device_count()\n",
    "print(f\"Available devices: {n_devices}\")\n",
    "\n",
    "# Create test data\n",
    "test_data = jax.random.normal(jax.random.PRNGKey(42), (n_devices, 100))\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "\n",
    "def demonstrate_basic_collectives(x):\n",
    "    \"\"\"Demonstrate basic collective operations\"\"\"\n",
    "    \n",
    "    # Local computation\n",
    "    local_sum = jnp.sum(x)\n",
    "    local_mean = jnp.mean(x)\n",
    "    local_max = jnp.max(x)\n",
    "    local_min = jnp.min(x)\n",
    "    \n",
    "    # Cross-device reductions\n",
    "    global_sum = lax.psum(local_sum, axis_name='devices')\n",
    "    global_mean = lax.pmean(local_mean, axis_name='devices')  # Average of local means\n",
    "    global_max = lax.pmax(local_max, axis_name='devices')\n",
    "    global_min = lax.pmin(local_min, axis_name='devices')\n",
    "    \n",
    "    return {\n",
    "        'local_sum': local_sum,\n",
    "        'global_sum': global_sum,\n",
    "        'local_mean': local_mean,\n",
    "        'global_mean': global_mean,\n",
    "        'global_max': global_max,\n",
    "        'global_min': global_min\n",
    "    }\n",
    "\n",
    "pmapped_collectives = pmap(demonstrate_basic_collectives, axis_name='devices')\n",
    "results = pmapped_collectives(test_data)\n",
    "\n",
    "print(\"\\nCollective Results:\")\n",
    "print(f\"Local sums: {results['local_sum']}\")\n",
    "print(f\"Global sum (each device): {results['global_sum'][0]}\")\n",
    "print(f\"Local means: {results['local_mean']}\")\n",
    "print(f\"Global mean: {results['global_mean'][0]}\")\n",
    "print(f\"Global max: {results['global_max'][0]}\")\n",
    "print(f\"Global min: {results['global_min'][0]}\")\n",
    "\n",
    "# Verify correctness\n",
    "expected_global_sum = jnp.sum(test_data)\n",
    "expected_global_max = jnp.max(test_data)\n",
    "expected_global_min = jnp.min(test_data)\n",
    "\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"Sum correct: {jnp.allclose(results['global_sum'][0], expected_global_sum)}\")\n",
    "print(f\"Max correct: {jnp.allclose(results['global_max'][0], expected_global_max)}\")\n",
    "print(f\"Min correct: {jnp.allclose(results['global_min'][0], expected_global_min)}\")\n",
    "```\n",
    "\n",
    "## Advanced Reduction Operations\n",
    "\n",
    "```python\n",
    "def advanced_reductions(x):\n",
    "    \"\"\"More complex reduction patterns\"\"\"\n",
    "    \n",
    "    # Statistics computation\n",
    "    local_count = x.size\n",
    "    local_sum = jnp.sum(x)\n",
    "    local_sum_squares = jnp.sum(x ** 2)\n",
    "    \n",
    "    # Global statistics\n",
    "    global_count = lax.psum(local_count, axis_name='devices')\n",
    "    global_sum = lax.psum(local_sum, axis_name='devices')\n",
    "    global_sum_squares = lax.psum(local_sum_squares, axis_name='devices')\n",
    "    \n",
    "    # Compute global mean and variance\n",
    "    global_mean = global_sum / global_count\n",
    "    global_variance = (global_sum_squares / global_count) - (global_mean ** 2)\n",
    "    global_std = jnp.sqrt(global_variance)\n",
    "    \n",
    "    # Normalize data using global statistics\n",
    "    normalized_x = (x - global_mean) / global_std\n",
    "    \n",
    "    return {\n",
    "        'global_mean': global_mean,\n",
    "        'global_std': global_std,\n",
    "        'normalized_data': normalized_x,\n",
    "        'local_stats': {\n",
    "            'count': local_count,\n",
    "            'sum': local_sum,\n",
    "            'sum_squares': local_sum_squares\n",
    "        }\n",
    "    }\n",
    "\n",
    "pmapped_advanced = pmap(advanced_reductions, axis_name='devices')\n",
    "advanced_results = pmapped_advanced(test_data)\n",
    "\n",
    "print(\"Advanced Reduction Results:\")\n",
    "print(f\"Global mean: {advanced_results['global_mean'][0]}\")\n",
    "print(f\"Global std: {advanced_results['global_std'][0]}\")\n",
    "print(f\"Normalized data shape: {advanced_results['normalized_data'].shape}\")\n",
    "\n",
    "# Verify normalized data has zero mean and unit variance\n",
    "normalized_flat = advanced_results['normalized_data'].reshape(-1)\n",
    "print(f\"Normalized mean ≈ 0: {jnp.abs(jnp.mean(normalized_flat)) < 1e-6}\")\n",
    "print(f\"Normalized std ≈ 1: {jnp.abs(jnp.std(normalized_flat) - 1) < 1e-6}\")\n",
    "```\n",
    "\n",
    "## All-to-All Communication\n",
    "\n",
    "```python\n",
    "def all_to_all_example(x):\n",
    "    \"\"\"Demonstrate all-to-all communication patterns\"\"\"\n",
    "    \n",
    "    # All-gather: collect data from all devices\n",
    "    all_gathered = lax.all_gather(x, axis_name='devices', axis=0)\n",
    "    \n",
    "    # Broadcast from device 0 to all devices\n",
    "    device_id = lax.axis_index(axis_name='devices')\n",
    "    is_root = device_id == 0\n",
    "    root_value = lax.cond(is_root, lambda: jnp.sum(x), lambda: 0.0)\n",
    "    broadcasted = lax.psum(root_value, axis_name='devices')  # Sum gives broadcast effect\n",
    "    \n",
    "    # All-reduce with custom operation\n",
    "    custom_all_reduce = lax.psum(x ** 2, axis_name='devices')\n",
    "    \n",
    "    # Reduce-scatter: reduce then distribute pieces\n",
    "    reduced = lax.psum(x, axis_name='devices')\n",
    "    scattered = lax.dynamic_slice(reduced, (device_id * (x.shape[0] // n_devices),), (x.shape[0] // n_devices,))\n",
    "    \n",
    "    return {\n",
    "        'original': x,\n",
    "        'all_gathered': all_gathered,\n",
    "        'broadcasted': broadcasted,\n",
    "        'custom_reduced': custom_all_reduce,\n",
    "        'scattered': scattered,\n",
    "        'device_id': device_id\n",
    "    }\n",
    "\n",
    "pmapped_all_to_all = pmap(all_to_all_example, axis_name='devices')\n",
    "all_to_all_results = pmapped_all_to_all(test_data)\n",
    "\n",
    "print(\"All-to-All Communication Results:\")\n",
    "print(f\"Original shape per device: {all_to_all_results['original'].shape}\")\n",
    "print(f\"All-gathered shape per device: {all_to_all_results['all_gathered'].shape}\")\n",
    "print(f\"Broadcasted value (should be same): {all_to_all_results['broadcasted'][:3]}\")\n",
    "print(f\"Device IDs: {all_to_all_results['device_id']}\")\n",
    "print(f\"Scattered shape per device: {all_to_all_results['scattered'].shape}\")\n",
    "```\n",
    "\n",
    "## Gradient Synchronization Patterns\n",
    "\n",
    "```python\n",
    "def create_synchronized_training_step():\n",
    "    \"\"\"Create training step with proper gradient synchronization\"\"\"\n",
    "    \n",
    "    def loss_fn(params, x, y):\n",
    "        # Simple linear model\n",
    "        pred = params['w'] @ x + params['b']\n",
    "        return jnp.mean((pred - y) ** 2)\n",
    "    \n",
    "    def sync_training_step(params, x_batch, y_batch, lr):\n",
    "        # Compute local gradients\n",
    "        local_loss, local_grads = jax.value_and_grad(loss_fn)(params, x_batch, y_batch)\n",
    "        \n",
    "        # Method 1: Average gradients across devices\n",
    "        sync_grads = jax.tree_map(lambda g: lax.pmean(g, axis_name='devices'), local_grads)\n",
    "        \n",
    "        # Method 2: Sum gradients and normalize by device count\n",
    "        # sum_grads = jax.tree_map(lambda g: lax.psum(g, axis_name='devices') / n_devices, local_grads)\n",
    "        \n",
    "        # Update parameters with synchronized gradients\n",
    "        new_params = jax.tree_map(lambda p, g: p - lr * g, params, sync_grads)\n",
    "        \n",
    "        # Synchronize loss for monitoring\n",
    "        avg_loss = lax.pmean(local_loss, axis_name='devices')\n",
    "        \n",
    "        return new_params, avg_loss, {\n",
    "            'local_loss': local_loss,\n",
    "            'local_grad_norm': jnp.sqrt(sum(jnp.sum(g**2) for g in jax.tree_leaves(local_grads))),\n",
    "            'sync_grad_norm': jnp.sqrt(sum(jnp.sum(g**2) for g in jax.tree_leaves(sync_grads)))\n",
    "        }\n",
    "    \n",
    "    return pmap(sync_training_step, axis_name='devices')\n",
    "\n",
    "# Create synchronized training step\n",
    "sync_train_step = create_synchronized_training_step()\n",
    "\n",
    "# Initialize model parameters\n",
    "params = {\n",
    "    'w': jax.random.normal(jax.random.PRNGKey(0), (10, 50)),\n",
    "    'b': jnp.zeros(10)\n",
    "}\n",
    "\n",
    "# Create training data (different on each device to simulate real scenario)\n",
    "keys = jax.random.split(jax.random.PRNGKey(1), n_devices)\n",
    "x_data = jnp.stack([jax.random.normal(k, (32, 50)) for k in keys])\n",
    "y_data = jnp.stack([jax.random.normal(jax.random.split(k)[0], (32, 10)) for k in keys])\n",
    "\n",
    "print(\"Synchronized Training:\")\n",
    "print(f\"Training data shapes: {x_data.shape}, {y_data.shape}\")\n",
    "\n",
    "# Perform training step\n",
    "new_params, avg_loss, metrics = sync_train_step(params, x_data, y_data, 0.01)\n",
    "\n",
    "print(f\"Average loss: {avg_loss[0]}\")\n",
    "print(f\"Local losses: {metrics['local_loss']}\")\n",
    "print(f\"Local gradient norms: {metrics['local_grad_norm']}\")\n",
    "print(f\"Synchronized gradient norms: {metrics['sync_grad_norm']}\")\n",
    "\n",
    "# Verify that synchronized parameters are identical across devices\n",
    "w_diff = jnp.max(jnp.abs(new_params['w'] - new_params['w'][0]))\n",
    "b_diff = jnp.max(jnp.abs(new_params['b'] - new_params['b'][0]))\n",
    "print(f\"Parameter synchronization - W diff: {w_diff}, B diff: {b_diff}\")\n",
    "```\n",
    "\n",
    "## Custom Collective Operations\n",
    "\n",
    "```python\n",
    "def custom_collectives(x):\n",
    "    \"\"\"Implement custom collective operations\"\"\"\n",
    "    \n",
    "    # Custom operation: weighted average based on device ID\n",
    "    device_id = lax.axis_index(axis_name='devices')\n",
    "    weight = (device_id + 1) / n_devices  # Weight increases with device ID\n",
    "    \n",
    "    weighted_value = x * weight\n",
    "    weighted_sum = lax.psum(weighted_value, axis_name='devices')\n",
    "    weight_sum = lax.psum(weight, axis_name='devices')\n",
    "    weighted_average = weighted_sum / weight_sum\n",
    "    \n",
    "    # Custom reduction: geometric mean\n",
    "    log_values = jnp.log(jnp.abs(x) + 1e-8)  # Avoid log(0)\n",
    "    sum_log_values = lax.psum(log_values, axis_name='devices')\n",
    "    geometric_mean = jnp.exp(sum_log_values / n_devices)\n",
    "    \n",
    "    # Custom all-reduce: median approximation via sorting\n",
    "    all_values = lax.all_gather(x, axis_name='devices', axis=0)\n",
    "    sorted_values = jnp.sort(all_values.reshape(-1))\n",
    "    approx_median = sorted_values[sorted_values.shape[0] // 2]\n",
    "    \n",
    "    # Consensus operation: agree on maximum value's location\n",
    "    local_max = jnp.max(x)\n",
    "    global_max = lax.pmax(local_max, axis_name='devices')\n",
    "    has_global_max = jnp.isclose(local_max, global_max)\n",
    "    max_device = lax.psum(device_id * has_global_max, axis_name='devices')\n",
    "    \n",
    "    return {\n",
    "        'device_id': device_id,\n",
    "        'weight': weight,\n",
    "        'weighted_average': weighted_average,\n",
    "        'geometric_mean': geometric_mean,\n",
    "        'approx_median': approx_median,\n",
    "        'global_max': global_max,\n",
    "        'max_device': max_device,\n",
    "        'has_max': has_global_max\n",
    "    }\n",
    "\n",
    "pmapped_custom = pmap(custom_collectives, axis_name='devices')\n",
    "custom_results = pmapped_custom(test_data)\n",
    "\n",
    "print(\"Custom Collective Operations:\")\n",
    "print(f\"Device weights: {custom_results['weight']}\")\n",
    "print(f\"Weighted average: {custom_results['weighted_average'][0]}\")\n",
    "print(f\"Geometric mean: {custom_results['geometric_mean'][0]}\")\n",
    "print(f\"Approximate median: {custom_results['approx_median'][0]}\")\n",
    "print(f\"Global maximum: {custom_results['global_max'][0]}\")\n",
    "print(f\"Device with max: {custom_results['max_device'][0]}\")\n",
    "print(f\"Devices with max: {custom_results['has_max']}\")\n",
    "```\n",
    "\n",
    "## Barrier and Synchronization\n",
    "\n",
    "```python\n",
    "def synchronization_example(x):\n",
    "    \"\"\"Demonstrate synchronization patterns\"\"\"\n",
    "    \n",
    "    device_id = lax.axis_index(axis_name='devices')\n",
    "    \n",
    "    # Phase 1: Local computation with varying time\n",
    "    # Simulate different computation times per device\n",
    "    computation_factor = device_id + 1\n",
    "    local_result = x\n",
    "    for _ in range(computation_factor):\n",
    "        local_result = jnp.sin(local_result) + jnp.cos(local_result)\n",
    "    \n",
    "    # Implicit barrier: all devices must reach this point\n",
    "    barrier_value = lax.psum(1, axis_name='devices')  # Count of devices that reached here\n",
    "    \n",
    "    # Phase 2: Synchronized computation\n",
    "    synchronized_input = lax.pmean(local_result, axis_name='devices')\n",
    "    \n",
    "    # Phase 3: Leader election for coordination\n",
    "    # Device 0 performs special computation\n",
    "    is_leader = device_id == 0\n",
    "    leader_computation = lax.cond(\n",
    "        is_leader,\n",
    "        lambda: jnp.sum(synchronized_input ** 2),\n",
    "        lambda: 0.0\n",
    "    )\n",
    "    \n",
    "    # Broadcast leader's result to all devices\n",
    "    shared_result = lax.psum(leader_computation, axis_name='devices')\n",
    "    \n",
    "    return {\n",
    "        'device_id': device_id,\n",
    "        'computation_factor': computation_factor,\n",
    "        'local_result_norm': jnp.linalg.norm(local_result),\n",
    "        'barrier_count': barrier_value,\n",
    "        'synchronized_norm': jnp.linalg.norm(synchronized_input),\n",
    "        'shared_result': shared_result,\n",
    "        'is_leader': is_leader\n",
    "    }\n",
    "\n",
    "pmapped_sync = pmap(synchronization_example, axis_name='devices')\n",
    "sync_results = pmapped_sync(test_data)\n",
    "\n",
    "print(\"Synchronization Example:\")\n",
    "print(f\"Computation factors: {sync_results['computation_factor']}\")\n",
    "print(f\"Local result norms: {sync_results['local_result_norm']}\")\n",
    "print(f\"Barrier count: {sync_results['barrier_count'][0]} (should equal {n_devices})\")\n",
    "print(f\"Synchronized norms: {sync_results['synchronized_norm']}\")\n",
    "print(f\"Shared result: {sync_results['shared_result'][0]}\")\n",
    "print(f\"Leaders: {sync_results['is_leader']}\")\n",
    "```\n",
    "\n",
    "## Hierarchical Communication Patterns\n",
    "\n",
    "```python\n",
    "def hierarchical_communication(x):\n",
    "    \"\"\"Demonstrate hierarchical reduction patterns\"\"\"\n",
    "    \n",
    "    device_id = lax.axis_index(axis_name='devices')\n",
    "    \n",
    "    # Group devices into pairs/hierarchies\n",
    "    group_size = 2\n",
    "    group_id = device_id // group_size\n",
    "    local_id_in_group = device_id % group_size\n",
    "    \n",
    "    # Step 1: Reduce within groups\n",
    "    # For simplicity, just sum within conceptual groups\n",
    "    group_sum = lax.psum(x, axis_name='devices')  # This sums across all, but we simulate groups\n",
    "    \n",
    "    # Step 2: Simulate hierarchy with conditional operations\n",
    "    is_group_leader = local_id_in_group == 0\n",
    "    \n",
    "    # Group leaders do inter-group communication\n",
    "    inter_group_value = lax.cond(\n",
    "        is_group_leader,\n",
    "        lambda: jnp.mean(x),\n",
    "        lambda: 0.0\n",
    "    )\n",
    "    \n",
    "    # Sum across group leaders (simulated)\n",
    "    global_leader_sum = lax.psum(inter_group_value, axis_name='devices')\n",
    "    \n",
    "    # Step 3: Broadcast back down hierarchy\n",
    "    final_result = global_leader_sum  # All devices get the same result\n",
    "    \n",
    "    return {\n",
    "        'device_id': device_id,\n",
    "        'group_id': group_id,\n",
    "        'local_id_in_group': local_id_in_group,\n",
    "        'is_group_leader': is_group_leader,\n",
    "        'inter_group_value': inter_group_value,\n",
    "        'final_result': final_result\n",
    "    }\n",
    "\n",
    "pmapped_hierarchical = pmap(hierarchical_communication, axis_name='devices')\n",
    "hier_results = pmapped_hierarchical(test_data)\n",
    "\n",
    "print(\"Hierarchical Communication:\")\n",
    "print(f\"Device IDs: {hier_results['device_id']}\")\n",
    "print(f\"Group IDs: {hier_results['group_id']}\")\n",
    "print(f\"Local IDs in group: {hier_results['local_id_in_group']}\")\n",
    "print(f\"Group leaders: {hier_results['is_group_leader']}\")\n",
    "print(f\"Inter-group values: {hier_results['inter_group_value']}\")\n",
    "print(f\"Final result: {hier_results['final_result'][0]}\")\n",
    "```\n",
    "\n",
    "## Communication-Efficient Training Patterns\n",
    "\n",
    "```python\n",
    "def create_communication_efficient_training():\n",
    "    \"\"\"Create training with reduced communication overhead\"\"\"\n",
    "    \n",
    "    def efficient_training_step(params, x_batch, y_batch, lr, step_count, sync_frequency=4):\n",
    "        # Compute local gradients\n",
    "        def loss_fn(p, x, y):\n",
    "            pred = p['w'] @ x + p['b']\n",
    "            return jnp.mean((pred - y) ** 2)\n",
    "        \n",
    "        loss, grads = jax.value_and_grad(loss_fn)(params, x_batch, y_batch)\n",
    "        \n",
    "        # Conditional synchronization based on step count\n",
    "        should_sync = (step_count % sync_frequency) == 0\n",
    "        \n",
    "        def sync_grads():\n",
    "            return jax.tree_map(lambda g: lax.pmean(g, axis_name='devices'), grads)\n",
    "        \n",
    "        def keep_local_grads():\n",
    "            return grads\n",
    "        \n",
    "        final_grads = lax.cond(should_sync, sync_grads, keep_local_grads)\n",
    "        \n",
    "        # Update parameters\n",
    "        new_params = jax.tree_map(lambda p, g: p - lr * g, params, final_grads)\n",
    "        \n",
    "        # Optionally sync parameters periodically for stability\n",
    "        def sync_params():\n",
    "            return jax.tree_map(lambda p: lax.pmean(p, axis_name='devices'), new_params)\n",
    "        \n",
    "        def keep_params():\n",
    "            return new_params\n",
    "        \n",
    "        final_params = lax.cond(should_sync, sync_params, keep_params)\n",
    "        \n",
    "        return final_params, loss, {\n",
    "            'should_sync': should_sync,\n",
    "            'grad_norm': jnp.sqrt(sum(jnp.sum(g**2) for g in jax.tree_leaves(grads))),\n",
    "            'step_count': step_count\n",
    "        }\n",
    "    \n",
    "    return pmap(efficient_training_step, axis_name='devices')\n",
    "\n",
    "# Create efficient training step\n",
    "efficient_train_step = create_communication_efficient_training()\n",
    "\n",
    "# Simulate multiple training steps\n",
    "params = {\n",
    "    'w': jax.random.normal(jax.random.PRNGKey(100), (5, 20)),\n",
    "    'b': jnp.zeros(5)\n",
    "}\n",
    "\n",
    "print(\"Communication-Efficient Training:\")\n",
    "for step in range(8):\n",
    "    # Generate new data for each step\n",
    "    keys = jax.random.split(jax.random.PRNGKey(step + 200), n_devices)\n",
    "    x_batch = jnp.stack([jax.random.normal(k, (16, 20)) for k in keys])\n",
    "    y_batch = jnp.stack([jax.random.normal(jax.random.split(k)[0], (16, 5)) for k in keys])\n",
    "    \n",
    "    params, loss, metrics = efficient_train_step(\n",
    "        params, x_batch, y_batch, 0.01, step, sync_frequency=3\n",
    "    )\n",
    "    \n",
    "    print(f\"Step {step}: Loss={loss[0]:.4f}, Sync={metrics['should_sync'][0]}, \"\n",
    "          f\"GradNorm={metrics['grad_norm'][0]:.4f}\")\n",
    "\n",
    "# Check final parameter synchronization\n",
    "param_sync_diff = jnp.max(jnp.abs(params['w'] - params['w'][0]))\n",
    "print(f\"Final parameter difference across devices: {param_sync_diff:.6f}\")\n",
    "```\n",
    "\n",
    "## Performance Analysis of Collectives\n",
    "\n",
    "```python\n",
    "import time\n",
    "\n",
    "def benchmark_collective_operations():\n",
    "    \"\"\"Benchmark different collective operations\"\"\"\n",
    "    \n",
    "    # Create test data of different sizes\n",
    "    sizes = [100, 1000, 10000]\n",
    "    operations = {}\n",
    "    \n",
    "    def create_benchmark_fn(op_name, collective_fn):\n",
    "        def benchmark_op(x):\n",
    "            return collective_fn(x)\n",
    "        return pmap(benchmark_op, axis_name='devices')\n",
    "    \n",
    "    # Define collective operations to benchmark\n",
    "    collective_ops = {\n",
    "        'psum': lambda x: lax.psum(x, axis_name='devices'),\n",
    "        'pmean': lambda x: lax.pmean(x, axis_name='devices'), \n",
    "        'pmax': lambda x: lax.pmax(x, axis_name='devices'),\n",
    "        'all_gather': lambda x: lax.all_gather(x, axis_name='devices', axis=0),\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for size in sizes:\n",
    "        print(f\"\\nBenchmarking with array size: {size}\")\n",
    "        test_array = jax.random.normal(jax.random.PRNGKey(300), (n_devices, size))\n",
    "        \n",
    "        size_results = {}\n",
    "        for op_name, op_fn in collective_ops.items():\n",
    "            benchmark_fn = create_benchmark_fn(op_name, op_fn)\n",
    "            \n",
    "            # Warmup\n",
    "            _ = benchmark_fn(test_array)\n",
    "            \n",
    "            # Timing\n",
    "            n_trials = 10\n",
    "            start_time = time.time()\n",
    "            for _ in range(n_trials):\n",
    "                result = benchmark_fn(test_array)\n",
    "                if isinstance(result, jax.Array):\n",
    "                    result.block_until_ready()\n",
    "            \n",
    "            avg_time = (time.time() - start_time) / n_trials\n",
    "            size_results[op_name] = avg_time\n",
    "            \n",
    "            print(f\"  {op_name:12}: {avg_time:.4f}s\")\n",
    "        \n",
    "        results[size] = size_results\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run benchmarks\n",
    "benchmark_results = benchmark_collective_operations()\n",
    "\n",
    "# Analyze scaling\n",
    "print(\"\\nScaling Analysis:\")\n",
    "for op_name in ['psum', 'pmean', 'pmax', 'all_gather']:\n",
    "    times = [benchmark_results[size][op_name] for size in [100, 1000, 10000]]\n",
    "    scaling_100_to_1k = times[1] / times[0]\n",
    "    scaling_1k_to_10k = times[2] / times[1]\n",
    "    \n",
    "    print(f\"{op_name:12}: 100->1K: {scaling_100_to_1k:.2f}x, 1K->10K: {scaling_1k_to_10k:.2f}x\")\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we explored collective communication operations in JAX:\n",
    "\n",
    "**Basic Collectives:**\n",
    "- `psum`: Sum across devices\n",
    "- `pmean`: Average across devices  \n",
    "- `pmax/pmin`: Maximum/minimum across devices\n",
    "- `all_gather`: Gather data from all devices\n",
    "\n",
    "**Advanced Patterns:**\n",
    "- Gradient synchronization for distributed training\n",
    "- Custom collective operations for specific algorithms\n",
    "- Hierarchical communication patterns\n",
    "- Communication-efficient training strategies\n",
    "\n",
    "**Key Applications:**\n",
    "- **Distributed Training**: Gradient averaging and parameter synchronization\n",
    "- **Statistics Computation**: Global mean, variance, and other statistics\n",
    "- **Consensus Algorithms**: Agreement on values across devices\n",
    "- **Load Balancing**: Distributing work based on global information\n",
    "\n",
    "**Performance Considerations:**\n",
    "- Communication overhead scales with data size and device count\n",
    "- Reduce communication frequency when possible\n",
    "- Use hierarchical patterns for large device counts\n",
    "- Consider bandwidth and latency trade-offs\n",
    "\n",
    "**Best Practices:**\n",
    "- Synchronize gradients, not parameters when possible\n",
    "- Use `pmean` instead of `psum` + manual division\n",
    "- Batch collective operations to reduce overhead\n",
    "- Profile communication patterns to identify bottlenecks\n",
    "\n",
    "Collective operations are essential for scaling JAX computations across multiple devices while maintaining correctness and efficiency in distributed algorithms."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
