{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e508127",
   "metadata": {},
   "source": [
    "# Location: notebooks/05_parallelism/14_pmap_basics.ipynb\n",
    "\n",
    "## Introduction to pmap: Data Parallelism in JAX\n",
    "\n",
    "This notebook introduces JAX's `pmap` (parallel map) transformation for data parallelism. We'll learn how to distribute computations across multiple devices and handle device placement efficiently.\n",
    "\n",
    "## Basic pmap Usage\n",
    "\n",
    "```python\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import pmap\n",
    "import numpy as np\n",
    "\n",
    "# Check available devices\n",
    "print(f\"Available devices: {jax.devices()}\")\n",
    "print(f\"Device count: {jax.device_count()}\")\n",
    "\n",
    "# Simple pmap example\n",
    "def simple_add(x, y):\n",
    "    return x + y\n",
    "\n",
    "# Create pmapped version\n",
    "pmapped_add = pmap(simple_add)\n",
    "\n",
    "# Create data for multiple devices\n",
    "n_devices = jax.device_count()\n",
    "x = jnp.arange(n_devices * 4).reshape(n_devices, 4)\n",
    "y = jnp.ones((n_devices, 4))\n",
    "\n",
    "print(f\"Input x shape: {x.shape}\")\n",
    "print(f\"Input y shape: {y.shape}\")\n",
    "\n",
    "result = pmapped_add(x, y)\n",
    "print(f\"Result shape: {result.shape}\")\n",
    "print(f\"Result: {result}\")\n",
    "```\n",
    "\n",
    "## Matrix Operations with pmap\n",
    "\n",
    "```python\n",
    "def matrix_multiply(A, B):\n",
    "    return jnp.dot(A, B)\n",
    "\n",
    "pmapped_matmul = pmap(matrix_multiply)\n",
    "\n",
    "# Create batch of matrices\n",
    "batch_size = n_devices\n",
    "A = jax.random.normal(jax.random.PRNGKey(42), (batch_size, 64, 32))\n",
    "B = jax.random.normal(jax.random.PRNGKey(43), (batch_size, 32, 16))\n",
    "\n",
    "# Parallel matrix multiplication\n",
    "result = pmapped_matmul(A, B)\n",
    "print(f\"A shape: {A.shape}\")\n",
    "print(f\"B shape: {B.shape}\")\n",
    "print(f\"Result shape: {result.shape}\")\n",
    "\n",
    "# Verify correctness\n",
    "sequential_result = jnp.stack([jnp.dot(A[i], B[i]) for i in range(batch_size)])\n",
    "print(f\"Results match: {jnp.allclose(result, sequential_result)}\")\n",
    "```\n",
    "\n",
    "## Neural Network Forward Pass with pmap\n",
    "\n",
    "```python\n",
    "def init_mlp_params(key, layer_sizes):\n",
    "    keys = jax.random.split(key, len(layer_sizes))\n",
    "    params = []\n",
    "    for i, (in_size, out_size) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "        w_key, b_key = jax.random.split(keys[i])\n",
    "        params.append({\n",
    "            'w': jax.random.normal(w_key, (in_size, out_size)) * 0.1,\n",
    "            'b': jnp.zeros(out_size)\n",
    "        })\n",
    "    return params\n",
    "\n",
    "def mlp_forward(params, x):\n",
    "    for layer in params[:-1]:\n",
    "        x = jnp.tanh(jnp.dot(x, layer['w']) + layer['b'])\n",
    "    # Final layer (no activation)\n",
    "    final_layer = params[-1]\n",
    "    return jnp.dot(x, final_layer['w']) + final_layer['b']\n",
    "\n",
    "# Create model\n",
    "layer_sizes = [784, 256, 128, 10]\n",
    "params = init_mlp_params(jax.random.PRNGKey(0), layer_sizes)\n",
    "\n",
    "# Create pmapped forward pass\n",
    "pmapped_forward = pmap(mlp_forward, in_axes=(None, 0))\n",
    "\n",
    "# Create batch of inputs\n",
    "batch_size = n_devices\n",
    "inputs = jax.random.normal(jax.random.PRNGKey(1), (batch_size, 784))\n",
    "\n",
    "# Parallel forward pass\n",
    "outputs = pmapped_forward(params, inputs)\n",
    "print(f\"Input shape: {inputs.shape}\")\n",
    "print(f\"Output shape: {outputs.shape}\")\n",
    "```\n",
    "\n",
    "## Handling Device Placement and Replication\n",
    "\n",
    "```python\n",
    "# Device placement utilities\n",
    "def replicate_across_devices(x):\n",
    "    \"\"\"Replicate data across all devices\"\"\"\n",
    "    return jnp.broadcast_to(x, (n_devices,) + x.shape)\n",
    "\n",
    "def shard_across_devices(x, axis=0):\n",
    "    \"\"\"Shard data across devices along specified axis\"\"\"\n",
    "    return x.reshape(n_devices, -1, *x.shape[axis+1:])\n",
    "\n",
    "# Example: replicate parameters, shard data\n",
    "single_param = jax.random.normal(jax.random.PRNGKey(10), (64, 32))\n",
    "replicated_params = replicate_across_devices(single_param)\n",
    "\n",
    "large_data = jax.random.normal(jax.random.PRNGKey(11), (n_devices * 8, 64))\n",
    "sharded_data = shard_across_devices(large_data)\n",
    "\n",
    "print(f\"Original param shape: {single_param.shape}\")\n",
    "print(f\"Replicated param shape: {replicated_params.shape}\")\n",
    "print(f\"Original data shape: {large_data.shape}\")\n",
    "print(f\"Sharded data shape: {sharded_data.shape}\")\n",
    "\n",
    "# Simple computation with replicated params and sharded data\n",
    "def compute_with_params(params, data):\n",
    "    return jnp.sum(data @ params, axis=1)\n",
    "\n",
    "pmapped_compute = pmap(compute_with_params, in_axes=(0, 0))\n",
    "result = pmapped_compute(replicated_params, sharded_data)\n",
    "print(f\"Computation result shape: {result.shape}\")\n",
    "```\n",
    "\n",
    "## Reduction Operations in pmap\n",
    "\n",
    "```python\n",
    "from jax import lax\n",
    "\n",
    "def parallel_sum_and_mean(x):\n",
    "    # Local computation\n",
    "    local_sum = jnp.sum(x)\n",
    "    local_count = x.size\n",
    "    \n",
    "    # Cross-device reductions\n",
    "    global_sum = lax.psum(local_sum, axis_name='devices')\n",
    "    global_count = lax.psum(local_count, axis_name='devices')\n",
    "    \n",
    "    global_mean = global_sum / global_count\n",
    "    \n",
    "    return {\n",
    "        'local_sum': local_sum,\n",
    "        'global_sum': global_sum,\n",
    "        'global_mean': global_mean\n",
    "    }\n",
    "\n",
    "pmapped_reduction = pmap(parallel_sum_and_mean, axis_name='devices')\n",
    "\n",
    "# Test data\n",
    "test_data = jax.random.normal(jax.random.PRNGKey(20), (n_devices, 1000))\n",
    "\n",
    "results = pmapped_reduction(test_data)\n",
    "print(f\"Local sums: {results['local_sum']}\")\n",
    "print(f\"Global sum (all devices): {results['global_sum'][0]}\")\n",
    "print(f\"Global mean (all devices): {results['global_mean'][0]}\")\n",
    "\n",
    "# Verify\n",
    "expected_sum = jnp.sum(test_data)\n",
    "expected_mean = jnp.mean(test_data)\n",
    "print(f\"Expected sum: {expected_sum}\")\n",
    "print(f\"Expected mean: {expected_mean}\")\n",
    "print(f\"Sum matches: {jnp.allclose(results['global_sum'][0], expected_sum)}\")\n",
    "print(f\"Mean matches: {jnp.allclose(results['global_mean'][0], expected_mean)}\")\n",
    "```\n",
    "\n",
    "## Training Step with pmap\n",
    "\n",
    "```python\n",
    "def loss_fn(params, x, y):\n",
    "    \"\"\"Simple MSE loss\"\"\"\n",
    "    pred = mlp_forward(params, x)\n",
    "    return jnp.mean((pred - y) ** 2)\n",
    "\n",
    "def training_step(params, x, y, lr=0.01):\n",
    "    \"\"\"Single training step with gradient computation\"\"\"\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params, x, y)\n",
    "    \n",
    "    # Update parameters\n",
    "    new_params = []\n",
    "    for param, grad in zip(params, grads):\n",
    "        new_param = {\n",
    "            'w': param['w'] - lr * grad['w'],\n",
    "            'b': param['b'] - lr * grad['b']\n",
    "        }\n",
    "        new_params.append(new_param)\n",
    "    \n",
    "    return new_params, loss\n",
    "\n",
    "# Create pmapped training step\n",
    "pmapped_train_step = pmap(training_step, in_axes=(None, 0, 0), axis_name='devices')\n",
    "\n",
    "# Create training data\n",
    "train_x = jax.random.normal(jax.random.PRNGKey(30), (n_devices, 784))\n",
    "train_y = jax.random.normal(jax.random.PRNGKey(31), (n_devices, 10))\n",
    "\n",
    "# Perform parallel training step\n",
    "new_params, losses = pmapped_train_step(params, train_x, train_y)\n",
    "\n",
    "print(f\"Losses across devices: {losses}\")\n",
    "print(f\"New params type: {type(new_params)}\")\n",
    "print(f\"First layer weight shape: {new_params[0]['w'].shape}\")\n",
    "\n",
    "# Average gradients across devices (for synchronous training)\n",
    "def average_params_across_devices(params):\n",
    "    \"\"\"Average parameters across all devices\"\"\"\n",
    "    averaged_params = []\n",
    "    for layer in params:\n",
    "        averaged_layer = {\n",
    "            'w': jnp.mean(layer['w'], axis=0, keepdims=True),\n",
    "            'b': jnp.mean(layer['b'], axis=0, keepdims=True)\n",
    "        }\n",
    "        averaged_params.append(averaged_layer)\n",
    "    return averaged_params\n",
    "\n",
    "# This would typically be done after psum in the training step\n",
    "# averaged_params = average_params_across_devices(new_params)\n",
    "```\n",
    "\n",
    "## Performance Comparison\n",
    "\n",
    "```python\n",
    "import time\n",
    "\n",
    "def benchmark_computation(data, n_trials=10):\n",
    "    \"\"\"Benchmark sequential vs parallel computation\"\"\"\n",
    "    \n",
    "    def sequential_computation(x):\n",
    "        return jnp.sum(x ** 2 + jnp.sin(x) * jnp.cos(x))\n",
    "    \n",
    "    def parallel_computation(x):\n",
    "        return jnp.sum(x ** 2 + jnp.sin(x) * jnp.cos(x))\n",
    "    \n",
    "    pmapped_computation = pmap(parallel_computation)\n",
    "    \n",
    "    # Sequential timing\n",
    "    sequential_data = data.reshape(-1, data.shape[-1])\n",
    "    start_time = time.time()\n",
    "    for _ in range(n_trials):\n",
    "        results = [sequential_computation(x) for x in sequential_data]\n",
    "    sequential_time = (time.time() - start_time) / n_trials\n",
    "    \n",
    "    # Parallel timing\n",
    "    start_time = time.time()\n",
    "    for _ in range(n_trials):\n",
    "        result = pmapped_computation(data)\n",
    "    parallel_time = (time.time() - start_time) / n_trials\n",
    "    \n",
    "    return sequential_time, parallel_time\n",
    "\n",
    "# Benchmark with different data sizes\n",
    "sizes = [1000, 10000, 100000]\n",
    "for size in sizes:\n",
    "    test_data = jax.random.normal(jax.random.PRNGKey(40), (n_devices, size))\n",
    "    seq_time, par_time = benchmark_computation(test_data)\n",
    "    speedup = seq_time / par_time\n",
    "    \n",
    "    print(f\"Data size: {size}\")\n",
    "    print(f\"  Sequential time: {seq_time:.4f}s\")\n",
    "    print(f\"  Parallel time: {par_time:.4f}s\")\n",
    "    print(f\"  Speedup: {speedup:.2f}x\")\n",
    "    print()\n",
    "```\n",
    "\n",
    "## Common pmap Patterns and Best Practices\n",
    "\n",
    "```python\n",
    "# Pattern 1: Device-specific computations\n",
    "def device_specific_computation(x, device_id):\n",
    "    # Use device_id for different behavior per device\n",
    "    return x * (device_id + 1)\n",
    "\n",
    "# Get device IDs for pmap\n",
    "device_ids = jnp.arange(n_devices)\n",
    "test_data = jnp.ones((n_devices, 10))\n",
    "\n",
    "pmapped_device_specific = pmap(device_specific_computation)\n",
    "result = pmapped_device_specific(test_data, device_ids)\n",
    "print(f\"Device-specific results:\\n{result}\")\n",
    "\n",
    "# Pattern 2: Conditional execution based on device\n",
    "def conditional_computation(x, is_lead_device):\n",
    "    def lead_computation():\n",
    "        return jnp.sum(x) * 2\n",
    "    \n",
    "    def other_computation():\n",
    "        return jnp.mean(x)\n",
    "    \n",
    "    return lax.cond(is_lead_device, lead_computation, other_computation)\n",
    "\n",
    "lead_flags = jnp.array([i == 0 for i in range(n_devices)])\n",
    "pmapped_conditional = pmap(conditional_computation)\n",
    "result = pmapped_conditional(test_data, lead_flags)\n",
    "print(f\"Conditional results: {result}\")\n",
    "\n",
    "# Pattern 3: Efficient data movement\n",
    "def efficient_data_pattern(large_array):\n",
    "    # Minimize data movement by keeping computations local\n",
    "    local_mean = jnp.mean(large_array, axis=0)\n",
    "    local_std = jnp.std(large_array, axis=0)\n",
    "    \n",
    "    # Only communicate necessary statistics\n",
    "    global_mean = lax.pmean(local_mean, axis_name='devices')\n",
    "    \n",
    "    return {\n",
    "        'local_mean': local_mean,\n",
    "        'global_mean': global_mean,\n",
    "        'local_std': local_std\n",
    "    }\n",
    "\n",
    "pmapped_efficient = pmap(efficient_data_pattern, axis_name='devices')\n",
    "large_test_data = jax.random.normal(jax.random.PRNGKey(50), (n_devices, 1000, 64))\n",
    "results = pmapped_efficient(large_test_data)\n",
    "print(f\"Efficient pattern results shapes:\")\n",
    "print(f\"  Local mean: {results['local_mean'].shape}\")\n",
    "print(f\"  Global mean: {results['global_mean'].shape}\")\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we explored JAX's `pmap` for data parallelism:\n",
    "\n",
    "**Key Concepts:**\n",
    "- `pmap` distributes computation across multiple devices\n",
    "- Data must be sharded along the leading dimension to match device count\n",
    "- Parameters can be replicated across devices using appropriate `in_axes`\n",
    "- Collective operations like `psum` and `pmean` enable cross-device communication\n",
    "\n",
    "**Best Practices:**\n",
    "- Use `pmap` for embarrassingly parallel problems\n",
    "- Minimize cross-device communication for better performance\n",
    "- Replicate parameters and shard data appropriately\n",
    "- Consider device-specific computations when needed\n",
    "\n",
    "**Common Patterns:**\n",
    "- Training steps with gradient averaging\n",
    "- Batch processing with parameter sharing\n",
    "- Reduction operations across devices\n",
    "- Efficient data movement strategies\n",
    "\n",
    "`pmap` provides a simple but powerful way to scale computations across multiple devices while maintaining JAX's functional programming model and automatic differentiation capabilities."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
