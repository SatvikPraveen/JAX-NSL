{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d39a1d7e",
   "metadata": {},
   "source": [
    "# File: notebooks/02_linear_algebra/07_numerical_stability.ipynb\n",
    "\n",
    "## JAX Linear Algebra: Numerical Stability\n",
    "\n",
    "This notebook explores numerical stability in linear algebra computations. We'll cover condition numbers, numerical precision, stable algorithms, and techniques for handling ill-conditioned problems. Understanding numerical stability is crucial for reliable scientific computing and machine learning applications.\n",
    "\n",
    "Numerical stability determines whether small perturbations in input data lead to small changes in output, which is essential for robust algorithms in the presence of round-off errors and data uncertainty.\n",
    "\n",
    "## Setting Up the Environment\n",
    "\n",
    "```python\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, lax\n",
    "from jax import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Optional\n",
    "import functools\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "```\n",
    "\n",
    "## Understanding Condition Numbers\n",
    "\n",
    "### Matrix Condition Number Analysis\n",
    "\n",
    "```python\n",
    "def condition_number_analysis():\n",
    "    \"\"\"Analyze how condition numbers affect numerical stability\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(42)\n",
    "    n = 10\n",
    "    \n",
    "    # Create matrices with different condition numbers\n",
    "    condition_numbers = [1e1, 1e3, 1e6, 1e9, 1e12]\n",
    "    \n",
    "    print(\"Condition Number Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for cond_target in condition_numbers:\n",
    "        # Create matrix with specific condition number\n",
    "        U = random.orthogonal(key, n)\n",
    "        V = random.orthogonal(random.split(key)[1], n)\n",
    "        \n",
    "        # Logarithmically spaced singular values\n",
    "        singular_values = jnp.logspace(0, jnp.log10(cond_target), n)\n",
    "        A = U @ jnp.diag(singular_values) @ V.T\n",
    "        \n",
    "        actual_cond = jnp.linalg.cond(A)\n",
    "        \n",
    "        # Test linear solve stability\n",
    "        x_true = random.normal(key, (n,))\n",
    "        b = A @ x_true\n",
    "        \n",
    "        # Add small perturbation to b\n",
    "        perturbation = 1e-14 * random.normal(random.split(key)[1], (n,))\n",
    "        b_perturbed = b + perturbation\n",
    "        \n",
    "        # Solve both systems\n",
    "        x_computed = jnp.linalg.solve(A, b)\n",
    "        x_perturbed = jnp.linalg.solve(A, b_perturbed)\n",
    "        \n",
    "        # Compute relative errors\n",
    "        rel_error = jnp.linalg.norm(x_computed - x_true) / jnp.linalg.norm(x_true)\n",
    "        perturbation_amplification = (jnp.linalg.norm(x_perturbed - x_computed) / jnp.linalg.norm(x_computed)) / (jnp.linalg.norm(perturbation) / jnp.linalg.norm(b))\n",
    "        \n",
    "        print(f\"Target κ: {cond_target:.0e}, Actual κ: {actual_cond:.2e}\")\n",
    "        print(f\"  Relative solve error: {rel_error:.2e}\")\n",
    "        print(f\"  Perturbation amplification: {perturbation_amplification:.2e}\")\n",
    "        print(f\"  Theoretical bound: {actual_cond * 1e-14:.2e}\")\n",
    "        print()\n",
    "\n",
    "condition_number_analysis()\n",
    "```\n",
    "\n",
    "### Hilbert Matrix Example\n",
    "\n",
    "```python\n",
    "def hilbert_matrix_stability():\n",
    "    \"\"\"Analyze the notoriously ill-conditioned Hilbert matrix\"\"\"\n",
    "    \n",
    "    def hilbert_matrix(n):\n",
    "        \"\"\"Generate n×n Hilbert matrix H[i,j] = 1/(i+j+1)\"\"\"\n",
    "        i, j = jnp.meshgrid(jnp.arange(n), jnp.arange(n))\n",
    "        return 1.0 / (i + j + 1)\n",
    "    \n",
    "    print(\"Hilbert Matrix Stability Analysis:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for n in [5, 8, 10, 12]:\n",
    "        H = hilbert_matrix(n)\n",
    "        cond_H = jnp.linalg.cond(H)\n",
    "        \n",
    "        # Known exact solution\n",
    "        x_exact = jnp.ones(n)\n",
    "        b = H @ x_exact\n",
    "        \n",
    "        # Solve using different methods\n",
    "        x_solve = jnp.linalg.solve(H, b)\n",
    "        x_lstsq = jnp.linalg.lstsq(H, b, rcond=None)[0]\n",
    "        \n",
    "        # SVD-based solve with truncation\n",
    "        U, s, Vt = jnp.linalg.svd(H)\n",
    "        threshold = 1e-12\n",
    "        s_inv = jnp.where(s > threshold, 1/s, 0)\n",
    "        x_svd = Vt.T @ (s_inv[:, None] * (U.T @ b[:, None]))[:, 0]\n",
    "        \n",
    "        print(f\"n = {n}: κ(H) = {cond_H:.2e}\")\n",
    "        print(f\"  Direct solve error: {jnp.linalg.norm(x_solve - x_exact):.2e}\")\n",
    "        print(f\"  Least squares error: {jnp.linalg.norm(x_lstsq - x_exact):.2e}\")\n",
    "        print(f\"  SVD solve error: {jnp.linalg.norm(x_svd - x_exact):.2e}\")\n",
    "        print()\n",
    "\n",
    "hilbert_matrix_stability()\n",
    "```\n",
    "\n",
    "## Stable Algorithms for Common Operations\n",
    "\n",
    "### Numerically Stable Sum and Products\n",
    "\n",
    "```python\n",
    "def stable_summation():\n",
    "    \"\"\"Compare numerical stability of different summation algorithms\"\"\"\n",
    "    \n",
    "    # Create data that challenges floating-point arithmetic\n",
    "    key = random.PRNGKey(0)\n",
    "    \n",
    "    # Large numbers with small differences\n",
    "    large_vals = 1e10 + random.normal(key, (1000,)) * 1e-5\n",
    "    small_vals = random.normal(random.split(key)[1], (1000,)) * 1e-10\n",
    "    \n",
    "    mixed_data = jnp.concatenate([large_vals, small_vals])\n",
    "    mixed_data = random.permutation(random.split(key, 3)[2], mixed_data)\n",
    "    \n",
    "    # Standard summation\n",
    "    sum_standard = jnp.sum(mixed_data)\n",
    "    \n",
    "    # Kahan summation (stable summation)\n",
    "    def kahan_sum(arr):\n",
    "        def kahan_step(carry, x):\n",
    "            sum_val, c = carry\n",
    "            y = x - c\n",
    "            t = sum_val + y\n",
    "            c = (t - sum_val) - y\n",
    "            return (t, c), t\n",
    "        \n",
    "        final_carry, _ = lax.scan(kahan_step, (0.0, 0.0), arr)\n",
    "        return final_carry[0]\n",
    "    \n",
    "    sum_kahan = kahan_sum(mixed_data)\n",
    "    \n",
    "    # Sorted summation (add small values first)\n",
    "    sorted_data = jnp.sort(jnp.abs(mixed_data))\n",
    "    sum_sorted = jnp.sum(jnp.where(mixed_data >= 0, \n",
    "                                  sorted_data, \n",
    "                                  -sorted_data))\n",
    "    \n",
    "    print(\"Stable Summation Comparison:\")\n",
    "    print(f\"Standard sum: {sum_standard:.10e}\")\n",
    "    print(f\"Kahan sum: {sum_kahan:.10e}\")  \n",
    "    print(f\"Sorted sum: {sum_sorted:.10e}\")\n",
    "    print(f\"Difference (Kahan - Standard): {sum_kahan - sum_standard:.2e}\")\n",
    "\n",
    "stable_summation()\n",
    "```\n",
    "\n",
    "### Stable Matrix Operations\n",
    "\n",
    "```python\n",
    "def stable_matrix_operations():\n",
    "    \"\"\"Demonstrate stable implementations of common matrix operations\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(123)\n",
    "    \n",
    "    # Log-sum-exp: stable computation of log(sum(exp(x)))\n",
    "    def logsumexp_unstable(x):\n",
    "        return jnp.log(jnp.sum(jnp.exp(x)))\n",
    "    \n",
    "    def logsumexp_stable(x):\n",
    "        x_max = jnp.max(x)\n",
    "        return x_max + jnp.log(jnp.sum(jnp.exp(x - x_max)))\n",
    "    \n",
    "    # Test with large values\n",
    "    large_values = jnp.array([700.0, 800.0, 900.0])  # Would overflow exp()\n",
    "    \n",
    "    print(\"Log-Sum-Exp Stability:\")\n",
    "    try:\n",
    "        unstable_result = logsumexp_unstable(large_values)\n",
    "        print(f\"Unstable result: {unstable_result}\")\n",
    "    except:\n",
    "        print(\"Unstable version: overflow/underflow\")\n",
    "    \n",
    "    stable_result = logsumexp_stable(large_values)\n",
    "    jax_result = jax.nn.logsumexp(large_values)\n",
    "    print(f\"Stable result: {stable_result:.6f}\")\n",
    "    print(f\"JAX result: {jax_result:.6f}\")\n",
    "    \n",
    "    # Stable softmax\n",
    "    def softmax_unstable(x):\n",
    "        exp_x = jnp.exp(x)\n",
    "        return exp_x / jnp.sum(exp_x)\n",
    "    \n",
    "    def softmax_stable(x):\n",
    "        x_shifted = x - jnp.max(x)\n",
    "        exp_x = jnp.exp(x_shifted)\n",
    "        return exp_x / jnp.sum(exp_x)\n",
    "    \n",
    "    # Test softmax\n",
    "    large_logits = jnp.array([1000.0, 1001.0, 999.0])\n",
    "    \n",
    "    print(\"\\nSoftmax Stability:\")\n",
    "    try:\n",
    "        unstable_softmax = softmax_unstable(large_logits)\n",
    "        print(f\"Unstable softmax: {unstable_softmax}\")\n",
    "    except:\n",
    "        print(\"Unstable softmax: overflow\")\n",
    "    \n",
    "    stable_softmax = softmax_stable(large_logits)\n",
    "    jax_softmax = jax.nn.softmax(large_logits)\n",
    "    print(f\"Stable softmax: {stable_softmax}\")\n",
    "    print(f\"JAX softmax: {jax_softmax}\")\n",
    "\n",
    "stable_matrix_operations()\n",
    "```\n",
    "\n",
    "## Regularization Techniques\n",
    "\n",
    "### Ridge Regression and Regularization\n",
    "\n",
    "```python\n",
    "def regularization_demo():\n",
    "    \"\"\"Demonstrate regularization for ill-conditioned problems\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(42)\n",
    "    \n",
    "    # Create ill-conditioned regression problem\n",
    "    n_samples, n_features = 50, 45  # More features than samples\n",
    "    X = random.normal(key, (n_samples, n_features))\n",
    "    true_beta = random.normal(random.split(key)[1], (n_features,))\n",
    "    y = X @ true_beta + 0.1 * random.normal(random.split(key, 3)[2], (n_samples,))\n",
    "    \n",
    "    # Normal equations: X^T X β = X^T y\n",
    "    XtX = X.T @ X\n",
    "    Xty = X.T @ y\n",
    "    \n",
    "    print(\"Regularization for Ill-conditioned Problems:\")\n",
    "    print(f\"Condition number of X^T X: {jnp.linalg.cond(XtX):.2e}\")\n",
    "    \n",
    "    # Different regularization levels\n",
    "    lambda_values = [0, 1e-6, 1e-3, 1e-1, 1.0]\n",
    "    \n",
    "    for lam in lambda_values:\n",
    "        # Ridge regression: (X^T X + λI) β = X^T y\n",
    "        regularized_matrix = XtX + lam * jnp.eye(n_features)\n",
    "        beta_ridge = jnp.linalg.solve(regularized_matrix, Xty)\n",
    "        \n",
    "        # Compute metrics\n",
    "        train_error = jnp.mean((X @ beta_ridge - y)**2)\n",
    "        param_error = jnp.linalg.norm(beta_ridge - true_beta)\n",
    "        cond_regularized = jnp.linalg.cond(regularized_matrix)\n",
    "        \n",
    "        print(f\"λ = {lam:6.0e}: cond = {cond_regularized:8.2e}, \"\n",
    "              f\"train_err = {train_error:.4f}, param_err = {param_error:.4f}\")\n",
    "\n",
    "regularization_demo()\n",
    "```\n",
    "\n",
    "### Singular Value Truncation\n",
    "\n",
    "```python\n",
    "def svd_regularization():\n",
    "    \"\"\"Use SVD truncation for regularization\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(0)\n",
    "    \n",
    "    # Create low-rank matrix with noise\n",
    "    rank = 5\n",
    "    m, n = 20, 15\n",
    "    \n",
    "    U_true = random.normal(key, (m, rank))\n",
    "    V_true = random.normal(random.split(key)[1], (n, rank))\n",
    "    A_clean = U_true @ V_true.T\n",
    "    \n",
    "    # Add noise\n",
    "    noise = 0.1 * random.normal(random.split(key, 3)[2], (m, n))\n",
    "    A_noisy = A_clean + noise\n",
    "    \n",
    "    print(\"SVD-based Regularization:\")\n",
    "    print(f\"True rank: {rank}, Matrix shape: {A_noisy.shape}\")\n",
    "    \n",
    "    # SVD of noisy matrix\n",
    "    U, s, Vt = jnp.linalg.svd(A_noisy, full_matrices=False)\n",
    "    \n",
    "    print(f\"Singular values: {s[:8]}\")\n",
    "    \n",
    "    # Truncated SVD reconstruction\n",
    "    truncation_ranks = [3, 5, 7, 10, min(m, n)]\n",
    "    \n",
    "    for r in truncation_ranks:\n",
    "        A_truncated = U[:, :r] @ jnp.diag(s[:r]) @ Vt[:r, :]\n",
    "        \n",
    "        reconstruction_error = jnp.linalg.norm(A_truncated - A_clean, 'fro')\n",
    "        compression_ratio = r * (m + n) / (m * n)\n",
    "        \n",
    "        print(f\"Rank {r:2d}: error = {reconstruction_error:.4f}, \"\n",
    "              f\"compression = {compression_ratio:.3f}\")\n",
    "\n",
    "svd_regularization()\n",
    "```\n",
    "\n",
    "## Precision and Round-off Error Analysis\n",
    "\n",
    "### Float32 vs Float64 Comparison\n",
    "\n",
    "```python\n",
    "def precision_comparison():\n",
    "    \"\"\"Compare numerical precision between float32 and float64\"\"\"\n",
    "    \n",
    "    # Test with different precision levels\n",
    "    def test_precision(dtype_str):\n",
    "        if dtype_str == 'float32':\n",
    "            jax.config.update(\"jax_enable_x64\", False)\n",
    "        else:\n",
    "            jax.config.update(\"jax_enable_x64\", True)\n",
    "        \n",
    "        key = random.PRNGKey(42)\n",
    "        \n",
    "        # Create a moderately ill-conditioned problem\n",
    "        n = 20\n",
    "        U = random.orthogonal(key, n)\n",
    "        singular_values = jnp.logspace(0, 6, n)  # Condition number ~1e6\n",
    "        A = U @ jnp.diag(singular_values) @ U.T\n",
    "        \n",
    "        x_true = jnp.ones(n)\n",
    "        b = A @ x_true\n",
    "        \n",
    "        # Solve system\n",
    "        x_computed = jnp.linalg.solve(A, b)\n",
    "        \n",
    "        # Compute errors\n",
    "        forward_error = jnp.linalg.norm(x_computed - x_true)\n",
    "        backward_error = jnp.linalg.norm(A @ x_computed - b)\n",
    "        \n",
    "        return forward_error, backward_error, jnp.linalg.cond(A)\n",
    "    \n",
    "    print(\"Precision Comparison (Float32 vs Float64):\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test float32\n",
    "    forward_32, backward_32, cond = test_precision('float32')\n",
    "    \n",
    "    # Test float64  \n",
    "    forward_64, backward_64, cond = test_precision('float64')\n",
    "    \n",
    "    print(f\"Matrix condition number: {cond:.2e}\")\n",
    "    print(f\"Float32 - Forward error: {forward_32:.2e}, Backward error: {backward_32:.2e}\")\n",
    "    print(f\"Float64 - Forward error: {forward_64:.2e}, Backward error: {backward_64:.2e}\")\n",
    "    print(f\"Improvement factor: {forward_32/forward_64:.1f}x\")\n",
    "    \n",
    "    # Reset to float64\n",
    "    jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "precision_comparison()\n",
    "```\n",
    "\n",
    "## Error Analysis and Bounds\n",
    "\n",
    "### Forward and Backward Error Analysis\n",
    "\n",
    "```python\n",
    "def error_analysis():\n",
    "    \"\"\"Demonstrate forward vs backward error analysis\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(123)\n",
    "    \n",
    "    # Create test problems with different condition numbers\n",
    "    condition_numbers = [1e2, 1e6, 1e10]\n",
    "    \n",
    "    print(\"Forward vs Backward Error Analysis:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for cond_target in condition_numbers:\n",
    "        n = 15\n",
    "        \n",
    "        # Create matrix with target condition number\n",
    "        U = random.orthogonal(key, n)\n",
    "        singular_values = jnp.logspace(0, jnp.log10(cond_target), n)\n",
    "        A = U @ jnp.diag(singular_values) @ U.T\n",
    "        \n",
    "        x_true = random.normal(key, (n,))\n",
    "        b_exact = A @ x_true\n",
    "        \n",
    "        # Add small perturbation to simulate round-off\n",
    "        perturbation = 1e-12 * jnp.linalg.norm(b_exact) * random.normal(random.split(key)[1], (n,))\n",
    "        b_perturbed = b_exact + perturbation\n",
    "        \n",
    "        # Solve perturbed system\n",
    "        x_computed = jnp.linalg.solve(A, b_perturbed)\n",
    "        \n",
    "        # Forward error: ||x_computed - x_true|| / ||x_true||\n",
    "        forward_error = jnp.linalg.norm(x_computed - x_true) / jnp.linalg.norm(x_true)\n",
    "        \n",
    "        # Backward error: ||A*x_computed - b|| / ||b||\n",
    "        backward_error = jnp.linalg.norm(A @ x_computed - b_exact) / jnp.linalg.norm(b_exact)\n",
    "        \n",
    "        # Theoretical bounds\n",
    "        actual_cond = jnp.linalg.cond(A)\n",
    "        data_perturbation = jnp.linalg.norm(perturbation) / jnp.linalg.norm(b_exact)\n",
    "        theoretical_bound = actual_cond * data_perturbation\n",
    "        \n",
    "        print(f\"κ = {actual_cond:.2e}\")\n",
    "        print(f\"  Data perturbation: {data_perturbation:.2e}\")\n",
    "        print(f\"  Forward error: {forward_error:.2e}\")\n",
    "        print(f\"  Backward error: {backward_error:.2e}\")\n",
    "        print(f\"  Theoretical bound: {theoretical_bound:.2e}\")\n",
    "        print(f\"  Bound tightness: {forward_error/theoretical_bound:.2f}\")\n",
    "        print()\n",
    "\n",
    "error_analysis()\n",
    "```\n",
    "\n",
    "## Iterative Refinement\n",
    "\n",
    "### Improving Solution Accuracy\n",
    "\n",
    "```python\n",
    "def iterative_refinement():\n",
    "    \"\"\"Demonstrate iterative refinement for improved accuracy\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(456)\n",
    "    \n",
    "    # Create moderately ill-conditioned system\n",
    "    n = 20\n",
    "    A = random.normal(key, (n, n))\n",
    "    A = A @ A.T + 1e-8 * jnp.eye(n)  # Make SPD but ill-conditioned\n",
    "    \n",
    "    x_true = random.normal(random.split(key)[1], (n,))\n",
    "    b = A @ x_true\n",
    "    \n",
    "    print(\"Iterative Refinement:\")\n",
    "    print(f\"Condition number: {jnp.linalg.cond(A):.2e}\")\n",
    "    \n",
    "    # Initial solution\n",
    "    x = jnp.linalg.solve(A, b)\n",
    "    \n",
    "    print(f\"Initial error: {jnp.linalg.norm(x - x_true):.2e}\")\n",
    "    \n",
    "    # Iterative refinement\n",
    "    for iteration in range(5):\n",
    "        # Compute residual\n",
    "        r = b - A @ x\n",
    "        \n",
    "        # Solve correction equation A * δx = r\n",
    "        delta_x = jnp.linalg.solve(A, r)\n",
    "        \n",
    "        # Update solution\n",
    "        x = x + delta_x\n",
    "        \n",
    "        error = jnp.linalg.norm(x - x_true)\n",
    "        residual_norm = jnp.linalg.norm(r)\n",
    "        \n",
    "        print(f\"Iter {iteration+1}: error = {error:.2e}, residual = {residual_norm:.2e}\")\n",
    "        \n",
    "        if residual_norm < 1e-14:\n",
    "            break\n",
    "\n",
    "iterative_refinement()\n",
    "```\n",
    "\n",
    "## Practical Stability Guidelines\n",
    "\n",
    "### Choosing Stable Algorithms\n",
    "\n",
    "```python\n",
    "def algorithm_stability_guide():\n",
    "    \"\"\"Guidelines for choosing stable algorithms\"\"\"\n",
    "    \n",
    "    print(\"Algorithm Stability Guidelines:\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    scenarios = [\n",
    "        (\"Well-conditioned SPD system\", \"Cholesky > LU > QR\"),\n",
    "        (\"Ill-conditioned system\", \"SVD with truncation > QR > LU\"),\n",
    "        (\"Overdetermined system\", \"QR factorization > Normal equations\"),\n",
    "        (\"Underdetermined system\", \"SVD > QR with pivoting\"),\n",
    "        (\"Large sparse system\", \"Iterative methods with preconditioning\"),\n",
    "        (\"Eigenvalue problems\", \"eigh for symmetric > eig for general\"),\n",
    "        (\"Optimization\", \"L-BFGS > Newton > Gradient descent\"),\n",
    "    ]\n",
    "    \n",
    "    for scenario, recommendation in scenarios:\n",
    "        print(f\"{scenario:.<30} {recommendation}\")\n",
    "    \n",
    "    print(\"\\nNumerical Stability Checklist:\")\n",
    "    print(\"✓ Check condition numbers before solving\")\n",
    "    print(\"✓ Use appropriate precision (float64 for ill-conditioned problems)\")\n",
    "    print(\"✓ Consider regularization for ill-posed problems\")\n",
    "    print(\"✓ Implement stable algorithms (avoid normal equations)\")\n",
    "    print(\"✓ Monitor residuals and backward error\")\n",
    "    print(\"✓ Use iterative refinement when necessary\")\n",
    "\n",
    "algorithm_stability_guide()\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we've explored critical aspects of numerical stability:\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "1. **Condition Numbers**: Measure sensitivity to perturbations\n",
    "2. **Stable Algorithms**: Methods that control error propagation  \n",
    "3. **Regularization**: Techniques for ill-posed problems\n",
    "4. **Precision Effects**: Float32 vs Float64 trade-offs\n",
    "5. **Error Analysis**: Forward vs backward error measurement\n",
    "\n",
    "**Stability Techniques:**\n",
    "- Use SVD for ill-conditioned problems\n",
    "- Implement regularization for under-determined systems\n",
    "- Choose appropriate algorithms based on matrix properties\n",
    "- Monitor condition numbers and residuals\n",
    "- Apply iterative refinement when needed\n",
    "\n",
    "**JAX-Specific Considerations:**\n",
    "- Enable float64 precision for critical computations\n",
    "- Use built-in stable functions (logsumexp, softmax)\n",
    "- Leverage automatic differentiation for error analysis\n",
    "- Consider numerical stability in custom VJP/JVP implementations\n",
    "\n",
    "**Best Practices:**\n",
    "- Always check condition numbers before solving\n",
    "- Use backward error as a stability indicator\n",
    "- Prefer QR over normal equations for least squares\n",
    "- Apply regularization judiciously\n",
    "- Validate results with multiple methods\n",
    "\n",
    "**Next Steps:**\n",
    "- The next notebook will cover neural network implementations\n",
    "- We'll apply these stability concepts to ML algorithms\n",
    "- Understanding numerical stability is crucial for reliable deep learning\n",
    "\n",
    "Numerical stability is fundamental to reliable scientific computing and forms the foundation for robust machine learning algorithms."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
