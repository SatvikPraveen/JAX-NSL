{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71cbf703",
   "metadata": {},
   "source": [
    "# File: notebooks/02_linear_algebra/05_matrix_ops.ipynb\n",
    "\n",
    "## JAX Linear Algebra: Matrix Operations\n",
    "\n",
    "Welcome to the linear algebra section of the JAX-NSL series! This notebook covers fundamental matrix operations in JAX, including matrix multiplication, decompositions, eigenvalue problems, and advanced tensor operations. We'll explore both the high-level `jax.numpy.linalg` interface and lower-level operations for performance-critical applications.\n",
    "\n",
    "Linear algebra is the backbone of scientific computing and machine learning. JAX's linear algebra operations are designed to be fast, differentiable, and compatible with JAX transformations like JIT compilation and vectorization.\n",
    "\n",
    "## Setting Up the Environment\n",
    "\n",
    "```python\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, lax\n",
    "from jax import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Optional\n",
    "import functools\n",
    "import time\n",
    "\n",
    "# Enable 64-bit precision for numerical stability\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Available devices: {jax.devices()}\")\n",
    "```\n",
    "\n",
    "## Basic Matrix Operations\n",
    "\n",
    "### Matrix Creation and Properties\n",
    "\n",
    "```python\n",
    "# Create various types of matrices\n",
    "def create_test_matrices():\n",
    "    \"\"\"Create different types of matrices for testing\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(42)\n",
    "    \n",
    "    # Random matrices\n",
    "    A_random = random.normal(key, (4, 4))\n",
    "    B_random = random.normal(random.split(key)[1], (4, 3))\n",
    "    \n",
    "    # Symmetric matrix\n",
    "    A_sym = A_random + A_random.T\n",
    "    \n",
    "    # Positive definite matrix\n",
    "    A_pd = A_random @ A_random.T + jnp.eye(4)\n",
    "    \n",
    "    # Orthogonal matrix (via QR decomposition)\n",
    "    Q, R = jnp.linalg.qr(A_random)\n",
    "    A_orth = Q\n",
    "    \n",
    "    # Diagonal matrix\n",
    "    A_diag = jnp.diag(jnp.array([1.0, 2.0, 3.0, 4.0]))\n",
    "    \n",
    "    matrices = {\n",
    "        'random': A_random,\n",
    "        'rectangular': B_random,\n",
    "        'symmetric': A_sym,\n",
    "        'positive_definite': A_pd,\n",
    "        'orthogonal': A_orth,\n",
    "        'diagonal': A_diag\n",
    "    }\n",
    "    \n",
    "    return matrices\n",
    "\n",
    "matrices = create_test_matrices()\n",
    "\n",
    "# Examine properties\n",
    "for name, mat in matrices.items():\n",
    "    print(f\"{name.capitalize()} Matrix:\")\n",
    "    print(f\"  Shape: {mat.shape}\")\n",
    "    print(f\"  Condition number: {jnp.linalg.cond(mat):.2f}\")\n",
    "    print(f\"  Frobenius norm: {jnp.linalg.norm(mat, 'fro'):.2f}\")\n",
    "    print(f\"  Determinant: {jnp.linalg.det(mat):.2e}\" if mat.shape[0] == mat.shape[1] else \"  N/A (not square)\")\n",
    "    print()\n",
    "```\n",
    "\n",
    "### Matrix Multiplication Variants\n",
    "\n",
    "```python\n",
    "# Different matrix multiplication approaches\n",
    "def matrix_multiplication_demo():\n",
    "    \"\"\"Demonstrate different matrix multiplication methods\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(123)\n",
    "    A = random.normal(key, (100, 50))\n",
    "    B = random.normal(random.split(key)[1], (50, 80))\n",
    "    C = random.normal(random.split(key, 3)[2], (80, 30))\n",
    "    \n",
    "    print(\"Matrix shapes: A(100,50), B(50,80), C(80,30)\")\n",
    "    \n",
    "    # Basic matrix multiplication\n",
    "    AB = A @ B  # or jnp.dot(A, B) or jnp.matmul(A, B)\n",
    "    print(f\"A @ B shape: {AB.shape}\")\n",
    "    \n",
    "    # Chain multiplication\n",
    "    ABC_left = (A @ B) @ C  # Left associative\n",
    "    ABC_right = A @ (B @ C)  # Right associative\n",
    "    print(f\"Chain multiplication difference: {jnp.max(jnp.abs(ABC_left - ABC_right)):.2e}\")\n",
    "    \n",
    "    # Batch matrix multiplication\n",
    "    batch_A = random.normal(key, (5, 10, 10))\n",
    "    batch_B = random.normal(random.split(key)[1], (5, 10, 10))\n",
    "    batch_result = batch_A @ batch_B  # Broadcasts correctly\n",
    "    print(f\"Batch multiplication shape: {batch_result.shape}\")\n",
    "    \n",
    "    # Using vmap for explicit batching\n",
    "    single_matmul = lambda a, b: a @ b\n",
    "    vmap_matmul = vmap(single_matmul)\n",
    "    vmap_result = vmap_matmul(batch_A, batch_B)\n",
    "    print(f\"Vmap result matches: {jnp.allclose(batch_result, vmap_result)}\")\n",
    "    \n",
    "    return A, B, C\n",
    "\n",
    "A, B, C = matrix_multiplication_demo()\n",
    "```\n",
    "\n",
    "### Einstein Summation (Einsum)\n",
    "\n",
    "```python\n",
    "# Advanced tensor operations using einsum\n",
    "def einsum_examples():\n",
    "    \"\"\"Demonstrate einsum for complex tensor operations\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(0)\n",
    "    \n",
    "    # Basic examples\n",
    "    a = random.normal(key, (3,))\n",
    "    b = random.normal(random.split(key)[1], (3,))\n",
    "    A = random.normal(random.split(key, 3)[2], (3, 4))\n",
    "    B = random.normal(random.split(key, 4)[3], (4, 5))\n",
    "    \n",
    "    print(\"Einsum Examples:\")\n",
    "    \n",
    "    # Dot product: a Â· b\n",
    "    dot1 = jnp.dot(a, b)\n",
    "    dot2 = jnp.einsum('i,i->', a, b)\n",
    "    print(f\"Dot product: {dot1:.4f} == {dot2:.4f}\")\n",
    "    \n",
    "    # Matrix-vector product: A @ a\n",
    "    mv1 = A @ a\n",
    "    mv2 = jnp.einsum('ij,j->i', A, a)\n",
    "    print(f\"Matrix-vector max diff: {jnp.max(jnp.abs(mv1 - mv2)):.2e}\")\n",
    "    \n",
    "    # Matrix multiplication: A @ B\n",
    "    mm1 = A @ B\n",
    "    mm2 = jnp.einsum('ij,jk->ik', A, B)\n",
    "    print(f\"Matrix mult max diff: {jnp.max(jnp.abs(mm1 - mm2)):.2e}\")\n",
    "    \n",
    "    # Trace\n",
    "    trace1 = jnp.trace(A @ A.T)\n",
    "    trace2 = jnp.einsum('ij,ji->', A, A.T)\n",
    "    print(f\"Trace: {trace1:.4f} == {trace2:.4f}\")\n",
    "    \n",
    "    # More complex tensor operations\n",
    "    T = random.normal(key, (3, 4, 5, 6))\n",
    "    \n",
    "    # Sum over specific axes\n",
    "    sum_02 = jnp.einsum('ijkl->jl', T)  # Sum over axes 0 and 2\n",
    "    print(f\"Tensor sum shape: {sum_02.shape}\")\n",
    "    \n",
    "    # Tensor contraction\n",
    "    contraction = jnp.einsum('ijkl,jl->ik', T, B)\n",
    "    print(f\"Tensor contraction shape: {contraction.shape}\")\n",
    "\n",
    "einsum_examples()\n",
    "```\n",
    "\n",
    "## Matrix Decompositions\n",
    "\n",
    "### QR Decomposition\n",
    "\n",
    "```python\n",
    "# QR decomposition and applications\n",
    "def qr_decomposition_demo():\n",
    "    \"\"\"Demonstrate QR decomposition and applications\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(42)\n",
    "    \n",
    "    # Create test matrix\n",
    "    A = random.normal(key, (6, 4))  # Tall matrix\n",
    "    \n",
    "    # QR decomposition\n",
    "    Q, R = jnp.linalg.qr(A, mode='reduced')  # Reduced QR\n",
    "    Q_full, R_full = jnp.linalg.qr(A, mode='complete')  # Full QR\n",
    "    \n",
    "    print(\"QR Decomposition:\")\n",
    "    print(f\"A shape: {A.shape}\")\n",
    "    print(f\"Q shape (reduced): {Q.shape}, R shape: {R.shape}\")\n",
    "    print(f\"Q shape (full): {Q_full.shape}, R shape: {R_full.shape}\")\n",
    "    \n",
    "    # Verify decomposition\n",
    "    reconstruction_error = jnp.max(jnp.abs(A - Q @ R))\n",
    "    print(f\"Reconstruction error: {reconstruction_error:.2e}\")\n",
    "    \n",
    "    # Verify Q is orthogonal\n",
    "    orthogonality_error = jnp.max(jnp.abs(Q.T @ Q - jnp.eye(Q.shape[1])))\n",
    "    print(f\"Q orthogonality error: {orthogonality_error:.2e}\")\n",
    "    \n",
    "    # Application: Solving least squares\n",
    "    b = random.normal(random.split(key)[1], (6,))\n",
    "    \n",
    "    # Least squares solution using QR\n",
    "    x_qr = jnp.linalg.solve(R, Q.T @ b)\n",
    "    \n",
    "    # Compare with direct solve\n",
    "    x_direct = jnp.linalg.lstsq(A, b, rcond=None)[0]\n",
    "    \n",
    "    print(f\"Least squares solution difference: {jnp.max(jnp.abs(x_qr - x_direct)):.2e}\")\n",
    "    \n",
    "    return Q, R, A, b\n",
    "\n",
    "Q, R, A, b = qr_decomposition_demo()\n",
    "```\n",
    "\n",
    "### SVD (Singular Value Decomposition)\n",
    "\n",
    "```python\n",
    "# Singular Value Decomposition\n",
    "def svd_demo():\n",
    "    \"\"\"Demonstrate SVD and applications\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(123)\n",
    "    \n",
    "    # Create test matrix with known rank\n",
    "    rank = 3\n",
    "    U_true = random.normal(key, (5, rank))\n",
    "    V_true = random.normal(random.split(key)[1], (4, rank))\n",
    "    S_true = jnp.array([10.0, 5.0, 1.0])  # Singular values\n",
    "    A = U_true @ jnp.diag(S_true) @ V_true.T\n",
    "    \n",
    "    print(\"SVD Analysis:\")\n",
    "    print(f\"Original matrix shape: {A.shape}\")\n",
    "    print(f\"True rank: {rank}\")\n",
    "    \n",
    "    # Perform SVD\n",
    "    U, S, Vt = jnp.linalg.svd(A, full_matrices=False)\n",
    "    \n",
    "    print(f\"SVD shapes: U{U.shape}, S{S.shape}, Vt{Vt.shape}\")\n",
    "    print(f\"Singular values: {S}\")\n",
    "    \n",
    "    # Verify decomposition\n",
    "    reconstruction = U @ jnp.diag(S) @ Vt\n",
    "    reconstruction_error = jnp.max(jnp.abs(A - reconstruction))\n",
    "    print(f\"Reconstruction error: {reconstruction_error:.2e}\")\n",
    "    \n",
    "    # Low-rank approximation\n",
    "    for k in [1, 2, 3]:\n",
    "        A_k = U[:, :k] @ jnp.diag(S[:k]) @ Vt[:k, :]\n",
    "        error = jnp.linalg.norm(A - A_k, 'fro')\n",
    "        compression_ratio = (k * (A.shape[0] + A.shape[1])) / (A.shape[0] * A.shape[1])\n",
    "        print(f\"Rank-{k} approximation: error={error:.4f}, compression={compression_ratio:.2f}\")\n",
    "    \n",
    "    return U, S, Vt, A\n",
    "\n",
    "U, S, Vt, A_svd = svd_demo()\n",
    "```\n",
    "\n",
    "### Eigenvalue Decomposition\n",
    "\n",
    "```python\n",
    "# Eigenvalue and eigenvector computation\n",
    "def eigenvalue_demo():\n",
    "    \"\"\"Demonstrate eigenvalue computations\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(456)\n",
    "    \n",
    "    # Create symmetric positive definite matrix\n",
    "    A_base = random.normal(key, (4, 4))\n",
    "    A = A_base @ A_base.T + jnp.eye(4)  # SPD matrix\n",
    "    \n",
    "    print(\"Eigenvalue Analysis:\")\n",
    "    \n",
    "    # Eigenvalues and eigenvectors\n",
    "    eigvals, eigvecs = jnp.linalg.eigh(A)  # For symmetric matrices\n",
    "    \n",
    "    print(f\"Matrix shape: {A.shape}\")\n",
    "    print(f\"Eigenvalues: {eigvals}\")\n",
    "    print(f\"Condition number: {eigvals[-1] / eigvals[0]:.2f}\")\n",
    "    \n",
    "    # Verify eigen-decomposition\n",
    "    reconstruction = eigvecs @ jnp.diag(eigvals) @ eigvecs.T\n",
    "    reconstruction_error = jnp.max(jnp.abs(A - reconstruction))\n",
    "    print(f\"Eigendecomposition reconstruction error: {reconstruction_error:.2e}\")\n",
    "    \n",
    "    # Verify individual eigenpairs\n",
    "    for i in range(len(eigvals)):\n",
    "        Av = A @ eigvecs[:, i]\n",
    "        lv = eigvals[i] * eigvecs[:, i]\n",
    "        error = jnp.max(jnp.abs(Av - lv))\n",
    "        print(f\"Eigenpair {i} error: {error:.2e}\")\n",
    "    \n",
    "    # For general matrices, use eig (complex eigenvalues possible)\n",
    "    B = random.normal(key, (3, 3))\n",
    "    eigvals_general, eigvecs_general = jnp.linalg.eig(B)\n",
    "    print(f\"General matrix eigenvalues (can be complex): {eigvals_general}\")\n",
    "    \n",
    "    return eigvals, eigvecs, A\n",
    "\n",
    "eigvals, eigvecs, A_eigen = eigenvalue_demo()\n",
    "```\n",
    "\n",
    "## Advanced Matrix Operations\n",
    "\n",
    "### Matrix Functions\n",
    "\n",
    "```python\n",
    "# Matrix functions: exp, log, sqrt, etc.\n",
    "def matrix_functions_demo():\n",
    "    \"\"\"Demonstrate matrix functions\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(789)\n",
    "    \n",
    "    # Create a well-conditioned symmetric positive definite matrix\n",
    "    A_base = random.normal(key, (3, 3))\n",
    "    A = 0.1 * (A_base @ A_base.T) + jnp.eye(3)  # Small eigenvalues for stability\n",
    "    \n",
    "    print(\"Matrix Functions:\")\n",
    "    print(f\"Matrix A condition number: {jnp.linalg.cond(A):.2f}\")\n",
    "    \n",
    "    # Matrix exponential via eigendecomposition\n",
    "    def matrix_exp_eigen(M):\n",
    "        eigvals, eigvecs = jnp.linalg.eigh(M)\n",
    "        return eigvecs @ jnp.diag(jnp.exp(eigvals)) @ eigvecs.T\n",
    "    \n",
    "    # Matrix square root\n",
    "    def matrix_sqrt_eigen(M):\n",
    "        eigvals, eigvecs = jnp.linalg.eigh(M)\n",
    "        return eigvecs @ jnp.diag(jnp.sqrt(jnp.maximum(eigvals, 1e-12))) @ eigvecs.T\n",
    "    \n",
    "    # Matrix logarithm\n",
    "    def matrix_log_eigen(M):\n",
    "        eigvals, eigvecs = jnp.linalg.eigh(M)\n",
    "        return eigvecs @ jnp.diag(jnp.log(jnp.maximum(eigvals, 1e-12))) @ eigvecs.T\n",
    "    \n",
    "    # Compute matrix functions\n",
    "    A_exp = matrix_exp_eigen(A)\n",
    "    A_sqrt = matrix_sqrt_eigen(A)\n",
    "    A_log = matrix_log_eigen(A)\n",
    "    \n",
    "    print(f\"Original matrix trace: {jnp.trace(A):.4f}\")\n",
    "    print(f\"Matrix exp trace: {jnp.trace(A_exp):.4f}\")\n",
    "    print(f\"Matrix sqrt trace: {jnp.trace(A_sqrt):.4f}\")\n",
    "    \n",
    "    # Verify properties\n",
    "    # (A^(1/2))^2 should equal A\n",
    "    sqrt_squared_error = jnp.max(jnp.abs(A_sqrt @ A_sqrt - A))\n",
    "    print(f\"Matrix sqrt verification error: {sqrt_squared_error:.2e}\")\n",
    "    \n",
    "    # exp(log(A)) should equal A\n",
    "    exp_log_error = jnp.max(jnp.abs(matrix_exp_eigen(A_log) - A))\n",
    "    print(f\"exp(log(A)) verification error: {exp_log_error:.2e}\")\n",
    "    \n",
    "    return A_exp, A_sqrt, A_log\n",
    "\n",
    "A_exp, A_sqrt, A_log = matrix_functions_demo()\n",
    "```\n",
    "\n",
    "### Kronecker Products and Vectorization\n",
    "\n",
    "```python\n",
    "# Kronecker products and vectorization operations\n",
    "def kronecker_demo():\n",
    "    \"\"\"Demonstrate Kronecker products and vec operations\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(0)\n",
    "    \n",
    "    A = random.normal(key, (2, 3))\n",
    "    B = random.normal(random.split(key)[1], (4, 2))\n",
    "    \n",
    "    print(\"Kronecker Products:\")\n",
    "    print(f\"A shape: {A.shape}, B shape: {B.shape}\")\n",
    "    \n",
    "    # Kronecker product A â B\n",
    "    kron_AB = jnp.kron(A, B)\n",
    "    print(f\"A â B shape: {kron_AB.shape}\")\n",
    "    \n",
    "    # Properties of Kronecker products\n",
    "    C = random.normal(random.split(key, 3)[2], (3, 2))\n",
    "    D = random.normal(random.split(key, 4)[3], (2, 3))\n",
    "    \n",
    "    # (A â B)(C â D) = (AC) â (BD) when dimensions compatible\n",
    "    if A.shape[1] == C.shape[0] and B.shape[1] == D.shape[0]:\n",
    "        left_side = jnp.kron(A, B) @ jnp.kron(C, D)\n",
    "        right_side = jnp.kron(A @ C, B @ D)\n",
    "        kron_property_error = jnp.max(jnp.abs(left_side - right_side))\n",
    "        print(f\"Kronecker product property error: {kron_property_error:.2e}\")\n",
    "    \n",
    "    # Vectorization\n",
    "    X = random.normal(key, (3, 4))\n",
    "    vec_X = X.flatten()  # or X.ravel()\n",
    "    print(f\"Matrix X shape: {X.shape}, vectorized shape: {vec_X.shape}\")\n",
    "    \n",
    "    # Relationship: vec(AXB) = (B^T â A) vec(X)\n",
    "    A_small = random.normal(key, (2, 3))\n",
    "    B_small = random.normal(random.split(key)[1], (4, 5))\n",
    "    AXB = A_small @ X @ B_small\n",
    "    \n",
    "    vec_AXB_direct = AXB.flatten()\n",
    "    vec_AXB_kron = jnp.kron(B_small.T, A_small) @ vec_X\n",
    "    \n",
    "    vectorization_error = jnp.max(jnp.abs(vec_AXB_direct - vec_AXB_kron))\n",
    "    print(f\"Vectorization identity error: {vectorization_error:.2e}\")\n",
    "    \n",
    "    return kron_AB\n",
    "\n",
    "kron_result = kronecker_demo()\n",
    "```\n",
    "\n",
    "## Numerical Linear Algebra Considerations\n",
    "\n",
    "### Conditioning and Stability\n",
    "\n",
    "```python\n",
    "# Numerical stability analysis\n",
    "def stability_analysis():\n",
    "    \"\"\"Analyze numerical stability of matrix operations\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(42)\n",
    "    \n",
    "    print(\"Numerical Stability Analysis:\")\n",
    "    \n",
    "    # Create ill-conditioned matrix\n",
    "    n = 5\n",
    "    U = random.orthogonal(key, n)\n",
    "    singular_values = jnp.array([1e6, 1e3, 1e0, 1e-3, 1e-6])  # Wide range\n",
    "    V = random.orthogonal(random.split(key)[1], n)\n",
    "    \n",
    "    A_ill = U @ jnp.diag(singular_values) @ V.T\n",
    "    \n",
    "    print(f\"Condition number: {jnp.linalg.cond(A_ill):.2e}\")\n",
    "    \n",
    "    # Test different solution methods\n",
    "    b = random.normal(random.split(key, 3)[2], (n,))\n",
    "    x_true = jnp.ones(n)  # Known solution\n",
    "    b = A_ill @ x_true  # Consistent right-hand side\n",
    "    \n",
    "    # Method 1: Direct solve\n",
    "    x_direct = jnp.linalg.solve(A_ill, b)\n",
    "    error_direct = jnp.linalg.norm(x_direct - x_true)\n",
    "    \n",
    "    # Method 2: SVD-based solve with truncation\n",
    "    U_svd, S_svd, Vt_svd = jnp.linalg.svd(A_ill)\n",
    "    tolerance = 1e-10\n",
    "    rank = jnp.sum(S_svd > tolerance)\n",
    "    \n",
    "    S_inv = jnp.where(S_svd > tolerance, 1.0 / S_svd, 0.0)\n",
    "    x_svd = Vt_svd.T @ (S_inv[:, None] * (U_svd.T @ b[:, None]))[:, 0]\n",
    "    error_svd = jnp.linalg.norm(x_svd - x_true)\n",
    "    \n",
    "    print(f\"Effective rank: {rank}/{n}\")\n",
    "    print(f\"Direct solve error: {error_direct:.2e}\")\n",
    "    print(f\"SVD solve error: {error_svd:.2e}\")\n",
    "    \n",
    "    # Perturbation analysis\n",
    "    perturbation = 1e-10 * random.normal(key, A_ill.shape)\n",
    "    A_perturbed = A_ill + perturbation\n",
    "    \n",
    "    x_perturbed = jnp.linalg.solve(A_perturbed, b)\n",
    "    perturbation_effect = jnp.linalg.norm(x_perturbed - x_direct) / jnp.linalg.norm(perturbation)\n",
    "    \n",
    "    print(f\"Perturbation amplification: {perturbation_effect:.2e}\")\n",
    "\n",
    "stability_analysis()\n",
    "```\n",
    "\n",
    "### Performance Benchmarking\n",
    "\n",
    "```python\n",
    "# Performance comparison of matrix operations\n",
    "def performance_benchmark():\n",
    "    \"\"\"Benchmark different matrix operations\"\"\"\n",
    "    \n",
    "    print(\"Performance Benchmarking:\")\n",
    "    \n",
    "    sizes = [64, 128, 256, 512, 1024]\n",
    "    \n",
    "    for n in sizes:\n",
    "        key = random.PRNGKey(42)\n",
    "        A = random.normal(key, (n, n))\n",
    "        B = random.normal(random.split(key)[1], (n, n))\n",
    "        \n",
    "        # Matrix multiplication\n",
    "        start = time.time()\n",
    "        for _ in range(10):\n",
    "            C = A @ B\n",
    "        matmul_time = (time.time() - start) / 10\n",
    "        \n",
    "        # Eigenvalue decomposition\n",
    "        A_sym = A @ A.T  # Make symmetric for stable eigh\n",
    "        start = time.time()\n",
    "        eigvals, eigvecs = jnp.linalg.eigh(A_sym)\n",
    "        eigen_time = time.time() - start\n",
    "        \n",
    "        # SVD\n",
    "        start = time.time()\n",
    "        U, S, Vt = jnp.linalg.svd(A)\n",
    "        svd_time = time.time() - start\n",
    "        \n",
    "        # QR decomposition\n",
    "        start = time.time()\n",
    "        Q, R = jnp.linalg.qr(A)\n",
    "        qr_time = time.time() - start\n",
    "        \n",
    "        print(f\"n={n:4d}: MatMul={matmul_time*1000:6.2f}ms, \"\n",
    "              f\"Eigen={eigen_time*1000:6.2f}ms, \"\n",
    "              f\"SVD={svd_time*1000:6.2f}ms, \"\n",
    "              f\"QR={qr_time*1000:6.2f}ms\")\n",
    "\n",
    "# Run benchmark (may take a moment)\n",
    "performance_benchmark()\n",
    "```\n",
    "\n",
    "## Specialized Matrix Operations\n",
    "\n",
    "### Block Matrix Operations\n",
    "\n",
    "```python\n",
    "# Block matrix operations\n",
    "def block_matrix_demo():\n",
    "    \"\"\"Demonstrate block matrix operations\"\"\"\n",
    "    \n",
    "    key = random.PRNGKey(0)\n",
    "    \n",
    "    # Create block matrices\n",
    "    A11 = random.normal(key, (2, 2))\n",
    "    A12 = random.normal(random.split(key, 2)[1], (2, 3))\n",
    "    A21 = random.normal(random.split(key, 3)[2], (3, 2))\n",
    "    A22 = random.normal(random.split(key, 4)[3], (3, 3))\n",
    "    \n",
    "    # Assemble block matrix\n",
    "    A_top = jnp.concatenate([A11, A12], axis=1)\n",
    "    A_bottom = jnp.concatenate([A21, A22], axis=1)\n",
    "    A_block = jnp.concatenate([A_top, A_bottom], axis=0)\n",
    "    \n",
    "    print(\"Block Matrix Operations:\")\n",
    "    print(f\"Block matrix shape: {A_block.shape}\")\n",
    "    print(f\"A11 shape: {A11.shape}, A12 shape: {A12.shape}\")\n",
    "    print(f\"A21 shape: {A21.shape}, A22 shape: {A22.shape}\")\n",
    "    \n",
    "    # Block matrix inverse (for 2x2 block structure when A11 is invertible)\n",
    "    def block_inverse_2x2(A11, A12, A21, A22):\n",
    "        \"\"\"Compute inverse of 2x2 block matrix using Schur complement\"\"\"\n",
    "        A11_inv = jnp.linalg.inv(A11)\n",
    "        schur = A22 - A21 @ A11_inv @ A12  # Schur complement\n",
    "        schur_inv = jnp.linalg.inv(schur)\n",
    "        \n",
    "        # Compute blocks of inverse\n",
    "        inv_11 = A11_inv + A11_inv @ A12 @ schur_inv @ A21 @ A11_inv\n",
    "        inv_12 = -A11_inv @ A12 @ schur_inv\n",
    "        inv_21 = -schur_inv @ A21 @ A11_inv\n",
    "        inv_22 = schur_inv\n",
    "        \n",
    "        return inv_11, inv_12, inv_21, inv_22\n",
    "    \n",
    "    # Test block inverse\n",
    "    if jnp.linalg.cond(A11) < 1e12 and jnp.linalg.cond(A22 - A21 @ jnp.linalg.inv(A11) @ A12) < 1e12:\n",
    "        inv_11, inv_12, inv_21, inv_22 = block_inverse_2x2(A11, A12, A21, A22)\n",
    "        \n",
    "        # Assemble block inverse\n",
    "        inv_top = jnp.concatenate([inv_11, inv_12], axis=1)\n",
    "        inv_bottom = jnp.concatenate([inv_21, inv_22], axis=1)\n",
    "        A_block_inv = jnp.concatenate([inv_top, inv_bottom], axis=0)\n",
    "        \n",
    "        # Verify inverse\n",
    "        identity_error = jnp.max(jnp.abs(A_block @ A_block_inv - jnp.eye(A_block.shape[0])))\n",
    "        print(f\"Block inverse verification error: {identity_error:.2e}\")\n",
    "    \n",
    "    return A_block\n",
    "\n",
    "A_block = block_matrix_demo()\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we've explored fundamental matrix operations in JAX:\n",
    "\n",
    "**Key Operations Covered:**\n",
    "\n",
    "1. **Basic Operations**: Matrix multiplication, einsum, batch operations\n",
    "2. **Decompositions**: QR, SVD, eigenvalue decomposition\n",
    "3. **Matrix Functions**: Exponential, square root, logarithm via eigendecomposition\n",
    "4. **Advanced Operations**: Kronecker products, vectorization, block matrices\n",
    "\n",
    "**Numerical Considerations:**\n",
    "- Condition number analysis for stability\n",
    "- Perturbation sensitivity\n",
    "- Choosing appropriate algorithms for different matrix properties\n",
    "- Performance characteristics of different operations\n",
    "\n",
    "**JAX-Specific Features:**\n",
    "- Automatic differentiation through linear algebra operations\n",
    "- JIT compilation for performance\n",
    "- Vectorization with vmap for batch processing\n",
    "- GPU/TPU acceleration\n",
    "\n",
    "**Best Practices:**\n",
    "- Use appropriate decompositions for matrix properties (eigh for symmetric, SVD for general)\n",
    "- Consider numerical stability for ill-conditioned problems\n",
    "- Leverage JAX transformations for performance\n",
    "- Choose efficient algorithms based on matrix structure\n",
    "\n",
    "**Next Steps:**\n",
    "- The next notebook will cover iterative solvers\n",
    "- We'll explore conjugate gradient, GMRES, and other iterative methods\n",
    "- Understanding direct methods enables comparison with iterative approaches\n",
    "\n",
    "Matrix operations form the computational backbone of scientific computing and machine learning. JAX's implementation provides both high performance and seamless integration with automatic differentiation, making it ideal for research and production applications."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
