{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd1208bf",
   "metadata": {},
   "source": [
    "# File: notebooks/02_linear_algebra/06_iterative_solvers.ipynb\n",
    "\n",
    "## JAX Linear Algebra: Iterative Solvers\n",
    "\n",
    "This notebook explores iterative methods for solving linear systems and optimization problems in JAX. We'll implement conjugate gradient, GMRES, gradient descent variants, and Newton-type methods. These methods are essential for large-scale problems where direct factorization is computationally prohibitive.\n",
    "\n",
    "Iterative solvers are particularly important in scientific computing and machine learning, where we often deal with large sparse systems or optimization problems that benefit from warm starts and approximate solutions.\n",
    "\n",
    "## Setting Up the Environment\n",
    "\n",
    "```python\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, lax\n",
    "from jax import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Callable\n",
    "import functools\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "```\n",
    "\n",
    "## Conjugate Gradient Method\n",
    "\n",
    "### Basic CG Implementation\n",
    "\n",
    "```python\n",
    "def conjugate_gradient(A, b, x0=None, max_iters=None, tol=1e-6):\n",
    "    \"\"\"Conjugate Gradient method for solving Ax = b where A is SPD\"\"\"\n",
    "    \n",
    "    n = len(b)\n",
    "    if x0 is None:\n",
    "        x0 = jnp.zeros(n)\n",
    "    if max_iters is None:\n",
    "        max_iters = n\n",
    "    \n",
    "    def cg_step(state, _):\n",
    "        x, r, p, iter_count = state\n",
    "        \n",
    "        # Compute alpha\n",
    "        Ap = A @ p\n",
    "        pAp = jnp.dot(p, Ap)\n",
    "        alpha = jnp.dot(r, r) / pAp\n",
    "        \n",
    "        # Update solution and residual\n",
    "        x_new = x + alpha * p\n",
    "        r_new = r - alpha * Ap\n",
    "        \n",
    "        # Compute beta for next iteration\n",
    "        beta = jnp.dot(r_new, r_new) / jnp.dot(r, r)\n",
    "        p_new = r_new + beta * p\n",
    "        \n",
    "        new_state = (x_new, r_new, p_new, iter_count + 1)\n",
    "        \n",
    "        # Return state and residual norm for tracking\n",
    "        return new_state, jnp.linalg.norm(r_new)\n",
    "    \n",
    "    # Initial state\n",
    "    r0 = b - A @ x0\n",
    "    p0 = r0\n",
    "    initial_state = (x0, r0, p0, 0)\n",
    "    \n",
    "    # Run iterations\n",
    "    def cg_cond(state):\n",
    "        _, r, _, iter_count = state\n",
    "        return (jnp.linalg.norm(r) > tol) & (iter_count < max_iters)\n",
    "    \n",
    "    def cg_body(state):\n",
    "        new_state, _ = cg_step(state, None)\n",
    "        return new_state\n",
    "    \n",
    "    final_state = lax.while_loop(cg_cond, cg_body, initial_state)\n",
    "    \n",
    "    return final_state[0], final_state[3]  # solution, iterations\n",
    "\n",
    "# Test CG method\n",
    "key = random.PRNGKey(42)\n",
    "n = 100\n",
    "\n",
    "# Create SPD matrix\n",
    "A_base = random.normal(key, (n, n))\n",
    "A = A_base @ A_base.T + jnp.eye(n)\n",
    "b = random.normal(random.split(key)[1], (n,))\n",
    "\n",
    "# Solve with CG\n",
    "x_cg, iters = conjugate_gradient(A, b, tol=1e-10)\n",
    "\n",
    "# Verify solution\n",
    "residual = jnp.linalg.norm(A @ x_cg - b)\n",
    "print(f\"CG iterations: {iters}\")\n",
    "print(f\"Residual norm: {residual:.2e}\")\n",
    "\n",
    "# Compare with direct solve\n",
    "x_direct = jnp.linalg.solve(A, b)\n",
    "solution_error = jnp.linalg.norm(x_cg - x_direct)\n",
    "print(f\"Solution error vs direct: {solution_error:.2e}\")\n",
    "```\n",
    "\n",
    "### Preconditioned CG\n",
    "\n",
    "```python\n",
    "def preconditioned_cg(A, b, M=None, x0=None, max_iters=None, tol=1e-6):\n",
    "    \"\"\"Preconditioned Conjugate Gradient method\"\"\"\n",
    "    \n",
    "    n = len(b)\n",
    "    if x0 is None:\n",
    "        x0 = jnp.zeros(n)\n",
    "    if max_iters is None:\n",
    "        max_iters = n\n",
    "    if M is None:\n",
    "        M = jnp.eye(n)  # No preconditioning\n",
    "    \n",
    "    # Solve M @ z = r for preconditioning\n",
    "    solve_M = lambda r: jnp.linalg.solve(M, r)\n",
    "    \n",
    "    def pcg_step(state):\n",
    "        x, r, p = state\n",
    "        \n",
    "        # Solve M z = r\n",
    "        z = solve_M(r)\n",
    "        \n",
    "        # Compute alpha\n",
    "        Ap = A @ p\n",
    "        alpha = jnp.dot(r, z) / jnp.dot(p, Ap)\n",
    "        \n",
    "        # Update solution and residual\n",
    "        x_new = x + alpha * p\n",
    "        r_new = r - alpha * Ap\n",
    "        \n",
    "        # Compute beta\n",
    "        z_new = solve_M(r_new)\n",
    "        beta = jnp.dot(r_new, z_new) / jnp.dot(r, z)\n",
    "        p_new = z_new + beta * p\n",
    "        \n",
    "        return (x_new, r_new, p_new), jnp.linalg.norm(r_new)\n",
    "    \n",
    "    # Initial state\n",
    "    r0 = b - A @ x0\n",
    "    z0 = solve_M(r0)\n",
    "    p0 = z0\n",
    "    initial_state = (x0, r0, p0)\n",
    "    \n",
    "    # Run iterations using scan\n",
    "    def scan_cond_body(carry, _):\n",
    "        state, converged, iter_count = carry\n",
    "        \n",
    "        # Only do step if not converged\n",
    "        new_state, residual_norm = lax.cond(\n",
    "            converged,\n",
    "            lambda s: (s, residual_norm),  # Don't update if converged\n",
    "            lambda s: pcg_step(s),\n",
    "            state\n",
    "        )\n",
    "        \n",
    "        new_converged = converged | (residual_norm < tol) | (iter_count >= max_iters)\n",
    "        new_carry = (new_state, new_converged, iter_count + 1)\n",
    "        \n",
    "        return new_carry, residual_norm\n",
    "    \n",
    "    # Run scan\n",
    "    init_carry = (initial_state, False, 0)\n",
    "    final_carry, residuals = lax.scan(scan_cond_body, init_carry, jnp.arange(max_iters))\n",
    "    \n",
    "    final_x = final_carry[0][0]\n",
    "    final_iters = final_carry[2]\n",
    "    \n",
    "    return final_x, final_iters, residuals\n",
    "\n",
    "# Test with diagonal preconditioning\n",
    "diag_A = jnp.diag(jnp.diag(A))  # Diagonal preconditioner\n",
    "x_pcg, iters_pcg, residuals = preconditioned_cg(A, b, M=diag_A, tol=1e-10)\n",
    "\n",
    "print(f\"Preconditioned CG iterations: {iters_pcg}\")\n",
    "print(f\"Final residual: {jnp.linalg.norm(A @ x_pcg - b):.2e}\")\n",
    "```\n",
    "\n",
    "## GMRES (Generalized Minimal Residual)\n",
    "\n",
    "### GMRES Implementation\n",
    "\n",
    "```python\n",
    "def gmres(A, b, x0=None, m=None, max_iters=100, tol=1e-6):\n",
    "    \"\"\"GMRES method for solving Ax = b\"\"\"\n",
    "    \n",
    "    n = len(b)\n",
    "    if x0 is None:\n",
    "        x0 = jnp.zeros(n)\n",
    "    if m is None:\n",
    "        m = min(n, 50)  # Restart parameter\n",
    "    \n",
    "    def arnoldi_process(A, v, m):\n",
    "        \"\"\"Arnoldi process to build Krylov subspace\"\"\"\n",
    "        n = len(v)\n",
    "        V = jnp.zeros((n, m + 1))\n",
    "        H = jnp.zeros((m + 1, m))\n",
    "        \n",
    "        V = V.at[:, 0].set(v / jnp.linalg.norm(v))\n",
    "        \n",
    "        def arnoldi_step(i, carry):\n",
    "            V, H = carry\n",
    "            \n",
    "            # Compute A @ V[:, i]\n",
    "            w = A @ V[:, i]\n",
    "            \n",
    "            # Orthogonalization\n",
    "            for j in range(i + 1):\n",
    "                h_ji = jnp.dot(w, V[:, j])\n",
    "                H = H.at[j, i].set(h_ji)\n",
    "                w = w - h_ji * V[:, j]\n",
    "            \n",
    "            # Normalization\n",
    "            h_next = jnp.linalg.norm(w)\n",
    "            H = H.at[i + 1, i].set(h_next)\n",
    "            \n",
    "            # Avoid division by zero\n",
    "            V = V.at[:, i + 1].set(jnp.where(h_next > 1e-14, w / h_next, w))\n",
    "            \n",
    "            return (V, H)\n",
    "        \n",
    "        V, H = lax.fori_loop(0, m, arnoldi_step, (V, H))\n",
    "        return V, H\n",
    "    \n",
    "    def gmres_solve(A, b, x0, m, tol):\n",
    "        r0 = b - A @ x0\n",
    "        beta = jnp.linalg.norm(r0)\n",
    "        \n",
    "        if beta < tol:\n",
    "            return x0, 0\n",
    "        \n",
    "        # Build Krylov subspace\n",
    "        V, H = arnoldi_process(A, r0, m)\n",
    "        \n",
    "        # Solve least squares problem\n",
    "        e1 = jnp.zeros(m + 1)\n",
    "        e1 = e1.at[0].set(1.0)\n",
    "        \n",
    "        # QR factorization of H\n",
    "        Q, R = jnp.linalg.qr(H)\n",
    "        \n",
    "        # Solve R y = beta * Q^T e1\n",
    "        rhs = beta * Q.T @ e1\n",
    "        y = jnp.linalg.solve(R[:m, :m], rhs[:m])\n",
    "        \n",
    "        # Update solution\n",
    "        x_new = x0 + V[:, :m] @ y\n",
    "        \n",
    "        return x_new, m\n",
    "    \n",
    "    # GMRES with restart\n",
    "    x = x0\n",
    "    total_iters = 0\n",
    "    \n",
    "    for restart in range(max_iters // m + 1):\n",
    "        x, iters = gmres_solve(A, b, x, m, tol)\n",
    "        total_iters += iters\n",
    "        \n",
    "        residual = jnp.linalg.norm(b - A @ x)\n",
    "        if residual < tol:\n",
    "            break\n",
    "    \n",
    "    return x, total_iters\n",
    "\n",
    "# Test GMRES\n",
    "A_nonsym = random.normal(key, (50, 50))  # Non-symmetric matrix\n",
    "A_nonsym = A_nonsym + 5 * jnp.eye(50)  # Make diagonally dominant\n",
    "b_nonsym = random.normal(random.split(key)[1], (50,))\n",
    "\n",
    "x_gmres, iters_gmres = gmres(A_nonsym, b_nonsym, m=20, tol=1e-8)\n",
    "\n",
    "print(f\"GMRES iterations: {iters_gmres}\")\n",
    "print(f\"GMRES residual: {jnp.linalg.norm(A_nonsym @ x_gmres - b_nonsym):.2e}\")\n",
    "```\n",
    "\n",
    "## Gradient Descent Methods\n",
    "\n",
    "### Steepest Descent\n",
    "\n",
    "```python\n",
    "def gradient_descent(A, b, x0=None, learning_rate=None, max_iters=1000, tol=1e-6):\n",
    "    \"\"\"Gradient descent for solving Ax = b (minimizes ||Ax - b||^2)\"\"\"\n",
    "    \n",
    "    n = len(b)\n",
    "    if x0 is None:\n",
    "        x0 = jnp.zeros(n)\n",
    "    \n",
    "    # Optimal learning rate for quadratic: 2 / (λ_max + λ_min)\n",
    "    if learning_rate is None:\n",
    "        eigvals = jnp.linalg.eigvals(A.T @ A)\n",
    "        learning_rate = 2.0 / (jnp.max(eigvals) + jnp.min(eigvals))\n",
    "    \n",
    "    def grad_step(x, _):\n",
    "        residual = A @ x - b\n",
    "        grad = A.T @ residual\n",
    "        x_new = x - learning_rate * grad\n",
    "        return x_new, jnp.linalg.norm(residual)\n",
    "    \n",
    "    x, residuals = lax.scan(grad_step, x0, jnp.arange(max_iters))\n",
    "    \n",
    "    # Find convergence point\n",
    "    converged_idx = jnp.argmax(residuals < tol)\n",
    "    actual_iters = jnp.where(jnp.any(residuals < tol), converged_idx, max_iters)\n",
    "    \n",
    "    return x, actual_iters, residuals\n",
    "\n",
    "# Test gradient descent\n",
    "x_gd, iters_gd, residuals_gd = gradient_descent(A, b, max_iters=500, tol=1e-8)\n",
    "\n",
    "print(f\"Gradient descent iterations: {iters_gd}\")\n",
    "print(f\"Final residual: {jnp.linalg.norm(A @ x_gd - b):.2e}\")\n",
    "```\n",
    "\n",
    "### Conjugate Gradient vs Gradient Descent Comparison\n",
    "\n",
    "```python\n",
    "def compare_methods():\n",
    "    \"\"\"Compare convergence of different iterative methods\"\"\"\n",
    "    \n",
    "    # Create test problem with different condition numbers\n",
    "    condition_numbers = [1e1, 1e3, 1e5]\n",
    "    \n",
    "    for cond_num in condition_numbers:\n",
    "        print(f\"\\nCondition number: {cond_num:.0e}\")\n",
    "        \n",
    "        # Create matrix with specific condition number\n",
    "        U = random.orthogonal(key, 30)\n",
    "        singular_values = jnp.logspace(0, jnp.log10(cond_num), 30)\n",
    "        A_test = U @ jnp.diag(singular_values) @ U.T\n",
    "        b_test = random.normal(key, (30,))\n",
    "        \n",
    "        # CG\n",
    "        x_cg, iters_cg = conjugate_gradient(A_test, b_test, tol=1e-8, max_iters=100)\n",
    "        \n",
    "        # Gradient descent\n",
    "        x_gd, iters_gd, _ = gradient_descent(A_test, b_test, tol=1e-8, max_iters=500)\n",
    "        \n",
    "        print(f\"  CG iterations: {iters_cg}\")\n",
    "        print(f\"  GD iterations: {iters_gd}\")\n",
    "        print(f\"  CG residual: {jnp.linalg.norm(A_test @ x_cg - b_test):.2e}\")\n",
    "        print(f\"  GD residual: {jnp.linalg.norm(A_test @ x_gd - b_test):.2e}\")\n",
    "\n",
    "compare_methods()\n",
    "```\n",
    "\n",
    "## Newton-Type Methods\n",
    "\n",
    "### Newton-Raphson for Nonlinear Systems\n",
    "\n",
    "```python\n",
    "def newton_raphson_system(F, J, x0, max_iters=20, tol=1e-8):\n",
    "    \"\"\"Newton-Raphson for solving F(x) = 0\"\"\"\n",
    "    \n",
    "    def newton_step(x, _):\n",
    "        f_val = F(x)\n",
    "        j_val = J(x)\n",
    "        \n",
    "        # Solve J(x) * delta = -F(x)\n",
    "        delta = jnp.linalg.solve(j_val, -f_val)\n",
    "        x_new = x + delta\n",
    "        \n",
    "        return x_new, jnp.linalg.norm(f_val)\n",
    "    \n",
    "    x, residuals = lax.scan(newton_step, x0, jnp.arange(max_iters))\n",
    "    \n",
    "    # Find convergence\n",
    "    converged_idx = jnp.argmax(residuals < tol)\n",
    "    actual_iters = jnp.where(jnp.any(residuals < tol), converged_idx, max_iters)\n",
    "    \n",
    "    return x, actual_iters\n",
    "\n",
    "# Example: Solve system x^2 + y^2 = 1, x - y = 0.5\n",
    "def nonlinear_system(x):\n",
    "    return jnp.array([\n",
    "        x[0]**2 + x[1]**2 - 1,\n",
    "        x[0] - x[1] - 0.5\n",
    "    ])\n",
    "\n",
    "def jacobian(x):\n",
    "    return jnp.array([\n",
    "        [2*x[0], 2*x[1]],\n",
    "        [1, -1]\n",
    "    ])\n",
    "\n",
    "x0 = jnp.array([1.0, 0.0])\n",
    "x_newton, iters_newton = newton_raphson_system(nonlinear_system, jacobian, x0)\n",
    "\n",
    "print(f\"Newton iterations: {iters_newton}\")\n",
    "print(f\"Solution: {x_newton}\")\n",
    "print(f\"Residual: {jnp.linalg.norm(nonlinear_system(x_newton)):.2e}\")\n",
    "```\n",
    "\n",
    "## Optimization Methods\n",
    "\n",
    "### L-BFGS\n",
    "\n",
    "```python\n",
    "def lbfgs(objective, grad_fn, x0, m=10, max_iters=100, tol=1e-6):\n",
    "    \"\"\"Limited-memory BFGS optimization\"\"\"\n",
    "    \n",
    "    def lbfgs_two_loop(s_list, y_list, rho_list, grad, k):\n",
    "        \"\"\"L-BFGS two-loop recursion\"\"\"\n",
    "        q = grad\n",
    "        alpha_list = jnp.zeros(m)\n",
    "        \n",
    "        # First loop\n",
    "        def first_loop_body(i, carry):\n",
    "            q, alpha_list = carry\n",
    "            idx = (k - 1 - i) % m\n",
    "            \n",
    "            alpha = rho_list[idx] * jnp.dot(s_list[idx], q)\n",
    "            alpha_list = alpha_list.at[i].set(alpha)\n",
    "            q = q - alpha * y_list[idx]\n",
    "            \n",
    "            return (q, alpha_list)\n",
    "        \n",
    "        q, alpha_list = lax.fori_loop(0, min(k, m), first_loop_body, (q, alpha_list))\n",
    "        \n",
    "        # Scale\n",
    "        if k > 0:\n",
    "            idx = (k - 1) % m\n",
    "            gamma = jnp.dot(s_list[idx], y_list[idx]) / jnp.dot(y_list[idx], y_list[idx])\n",
    "            r = gamma * q\n",
    "        else:\n",
    "            r = q\n",
    "        \n",
    "        # Second loop\n",
    "        def second_loop_body(i, r):\n",
    "            idx = (k - m + i) % m if k >= m else i\n",
    "            beta = rho_list[idx] * jnp.dot(y_list[idx], r)\n",
    "            r = r + s_list[idx] * (alpha_list[m - 1 - i] - beta)\n",
    "            return r\n",
    "        \n",
    "        r = lax.fori_loop(0, min(k, m), second_loop_body, r)\n",
    "        return -r  # Return search direction\n",
    "    \n",
    "    n = len(x0)\n",
    "    x = x0\n",
    "    \n",
    "    # Storage for L-BFGS\n",
    "    s_list = jnp.zeros((m, n))\n",
    "    y_list = jnp.zeros((m, n))\n",
    "    rho_list = jnp.zeros(m)\n",
    "    \n",
    "    grad_prev = grad_fn(x)\n",
    "    \n",
    "    for k in range(max_iters):\n",
    "        if jnp.linalg.norm(grad_prev) < tol:\n",
    "            break\n",
    "        \n",
    "        # Compute search direction\n",
    "        p = lbfgs_two_loop(s_list, y_list, rho_list, grad_prev, k)\n",
    "        \n",
    "        # Line search (simple backtracking)\n",
    "        alpha = 1.0\n",
    "        c1 = 1e-4\n",
    "        \n",
    "        f_current = objective(x)\n",
    "        grad_dot_p = jnp.dot(grad_prev, p)\n",
    "        \n",
    "        for _ in range(20):  # Max line search steps\n",
    "            x_new = x + alpha * p\n",
    "            f_new = objective(x_new)\n",
    "            \n",
    "            if f_new <= f_current + c1 * alpha * grad_dot_p:\n",
    "                break\n",
    "            alpha *= 0.5\n",
    "        \n",
    "        x_new = x + alpha * p\n",
    "        grad_new = grad_fn(x_new)\n",
    "        \n",
    "        # Update L-BFGS storage\n",
    "        s = x_new - x\n",
    "        y = grad_new - grad_prev\n",
    "        rho = 1.0 / jnp.dot(y, s)\n",
    "        \n",
    "        idx = k % m\n",
    "        s_list = s_list.at[idx].set(s)\n",
    "        y_list = y_list.at[idx].set(y)\n",
    "        rho_list = rho_list.at[idx].set(rho)\n",
    "        \n",
    "        x = x_new\n",
    "        grad_prev = grad_new\n",
    "    \n",
    "    return x, k\n",
    "\n",
    "# Test L-BFGS on Rosenbrock function\n",
    "def rosenbrock(x):\n",
    "    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n",
    "\n",
    "rosenbrock_grad = grad(rosenbrock)\n",
    "\n",
    "x0_opt = jnp.array([-1.0, 1.0])\n",
    "x_lbfgs, iters_lbfgs = lbfgs(rosenbrock, rosenbrock_grad, x0_opt)\n",
    "\n",
    "print(f\"L-BFGS iterations: {iters_lbfgs}\")\n",
    "print(f\"Solution: {x_lbfgs}\")\n",
    "print(f\"Function value: {rosenbrock(x_lbfgs):.2e}\")\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we've implemented and compared various iterative solvers:\n",
    "\n",
    "**Linear Solvers:**\n",
    "1. **Conjugate Gradient**: Optimal for SPD systems, O(√κ) convergence\n",
    "2. **Preconditioned CG**: Improved convergence with preconditioning  \n",
    "3. **GMRES**: General non-symmetric systems, restart for memory efficiency\n",
    "4. **Gradient Descent**: Simple but slower convergence\n",
    "\n",
    "**Nonlinear Solvers:**\n",
    "1. **Newton-Raphson**: Quadratic convergence for well-behaved systems\n",
    "2. **L-BFGS**: Quasi-Newton method for optimization problems\n",
    "\n",
    "**Key Insights:**\n",
    "- CG is superior to gradient descent for well-conditioned SPD systems\n",
    "- Preconditioning dramatically improves convergence rates\n",
    "- GMRES handles non-symmetric systems effectively\n",
    "- L-BFGS balances memory usage with Newton-like convergence\n",
    "\n",
    "**Performance Considerations:**\n",
    "- Condition number strongly affects convergence rates\n",
    "- Restarts in GMRES prevent memory growth\n",
    "- Line search ensures convergence in optimization\n",
    "- JAX's autodiff enables easy implementation of gradients and Jacobians\n",
    "\n",
    "**Next Steps:**\n",
    "- The next notebook covers numerical stability considerations\n",
    "- Understanding when to use direct vs iterative methods\n",
    "- Implementing custom preconditioners for specific problem types\n",
    "\n",
    "Iterative methods are essential for large-scale computational problems where direct methods become prohibitively expensive."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
